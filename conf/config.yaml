# Default configuration
hydra:
  output_subdir: null  # Don't create .hydra subdirectory with configs
  run:
    dir: .  # Run in current directory

# Algorithm to use
algo: PPO

# Training parameters
timesteps: 10000000
log_dir: logs
seed: 0

# Environment parameters
num_envs: 16
num_eval_envs: 2

# Checkpoint and evaluation parameters
checkpoint_freq: 1000
eval_freq: 5000
eval_episodes_during_training: 5
eval_episodes: 100
test_episodes: 10

# PPO algorithm parameters
PPO:
  learning_rate: 5e-4
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5

# RecurrentPPO algorithm parameters
RPPO:
  learning_rate: 5e-4
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  clip_range_vf: null

# SAC algorithm parameters
SAC:
  learning_rate: 3e-4
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  ent_coef: "auto"
  target_update_interval: 1
  target_entropy: "auto"

# TD3 algorithm parameters
TD3:
  learning_rate: 3e-4
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 100
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  policy_delay: 2
  target_policy_noise: 0.2
  target_noise_clip: 0.5

# DDPG algorithm parameters
DDPG:
  learning_rate: 1e-3
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 100
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null

# CrossQ algorithm parameters
CrossQ:
  learning_rate: 3e-4
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  ent_coef: 0.1
  sigma: 0.5

# ARS algorithm parameters
ARS:
  n_delta: 8
  delta_std: 0.05
  n_top: 4
  learning_rate: 0.01
  zero_policy: false

# TQC algorithm parameters
TQC:
  learning_rate: 3e-4
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  policy_kwargs: null
  top_quantiles_to_drop_per_net: 2
  ent_coef: "auto"
  target_entropy: "auto" 