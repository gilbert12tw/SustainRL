Configuration: 
algo: PPO
timesteps: 10000000
log_dir: logs
seed: 0
num_envs: 16
num_eval_envs: 2
checkpoint_freq: 50000
eval_freq: 5000
eval_episodes_during_training: 5
eval_episodes: 100
test_episodes: 10
PPO:
  learning_rate: 0.0005
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
RPPO:
  learning_rate: 0.0005
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  clip_range_vf: null
SAC:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  ent_coef: auto
  target_update_interval: 1
  target_entropy: auto
TD3:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 100
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  policy_delay: 2
  target_policy_noise: 0.2
  target_noise_clip: 0.5
DDPG:
  learning_rate: 0.001
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 100
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
CrossQ:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  ent_coef: 0.1
  sigma: 0.5
ARS:
  n_delta: 8
  delta_std: 0.05
  n_top: 4
  learning_rate: 0.01
  zero_policy: false
TQC:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  policy_kwargs: null
  top_quantiles_to_drop_per_net: 2
  ent_coef: auto
  target_entropy: auto

Observation space: Box([   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0. -288. -288. -288. -288. -288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.
 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.
 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.
 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 288. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.   1.   1.   1.   1.
   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
   1.   1.   1.   1.   1.   1.], (146,), float32)
Action space: Box(0.0, 1.0, (54,), float32)
Initializing PPO with parameters: {'learning_rate': 0.0005, 'n_steps': 2048, 'batch_size': 64, 'n_epochs': 10, 'gamma': 0.99, 'gae_lambda': 0.95, 'clip_range': 0.2, 'ent_coef': 0.01, 'vf_coef': 0.5, 'max_grad_norm': 0.5}
Using cuda device
Starting training with PPO...
Logging to logs/tensorboard/PPO_3
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.35     |
| time/              |          |
|    fps             | 364      |
|    iterations      | 1        |
|    time_elapsed    | 89       |
|    total_timesteps | 32768    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.25       |
| time/                   |            |
|    fps                  | 261        |
|    iterations           | 2          |
|    time_elapsed         | 250        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.08178195 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.2        |
|    entropy_loss         | -77.1      |
|    explained_variance   | 0.0125     |
|    learning_rate        | 0.0005     |
|    loss                 | -0.717     |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.00868   |
|    std                  | 1.01       |
|    value_loss           | 0.0278     |
----------------------------------------
Eval num_timesteps=80000, episode_reward=0.10 +/- 0.11
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 0.0998     |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.09863953 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.2        |
|    entropy_loss         | -77.6      |
|    explained_variance   | 0.613      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.779     |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00415   |
|    std                  | 1.02       |
|    value_loss           | 0.0242     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.59     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 3        |
|    time_elapsed    | 415      |
|    total_timesteps | 98304    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.82       |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 4          |
|    time_elapsed         | 576        |
|    total_timesteps      | 131072     |
| train/                  |            |
|    approx_kl            | 0.12329931 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.2        |
|    entropy_loss         | -78.2      |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.0005     |
|    loss                 | -0.771     |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.00627   |
|    std                  | 1.03       |
|    value_loss           | 0.0341     |
----------------------------------------
Eval num_timesteps=160000, episode_reward=0.28 +/- 0.11
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 0.281      |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.15783146 |
|    clip_fraction        | 0.548      |
|    clip_range           | 0.2        |
|    entropy_loss         | -78.9      |
|    explained_variance   | 0.829      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.786     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.00503   |
|    std                  | 1.05       |
|    value_loss           | 0.0401     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.85     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 5        |
|    time_elapsed    | 742      |
|    total_timesteps | 163840   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.51       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 6          |
|    time_elapsed         | 902        |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.18574743 |
|    clip_fraction        | 0.564      |
|    clip_range           | 0.2        |
|    entropy_loss         | -79.7      |
|    explained_variance   | 0.845      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.785     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.006     |
|    std                  | 1.06       |
|    value_loss           | 0.0444     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.41       |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 7          |
|    time_elapsed         | 1061       |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.20854372 |
|    clip_fraction        | 0.579      |
|    clip_range           | 0.2        |
|    entropy_loss         | -80.6      |
|    explained_variance   | 0.841      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.832     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.00245   |
|    std                  | 1.08       |
|    value_loss           | 0.0501     |
----------------------------------------
Eval num_timesteps=240000, episode_reward=0.43 +/- 0.17
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 0.431     |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.2271439 |
|    clip_fraction        | 0.589     |
|    clip_range           | 0.2       |
|    entropy_loss         | -81.5     |
|    explained_variance   | 0.845     |
|    learning_rate        | 0.0005    |
|    loss                 | -0.832    |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.00578  |
|    std                  | 1.1       |
|    value_loss           | 0.0533    |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.14     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 8        |
|    time_elapsed    | 1222     |
|    total_timesteps | 262144   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.57       |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 9          |
|    time_elapsed         | 1381       |
|    total_timesteps      | 294912     |
| train/                  |            |
|    approx_kl            | 0.25336915 |
|    clip_fraction        | 0.609      |
|    clip_range           | 0.2        |
|    entropy_loss         | -82.5      |
|    explained_variance   | 0.864      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.898     |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.00247   |
|    std                  | 1.12       |
|    value_loss           | 0.055      |
----------------------------------------
Eval num_timesteps=320000, episode_reward=0.68 +/- 0.33
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 0.682      |
| time/                   |            |
|    total_timesteps      | 320000     |
| train/                  |            |
|    approx_kl            | 0.28114069 |
|    clip_fraction        | 0.608      |
|    clip_range           | 0.2        |
|    entropy_loss         | -83.3      |
|    explained_variance   | 0.862      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.857     |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.00101    |
|    std                  | 1.14       |
|    value_loss           | 0.0606     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.57     |
| time/              |          |
|    fps             | 212      |
|    iterations      | 10       |
|    time_elapsed    | 1545     |
|    total_timesteps | 327680   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.4        |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 11         |
|    time_elapsed         | 1705       |
|    total_timesteps      | 360448     |
| train/                  |            |
|    approx_kl            | 0.28161025 |
|    clip_fraction        | 0.621      |
|    clip_range           | 0.2        |
|    entropy_loss         | -84.4      |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.877     |
|    n_updates            | 100        |
|    policy_gradient_loss | 0.000588   |
|    std                  | 1.16       |
|    value_loss           | 0.06       |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 288      |
|    ep_rew_mean          | 5.36     |
| time/                   |          |
|    fps                  | 210      |
|    iterations           | 12       |
|    time_elapsed         | 1863     |
|    total_timesteps      | 393216   |
| train/                  |          |
|    approx_kl            | 0.293541 |
|    clip_fraction        | 0.629    |
|    clip_range           | 0.2      |
|    entropy_loss         | -85.3    |
|    explained_variance   | 0.856    |
|    learning_rate        | 0.0005   |
|    loss                 | -0.871   |
|    n_updates            | 110      |
|    policy_gradient_loss | 0.00498  |
|    std                  | 1.18     |
|    value_loss           | 0.0605   |
--------------------------------------
Eval num_timesteps=400000, episode_reward=1.02 +/- 0.50
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 1.02      |
| time/                   |           |
|    total_timesteps      | 400000    |
| train/                  |           |
|    approx_kl            | 0.3021242 |
|    clip_fraction        | 0.626     |
|    clip_range           | 0.2       |
|    entropy_loss         | -86.3     |
|    explained_variance   | 0.854     |
|    learning_rate        | 0.0005    |
|    loss                 | -0.885    |
|    n_updates            | 120       |
|    policy_gradient_loss | 0.00345   |
|    std                  | 1.2       |
|    value_loss           | 0.0598    |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.19     |
| time/              |          |
|    fps             | 210      |
|    iterations      | 13       |
|    time_elapsed    | 2026     |
|    total_timesteps | 425984   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.79       |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 14         |
|    time_elapsed         | 2185       |
|    total_timesteps      | 458752     |
| train/                  |            |
|    approx_kl            | 0.30091226 |
|    clip_fraction        | 0.638      |
|    clip_range           | 0.2        |
|    entropy_loss         | -87.4      |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0005     |
|    loss                 | -0.876     |
|    n_updates            | 130        |
|    policy_gradient_loss | 0.00532    |
|    std                  | 1.23       |
|    value_loss           | 0.0578     |
----------------------------------------
Eval num_timesteps=480000, episode_reward=0.95 +/- 0.34
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 0.95       |
| time/                   |            |
|    total_timesteps      | 480000     |
| train/                  |            |
|    approx_kl            | 0.31799427 |
|    clip_fraction        | 0.646      |
|    clip_range           | 0.2        |
|    entropy_loss         | -88.5      |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0005     |
|    loss                 | -0.861     |
|    n_updates            | 140        |
|    policy_gradient_loss | 0.00731    |
|    std                  | 1.25       |
|    value_loss           | 0.0602     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.1      |
| time/              |          |
|    fps             | 209      |
|    iterations      | 15       |
|    time_elapsed    | 2351     |
|    total_timesteps | 491520   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.6        |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 16         |
|    time_elapsed         | 2512       |
|    total_timesteps      | 524288     |
| train/                  |            |
|    approx_kl            | 0.31169736 |
|    clip_fraction        | 0.635      |
|    clip_range           | 0.2        |
|    entropy_loss         | -89.6      |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.849     |
|    n_updates            | 150        |
|    policy_gradient_loss | 0.00867    |
|    std                  | 1.28       |
|    value_loss           | 0.0634     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.89      |
| time/                   |           |
|    fps                  | 208       |
|    iterations           | 17        |
|    time_elapsed         | 2671      |
|    total_timesteps      | 557056    |
| train/                  |           |
|    approx_kl            | 0.3281616 |
|    clip_fraction        | 0.643     |
|    clip_range           | 0.2       |
|    entropy_loss         | -90.7     |
|    explained_variance   | 0.873     |
|    learning_rate        | 0.0005    |
|    loss                 | -0.931    |
|    n_updates            | 160       |
|    policy_gradient_loss | 0.00672   |
|    std                  | 1.3       |
|    value_loss           | 0.057     |
---------------------------------------
Eval num_timesteps=560000, episode_reward=1.07 +/- 0.78
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.07       |
| time/                   |            |
|    total_timesteps      | 560000     |
| train/                  |            |
|    approx_kl            | 0.31581354 |
|    clip_fraction        | 0.651      |
|    clip_range           | 0.2        |
|    entropy_loss         | -91.9      |
|    explained_variance   | 0.88       |
|    learning_rate        | 0.0005     |
|    loss                 | -0.956     |
|    n_updates            | 170        |
|    policy_gradient_loss | 0.00601    |
|    std                  | 1.33       |
|    value_loss           | 0.0557     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.47     |
| time/              |          |
|    fps             | 207      |
|    iterations      | 18       |
|    time_elapsed    | 2837     |
|    total_timesteps | 589824   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.59       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 19         |
|    time_elapsed         | 2999       |
|    total_timesteps      | 622592     |
| train/                  |            |
|    approx_kl            | 0.32373294 |
|    clip_fraction        | 0.647      |
|    clip_range           | 0.2        |
|    entropy_loss         | -93.1      |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.922     |
|    n_updates            | 180        |
|    policy_gradient_loss | 0.00854    |
|    std                  | 1.36       |
|    value_loss           | 0.059      |
----------------------------------------
Eval num_timesteps=640000, episode_reward=1.04 +/- 0.36
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.04       |
| time/                   |            |
|    total_timesteps      | 640000     |
| train/                  |            |
|    approx_kl            | 0.32718173 |
|    clip_fraction        | 0.655      |
|    clip_range           | 0.2        |
|    entropy_loss         | -94.3      |
|    explained_variance   | 0.879      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.868     |
|    n_updates            | 190        |
|    policy_gradient_loss | 0.00875    |
|    std                  | 1.39       |
|    value_loss           | 0.0612     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.99     |
| time/              |          |
|    fps             | 207      |
|    iterations      | 20       |
|    time_elapsed    | 3163     |
|    total_timesteps | 655360   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.63       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 21         |
|    time_elapsed         | 3320       |
|    total_timesteps      | 688128     |
| train/                  |            |
|    approx_kl            | 0.32151997 |
|    clip_fraction        | 0.648      |
|    clip_range           | 0.2        |
|    entropy_loss         | -95.4      |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.989     |
|    n_updates            | 200        |
|    policy_gradient_loss | 0.00609    |
|    std                  | 1.42       |
|    value_loss           | 0.061      |
----------------------------------------
Eval num_timesteps=720000, episode_reward=1.34 +/- 0.92
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.34       |
| time/                   |            |
|    total_timesteps      | 720000     |
| train/                  |            |
|    approx_kl            | 0.31336725 |
|    clip_fraction        | 0.635      |
|    clip_range           | 0.2        |
|    entropy_loss         | -96.4      |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.949     |
|    n_updates            | 210        |
|    policy_gradient_loss | 0.00762    |
|    std                  | 1.45       |
|    value_loss           | 0.0593     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.82     |
| time/              |          |
|    fps             | 206      |
|    iterations      | 22       |
|    time_elapsed    | 3484     |
|    total_timesteps | 720896   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.85       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 23         |
|    time_elapsed         | 3643       |
|    total_timesteps      | 753664     |
| train/                  |            |
|    approx_kl            | 0.31879795 |
|    clip_fraction        | 0.645      |
|    clip_range           | 0.2        |
|    entropy_loss         | -97.5      |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.978     |
|    n_updates            | 220        |
|    policy_gradient_loss | 0.00629    |
|    std                  | 1.48       |
|    value_loss           | 0.0636     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.72      |
| time/                   |           |
|    fps                  | 206       |
|    iterations           | 24        |
|    time_elapsed         | 3802      |
|    total_timesteps      | 786432    |
| train/                  |           |
|    approx_kl            | 0.3013134 |
|    clip_fraction        | 0.652     |
|    clip_range           | 0.2       |
|    entropy_loss         | -98.8     |
|    explained_variance   | 0.879     |
|    learning_rate        | 0.0005    |
|    loss                 | -0.985    |
|    n_updates            | 230       |
|    policy_gradient_loss | 0.00729   |
|    std                  | 1.51      |
|    value_loss           | 0.065     |
---------------------------------------
Eval num_timesteps=800000, episode_reward=1.59 +/- 0.49
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.59       |
| time/                   |            |
|    total_timesteps      | 800000     |
| train/                  |            |
|    approx_kl            | 0.31585622 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.2        |
|    entropy_loss         | -99.8      |
|    explained_variance   | 0.876      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.902     |
|    n_updates            | 240        |
|    policy_gradient_loss | 0.00674    |
|    std                  | 1.54       |
|    value_loss           | 0.0661     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.42     |
| time/              |          |
|    fps             | 206      |
|    iterations      | 25       |
|    time_elapsed    | 3966     |
|    total_timesteps | 819200   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.57      |
| time/                   |           |
|    fps                  | 206       |
|    iterations           | 26        |
|    time_elapsed         | 4127      |
|    total_timesteps      | 851968    |
| train/                  |           |
|    approx_kl            | 0.2974009 |
|    clip_fraction        | 0.648     |
|    clip_range           | 0.2       |
|    entropy_loss         | -101      |
|    explained_variance   | 0.865     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.06     |
|    n_updates            | 250       |
|    policy_gradient_loss | 0.00737   |
|    std                  | 1.58      |
|    value_loss           | 0.0615    |
---------------------------------------
Eval num_timesteps=880000, episode_reward=0.99 +/- 0.46
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 0.995      |
| time/                   |            |
|    total_timesteps      | 880000     |
| train/                  |            |
|    approx_kl            | 0.30809087 |
|    clip_fraction        | 0.647      |
|    clip_range           | 0.2        |
|    entropy_loss         | -102       |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.984     |
|    n_updates            | 260        |
|    policy_gradient_loss | 0.0041     |
|    std                  | 1.61       |
|    value_loss           | 0.0579     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.88     |
| time/              |          |
|    fps             | 206      |
|    iterations      | 27       |
|    time_elapsed    | 4293     |
|    total_timesteps | 884736   |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 288      |
|    ep_rew_mean          | 5.69     |
| time/                   |          |
|    fps                  | 205      |
|    iterations           | 28       |
|    time_elapsed         | 4455     |
|    total_timesteps      | 917504   |
| train/                  |          |
|    approx_kl            | 0.291511 |
|    clip_fraction        | 0.643    |
|    clip_range           | 0.2      |
|    entropy_loss         | -103     |
|    explained_variance   | 0.881    |
|    learning_rate        | 0.0005   |
|    loss                 | -1.08    |
|    n_updates            | 270      |
|    policy_gradient_loss | 0.00587  |
|    std                  | 1.64     |
|    value_loss           | 0.0664   |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.36      |
| time/                   |           |
|    fps                  | 205       |
|    iterations           | 29        |
|    time_elapsed         | 4615      |
|    total_timesteps      | 950272    |
| train/                  |           |
|    approx_kl            | 0.2964538 |
|    clip_fraction        | 0.642     |
|    clip_range           | 0.2       |
|    entropy_loss         | -104      |
|    explained_variance   | 0.886     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.05     |
|    n_updates            | 280       |
|    policy_gradient_loss | 0.00639   |
|    std                  | 1.68      |
|    value_loss           | 0.0629    |
---------------------------------------
Eval num_timesteps=960000, episode_reward=1.42 +/- 0.70
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.42       |
| time/                   |            |
|    total_timesteps      | 960000     |
| train/                  |            |
|    approx_kl            | 0.29811788 |
|    clip_fraction        | 0.642      |
|    clip_range           | 0.2        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.03      |
|    n_updates            | 290        |
|    policy_gradient_loss | 0.0054     |
|    std                  | 1.72       |
|    value_loss           | 0.0714     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.2      |
| time/              |          |
|    fps             | 205      |
|    iterations      | 30       |
|    time_elapsed    | 4777     |
|    total_timesteps | 983040   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.75      |
| time/                   |           |
|    fps                  | 205       |
|    iterations           | 31        |
|    time_elapsed         | 4937      |
|    total_timesteps      | 1015808   |
| train/                  |           |
|    approx_kl            | 0.2787202 |
|    clip_fraction        | 0.625     |
|    clip_range           | 0.2       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.868     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.09     |
|    n_updates            | 300       |
|    policy_gradient_loss | 0.00383   |
|    std                  | 1.75      |
|    value_loss           | 0.0686    |
---------------------------------------
Eval num_timesteps=1040000, episode_reward=1.54 +/- 0.85
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.54       |
| time/                   |            |
|    total_timesteps      | 1040000    |
| train/                  |            |
|    approx_kl            | 0.30017456 |
|    clip_fraction        | 0.646      |
|    clip_range           | 0.2        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.88       |
|    learning_rate        | 0.0005     |
|    loss                 | -1.1       |
|    n_updates            | 310        |
|    policy_gradient_loss | 0.00379    |
|    std                  | 1.79       |
|    value_loss           | 0.0744     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.63     |
| time/              |          |
|    fps             | 205      |
|    iterations      | 32       |
|    time_elapsed    | 5101     |
|    total_timesteps | 1048576  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.87       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 33         |
|    time_elapsed         | 5261       |
|    total_timesteps      | 1081344    |
| train/                  |            |
|    approx_kl            | 0.30107927 |
|    clip_fraction        | 0.642      |
|    clip_range           | 0.2        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.13      |
|    n_updates            | 320        |
|    policy_gradient_loss | 0.00171    |
|    std                  | 1.83       |
|    value_loss           | 0.0659     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.66      |
| time/                   |           |
|    fps                  | 205       |
|    iterations           | 34        |
|    time_elapsed         | 5420      |
|    total_timesteps      | 1114112   |
| train/                  |           |
|    approx_kl            | 0.2740906 |
|    clip_fraction        | 0.633     |
|    clip_range           | 0.2       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.875     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.06     |
|    n_updates            | 330       |
|    policy_gradient_loss | 0.00109   |
|    std                  | 1.86      |
|    value_loss           | 0.0649    |
---------------------------------------
Eval num_timesteps=1120000, episode_reward=2.32 +/- 1.20
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.32       |
| time/                   |            |
|    total_timesteps      | 1120000    |
| train/                  |            |
|    approx_kl            | 0.27921146 |
|    clip_fraction        | 0.639      |
|    clip_range           | 0.2        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.13      |
|    n_updates            | 340        |
|    policy_gradient_loss | 0.000213   |
|    std                  | 1.9        |
|    value_loss           | 0.0679     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.13     |
| time/              |          |
|    fps             | 205      |
|    iterations      | 35       |
|    time_elapsed    | 5584     |
|    total_timesteps | 1146880  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.63      |
| time/                   |           |
|    fps                  | 205       |
|    iterations           | 36        |
|    time_elapsed         | 5743      |
|    total_timesteps      | 1179648   |
| train/                  |           |
|    approx_kl            | 0.2759956 |
|    clip_fraction        | 0.645     |
|    clip_range           | 0.2       |
|    entropy_loss         | -112      |
|    explained_variance   | 0.87      |
|    learning_rate        | 0.0005    |
|    loss                 | -1.13     |
|    n_updates            | 350       |
|    policy_gradient_loss | 0.00479   |
|    std                  | 1.94      |
|    value_loss           | 0.0764    |
---------------------------------------
Eval num_timesteps=1200000, episode_reward=1.55 +/- 0.26
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.55       |
| time/                   |            |
|    total_timesteps      | 1200000    |
| train/                  |            |
|    approx_kl            | 0.26316994 |
|    clip_fraction        | 0.62       |
|    clip_range           | 0.2        |
|    entropy_loss         | -113       |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.15      |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.00166   |
|    std                  | 1.98       |
|    value_loss           | 0.0734     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.69     |
| time/              |          |
|    fps             | 205      |
|    iterations      | 37       |
|    time_elapsed    | 5908     |
|    total_timesteps | 1212416  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 288      |
|    ep_rew_mean          | 6.12     |
| time/                   |          |
|    fps                  | 205      |
|    iterations           | 38       |
|    time_elapsed         | 6066     |
|    total_timesteps      | 1245184  |
| train/                  |          |
|    approx_kl            | 0.27099  |
|    clip_fraction        | 0.628    |
|    clip_range           | 0.2      |
|    entropy_loss         | -114     |
|    explained_variance   | 0.874    |
|    learning_rate        | 0.0005   |
|    loss                 | -1.14    |
|    n_updates            | 370      |
|    policy_gradient_loss | -0.00212 |
|    std                  | 2.02     |
|    value_loss           | 0.0738   |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.56      |
| time/                   |           |
|    fps                  | 205       |
|    iterations           | 39        |
|    time_elapsed         | 6225      |
|    total_timesteps      | 1277952   |
| train/                  |           |
|    approx_kl            | 0.2741452 |
|    clip_fraction        | 0.636     |
|    clip_range           | 0.2       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.879     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.18     |
|    n_updates            | 380       |
|    policy_gradient_loss | -0.000271 |
|    std                  | 2.06      |
|    value_loss           | 0.0799    |
---------------------------------------
Eval num_timesteps=1280000, episode_reward=2.48 +/- 1.25
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.48       |
| time/                   |            |
|    total_timesteps      | 1280000    |
| train/                  |            |
|    approx_kl            | 0.25509936 |
|    clip_fraction        | 0.613      |
|    clip_range           | 0.2        |
|    entropy_loss         | -116       |
|    explained_variance   | 0.877      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.1       |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.00361   |
|    std                  | 2.1        |
|    value_loss           | 0.0707     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.41     |
| time/              |          |
|    fps             | 205      |
|    iterations      | 40       |
|    time_elapsed    | 6390     |
|    total_timesteps | 1310720  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.66       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 41         |
|    time_elapsed         | 6550       |
|    total_timesteps      | 1343488    |
| train/                  |            |
|    approx_kl            | 0.26426637 |
|    clip_fraction        | 0.631      |
|    clip_range           | 0.2        |
|    entropy_loss         | -117       |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.15      |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.00145   |
|    std                  | 2.14       |
|    value_loss           | 0.0777     |
----------------------------------------
Eval num_timesteps=1360000, episode_reward=2.31 +/- 1.43
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.31       |
| time/                   |            |
|    total_timesteps      | 1360000    |
| train/                  |            |
|    approx_kl            | 0.24975562 |
|    clip_fraction        | 0.621      |
|    clip_range           | 0.2        |
|    entropy_loss         | -118       |
|    explained_variance   | 0.884      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.14      |
|    n_updates            | 410        |
|    policy_gradient_loss | 0.00057    |
|    std                  | 2.18       |
|    value_loss           | 0.0745     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.71     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 42       |
|    time_elapsed    | 6714     |
|    total_timesteps | 1376256  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 288      |
|    ep_rew_mean          | 6.36     |
| time/                   |          |
|    fps                  | 205      |
|    iterations           | 43       |
|    time_elapsed         | 6872     |
|    total_timesteps      | 1409024  |
| train/                  |          |
|    approx_kl            | 0.250628 |
|    clip_fraction        | 0.617    |
|    clip_range           | 0.2      |
|    entropy_loss         | -119     |
|    explained_variance   | 0.872    |
|    learning_rate        | 0.0005   |
|    loss                 | -1.16    |
|    n_updates            | 420      |
|    policy_gradient_loss | -0.00162 |
|    std                  | 2.22     |
|    value_loss           | 0.0858   |
--------------------------------------
Eval num_timesteps=1440000, episode_reward=1.68 +/- 0.54
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.68       |
| time/                   |            |
|    total_timesteps      | 1440000    |
| train/                  |            |
|    approx_kl            | 0.26912278 |
|    clip_fraction        | 0.635      |
|    clip_range           | 0.2        |
|    entropy_loss         | -121       |
|    explained_variance   | 0.888      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.2       |
|    n_updates            | 430        |
|    policy_gradient_loss | -0.00021   |
|    std                  | 2.27       |
|    value_loss           | 0.0775     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.75     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 44       |
|    time_elapsed    | 7037     |
|    total_timesteps | 1441792  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.97      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 45        |
|    time_elapsed         | 7198      |
|    total_timesteps      | 1474560   |
| train/                  |           |
|    approx_kl            | 0.2513206 |
|    clip_fraction        | 0.625     |
|    clip_range           | 0.2       |
|    entropy_loss         | -122      |
|    explained_variance   | 0.887     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.21     |
|    n_updates            | 440       |
|    policy_gradient_loss | -0.000758 |
|    std                  | 2.31      |
|    value_loss           | 0.0835    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.72       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 46         |
|    time_elapsed         | 7357       |
|    total_timesteps      | 1507328    |
| train/                  |            |
|    approx_kl            | 0.23760912 |
|    clip_fraction        | 0.605      |
|    clip_range           | 0.2        |
|    entropy_loss         | -122       |
|    explained_variance   | 0.885      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.24      |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.00585   |
|    std                  | 2.35       |
|    value_loss           | 0.0747     |
----------------------------------------
Eval num_timesteps=1520000, episode_reward=2.23 +/- 0.68
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.23       |
| time/                   |            |
|    total_timesteps      | 1520000    |
| train/                  |            |
|    approx_kl            | 0.23527525 |
|    clip_fraction        | 0.617      |
|    clip_range           | 0.2        |
|    entropy_loss         | -123       |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.18      |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.00666   |
|    std                  | 2.39       |
|    value_loss           | 0.081      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.61     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 47       |
|    time_elapsed    | 7520     |
|    total_timesteps | 1540096  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.69       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 48         |
|    time_elapsed         | 7676       |
|    total_timesteps      | 1572864    |
| train/                  |            |
|    approx_kl            | 0.23449755 |
|    clip_fraction        | 0.616      |
|    clip_range           | 0.2        |
|    entropy_loss         | -125       |
|    explained_variance   | 0.878      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.27      |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.00223   |
|    std                  | 2.44       |
|    value_loss           | 0.0728     |
----------------------------------------
Eval num_timesteps=1600000, episode_reward=1.87 +/- 1.00
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.87       |
| time/                   |            |
|    total_timesteps      | 1600000    |
| train/                  |            |
|    approx_kl            | 0.22201794 |
|    clip_fraction        | 0.596      |
|    clip_range           | 0.2        |
|    entropy_loss         | -125       |
|    explained_variance   | 0.883      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.26      |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.00519   |
|    std                  | 2.48       |
|    value_loss           | 0.073      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.78     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 49       |
|    time_elapsed    | 7839     |
|    total_timesteps | 1605632  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.73       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 50         |
|    time_elapsed         | 7999       |
|    total_timesteps      | 1638400    |
| train/                  |            |
|    approx_kl            | 0.21950683 |
|    clip_fraction        | 0.609      |
|    clip_range           | 0.2        |
|    entropy_loss         | -126       |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.26      |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.00514   |
|    std                  | 2.53       |
|    value_loss           | 0.089      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.8        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 51         |
|    time_elapsed         | 8160       |
|    total_timesteps      | 1671168    |
| train/                  |            |
|    approx_kl            | 0.24321315 |
|    clip_fraction        | 0.62       |
|    clip_range           | 0.2        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.22      |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.00609   |
|    std                  | 2.58       |
|    value_loss           | 0.0882     |
----------------------------------------
Eval num_timesteps=1680000, episode_reward=2.07 +/- 1.00
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.07       |
| time/                   |            |
|    total_timesteps      | 1680000    |
| train/                  |            |
|    approx_kl            | 0.24263915 |
|    clip_fraction        | 0.603      |
|    clip_range           | 0.2        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.25      |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.00644   |
|    std                  | 2.63       |
|    value_loss           | 0.0955     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.04     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 52       |
|    time_elapsed    | 8324     |
|    total_timesteps | 1703936  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.99       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 53         |
|    time_elapsed         | 8485       |
|    total_timesteps      | 1736704    |
| train/                  |            |
|    approx_kl            | 0.21735786 |
|    clip_fraction        | 0.594      |
|    clip_range           | 0.2        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.861      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.33      |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.0106    |
|    std                  | 2.67       |
|    value_loss           | 0.0834     |
----------------------------------------
Eval num_timesteps=1760000, episode_reward=1.40 +/- 0.85
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.4        |
| time/                   |            |
|    total_timesteps      | 1760000    |
| train/                  |            |
|    approx_kl            | 0.23512791 |
|    clip_fraction        | 0.613      |
|    clip_range           | 0.2        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.877      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.3       |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.0058    |
|    std                  | 2.72       |
|    value_loss           | 0.0879     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.3      |
| time/              |          |
|    fps             | 204      |
|    iterations      | 54       |
|    time_elapsed    | 8650     |
|    total_timesteps | 1769472  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.1        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 55         |
|    time_elapsed         | 8809       |
|    total_timesteps      | 1802240    |
| train/                  |            |
|    approx_kl            | 0.21691512 |
|    clip_fraction        | 0.602      |
|    clip_range           | 0.2        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.32      |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.00743   |
|    std                  | 2.78       |
|    value_loss           | 0.0868     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.26       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 56         |
|    time_elapsed         | 8970       |
|    total_timesteps      | 1835008    |
| train/                  |            |
|    approx_kl            | 0.21121934 |
|    clip_fraction        | 0.594      |
|    clip_range           | 0.2        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.37      |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.00522   |
|    std                  | 2.83       |
|    value_loss           | 0.0761     |
----------------------------------------
Eval num_timesteps=1840000, episode_reward=2.06 +/- 0.75
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.06       |
| time/                   |            |
|    total_timesteps      | 1840000    |
| train/                  |            |
|    approx_kl            | 0.20158322 |
|    clip_fraction        | 0.592      |
|    clip_range           | 0.2        |
|    entropy_loss         | -133       |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0005     |
|    loss                 | -1.32      |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.00753   |
|    std                  | 2.87       |
|    value_loss           | 0.0821     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.53     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 57       |
|    time_elapsed    | 9134     |
|    total_timesteps | 1867776  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.27       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 58         |
|    time_elapsed         | 9294       |
|    total_timesteps      | 1900544    |
| train/                  |            |
|    approx_kl            | 0.20715778 |
|    clip_fraction        | 0.601      |
|    clip_range           | 0.2        |
|    entropy_loss         | -134       |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.3       |
|    n_updates            | 570        |
|    policy_gradient_loss | -0.00697   |
|    std                  | 2.92       |
|    value_loss           | 0.0814     |
----------------------------------------
Eval num_timesteps=1920000, episode_reward=2.37 +/- 0.66
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.37       |
| time/                   |            |
|    total_timesteps      | 1920000    |
| train/                  |            |
|    approx_kl            | 0.19916087 |
|    clip_fraction        | 0.594      |
|    clip_range           | 0.2        |
|    entropy_loss         | -135       |
|    explained_variance   | 0.877      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.33      |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.00857   |
|    std                  | 2.98       |
|    value_loss           | 0.0881     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.11     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 59       |
|    time_elapsed    | 9457     |
|    total_timesteps | 1933312  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.05       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 60         |
|    time_elapsed         | 9617       |
|    total_timesteps      | 1966080    |
| train/                  |            |
|    approx_kl            | 0.19348055 |
|    clip_fraction        | 0.583      |
|    clip_range           | 0.2        |
|    entropy_loss         | -136       |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.39      |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.0105    |
|    std                  | 3.03       |
|    value_loss           | 0.0832     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.32      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 61        |
|    time_elapsed         | 9775      |
|    total_timesteps      | 1998848   |
| train/                  |           |
|    approx_kl            | 0.1945681 |
|    clip_fraction        | 0.592     |
|    clip_range           | 0.2       |
|    entropy_loss         | -137      |
|    explained_variance   | 0.872     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.4      |
|    n_updates            | 600       |
|    policy_gradient_loss | -0.00942  |
|    std                  | 3.08      |
|    value_loss           | 0.0845    |
---------------------------------------
Eval num_timesteps=2000000, episode_reward=2.12 +/- 0.35
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 2.12      |
| time/                   |           |
|    total_timesteps      | 2000000   |
| train/                  |           |
|    approx_kl            | 0.1987194 |
|    clip_fraction        | 0.589     |
|    clip_range           | 0.2       |
|    entropy_loss         | -138      |
|    explained_variance   | 0.876     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.34     |
|    n_updates            | 610       |
|    policy_gradient_loss | -0.00964  |
|    std                  | 3.14      |
|    value_loss           | 0.0787    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.64     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 62       |
|    time_elapsed    | 9940     |
|    total_timesteps | 2031616  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.48       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 63         |
|    time_elapsed         | 10099      |
|    total_timesteps      | 2064384    |
| train/                  |            |
|    approx_kl            | 0.19076693 |
|    clip_fraction        | 0.586      |
|    clip_range           | 0.2        |
|    entropy_loss         | -139       |
|    explained_variance   | 0.879      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.4       |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.00724   |
|    std                  | 3.19       |
|    value_loss           | 0.0858     |
----------------------------------------
Eval num_timesteps=2080000, episode_reward=1.54 +/- 0.52
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.54       |
| time/                   |            |
|    total_timesteps      | 2080000    |
| train/                  |            |
|    approx_kl            | 0.18155426 |
|    clip_fraction        | 0.563      |
|    clip_range           | 0.2        |
|    entropy_loss         | -140       |
|    explained_variance   | 0.884      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.39      |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.0126    |
|    std                  | 3.24       |
|    value_loss           | 0.0712     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.22     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 64       |
|    time_elapsed    | 10264    |
|    total_timesteps | 2097152  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.07       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 65         |
|    time_elapsed         | 10424      |
|    total_timesteps      | 2129920    |
| train/                  |            |
|    approx_kl            | 0.18719971 |
|    clip_fraction        | 0.567      |
|    clip_range           | 0.2        |
|    entropy_loss         | -141       |
|    explained_variance   | 0.877      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.39      |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.00921   |
|    std                  | 3.3        |
|    value_loss           | 0.0849     |
----------------------------------------
Eval num_timesteps=2160000, episode_reward=1.83 +/- 0.71
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.83       |
| time/                   |            |
|    total_timesteps      | 2160000    |
| train/                  |            |
|    approx_kl            | 0.18679601 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.2        |
|    entropy_loss         | -142       |
|    explained_variance   | 0.879      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.42      |
|    n_updates            | 650        |
|    policy_gradient_loss | -0.0108    |
|    std                  | 3.35       |
|    value_loss           | 0.0898     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.52     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 66       |
|    time_elapsed    | 10589    |
|    total_timesteps | 2162688  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.8        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 67         |
|    time_elapsed         | 10746      |
|    total_timesteps      | 2195456    |
| train/                  |            |
|    approx_kl            | 0.19649681 |
|    clip_fraction        | 0.589      |
|    clip_range           | 0.2        |
|    entropy_loss         | -143       |
|    explained_variance   | 0.89       |
|    learning_rate        | 0.0005     |
|    loss                 | -1.49      |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0101    |
|    std                  | 3.42       |
|    value_loss           | 0.0809     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.83       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 68         |
|    time_elapsed         | 10903      |
|    total_timesteps      | 2228224    |
| train/                  |            |
|    approx_kl            | 0.18304986 |
|    clip_fraction        | 0.571      |
|    clip_range           | 0.2        |
|    entropy_loss         | -144       |
|    explained_variance   | 0.888      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.44      |
|    n_updates            | 670        |
|    policy_gradient_loss | -0.00947   |
|    std                  | 3.47       |
|    value_loss           | 0.0817     |
----------------------------------------
Eval num_timesteps=2240000, episode_reward=1.68 +/- 0.63
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.68       |
| time/                   |            |
|    total_timesteps      | 2240000    |
| train/                  |            |
|    approx_kl            | 0.17348295 |
|    clip_fraction        | 0.571      |
|    clip_range           | 0.2        |
|    entropy_loss         | -144       |
|    explained_variance   | 0.881      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.44      |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.01      |
|    std                  | 3.53       |
|    value_loss           | 0.0849     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.1      |
| time/              |          |
|    fps             | 204      |
|    iterations      | 69       |
|    time_elapsed    | 11066    |
|    total_timesteps | 2260992  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.61      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 70        |
|    time_elapsed         | 11226     |
|    total_timesteps      | 2293760   |
| train/                  |           |
|    approx_kl            | 0.1754027 |
|    clip_fraction        | 0.574     |
|    clip_range           | 0.2       |
|    entropy_loss         | -145      |
|    explained_variance   | 0.892     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.45     |
|    n_updates            | 690       |
|    policy_gradient_loss | -0.00938  |
|    std                  | 3.59      |
|    value_loss           | 0.0894    |
---------------------------------------
Eval num_timesteps=2320000, episode_reward=2.53 +/- 0.68
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.53       |
| time/                   |            |
|    total_timesteps      | 2320000    |
| train/                  |            |
|    approx_kl            | 0.18002346 |
|    clip_fraction        | 0.563      |
|    clip_range           | 0.2        |
|    entropy_loss         | -146       |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.47      |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0119    |
|    std                  | 3.64       |
|    value_loss           | 0.0838     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.33     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 71       |
|    time_elapsed    | 11393    |
|    total_timesteps | 2326528  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.44       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 72         |
|    time_elapsed         | 11551      |
|    total_timesteps      | 2359296    |
| train/                  |            |
|    approx_kl            | 0.17558546 |
|    clip_fraction        | 0.569      |
|    clip_range           | 0.2        |
|    entropy_loss         | -147       |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.4       |
|    n_updates            | 710        |
|    policy_gradient_loss | -0.00949   |
|    std                  | 3.7        |
|    value_loss           | 0.0926     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.08       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 73         |
|    time_elapsed         | 11710      |
|    total_timesteps      | 2392064    |
| train/                  |            |
|    approx_kl            | 0.17544214 |
|    clip_fraction        | 0.57       |
|    clip_range           | 0.2        |
|    entropy_loss         | -148       |
|    explained_variance   | 0.9        |
|    learning_rate        | 0.0005     |
|    loss                 | -1.5       |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.00881   |
|    std                  | 3.76       |
|    value_loss           | 0.0916     |
----------------------------------------
Eval num_timesteps=2400000, episode_reward=1.62 +/- 0.62
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.62       |
| time/                   |            |
|    total_timesteps      | 2400000    |
| train/                  |            |
|    approx_kl            | 0.18533807 |
|    clip_fraction        | 0.565      |
|    clip_range           | 0.2        |
|    entropy_loss         | -149       |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.48      |
|    n_updates            | 730        |
|    policy_gradient_loss | -0.00932   |
|    std                  | 3.81       |
|    value_loss           | 0.0851     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.78     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 74       |
|    time_elapsed    | 11872    |
|    total_timesteps | 2424832  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.68       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 75         |
|    time_elapsed         | 12030      |
|    total_timesteps      | 2457600    |
| train/                  |            |
|    approx_kl            | 0.16234238 |
|    clip_fraction        | 0.558      |
|    clip_range           | 0.2        |
|    entropy_loss         | -149       |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.51      |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0126    |
|    std                  | 3.87       |
|    value_loss           | 0.079      |
----------------------------------------
Eval num_timesteps=2480000, episode_reward=2.12 +/- 0.69
Episode length: 288.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 288      |
|    mean_reward          | 2.12     |
| time/                   |          |
|    total_timesteps      | 2480000  |
| train/                  |          |
|    approx_kl            | 0.167918 |
|    clip_fraction        | 0.557    |
|    clip_range           | 0.2      |
|    entropy_loss         | -150     |
|    explained_variance   | 0.908    |
|    learning_rate        | 0.0005   |
|    loss                 | -1.51    |
|    n_updates            | 750      |
|    policy_gradient_loss | -0.0107  |
|    std                  | 3.92     |
|    value_loss           | 0.0794   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.36     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 76       |
|    time_elapsed    | 12192    |
|    total_timesteps | 2490368  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.54       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 77         |
|    time_elapsed         | 12349      |
|    total_timesteps      | 2523136    |
| train/                  |            |
|    approx_kl            | 0.15844828 |
|    clip_fraction        | 0.551      |
|    clip_range           | 0.2        |
|    entropy_loss         | -151       |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.42      |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.00984   |
|    std                  | 3.98       |
|    value_loss           | 0.0824     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.33       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 78         |
|    time_elapsed         | 12503      |
|    total_timesteps      | 2555904    |
| train/                  |            |
|    approx_kl            | 0.16379982 |
|    clip_fraction        | 0.559      |
|    clip_range           | 0.2        |
|    entropy_loss         | -152       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.55      |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.0102    |
|    std                  | 4.05       |
|    value_loss           | 0.0783     |
----------------------------------------
Eval num_timesteps=2560000, episode_reward=1.51 +/- 1.17
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.51       |
| time/                   |            |
|    total_timesteps      | 2560000    |
| train/                  |            |
|    approx_kl            | 0.16426322 |
|    clip_fraction        | 0.545      |
|    clip_range           | 0.2        |
|    entropy_loss         | -153       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.53      |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.0113    |
|    std                  | 4.1        |
|    value_loss           | 0.0774     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.78     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 79       |
|    time_elapsed    | 12664    |
|    total_timesteps | 2588672  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.13       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 80         |
|    time_elapsed         | 12820      |
|    total_timesteps      | 2621440    |
| train/                  |            |
|    approx_kl            | 0.16128413 |
|    clip_fraction        | 0.551      |
|    clip_range           | 0.2        |
|    entropy_loss         | -153       |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.56      |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.0107    |
|    std                  | 4.16       |
|    value_loss           | 0.0748     |
----------------------------------------
Eval num_timesteps=2640000, episode_reward=2.71 +/- 1.01
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.71       |
| time/                   |            |
|    total_timesteps      | 2640000    |
| train/                  |            |
|    approx_kl            | 0.15911344 |
|    clip_fraction        | 0.549      |
|    clip_range           | 0.2        |
|    entropy_loss         | -154       |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0005     |
|    loss                 | -1.55      |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0114    |
|    std                  | 4.22       |
|    value_loss           | 0.0736     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.31     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 81       |
|    time_elapsed    | 12983    |
|    total_timesteps | 2654208  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.95       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 82         |
|    time_elapsed         | 13139      |
|    total_timesteps      | 2686976    |
| train/                  |            |
|    approx_kl            | 0.15069717 |
|    clip_fraction        | 0.53       |
|    clip_range           | 0.2        |
|    entropy_loss         | -155       |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.55      |
|    n_updates            | 810        |
|    policy_gradient_loss | -0.0114    |
|    std                  | 4.27       |
|    value_loss           | 0.0745     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.14       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 83         |
|    time_elapsed         | 13297      |
|    total_timesteps      | 2719744    |
| train/                  |            |
|    approx_kl            | 0.16003674 |
|    clip_fraction        | 0.543      |
|    clip_range           | 0.2        |
|    entropy_loss         | -156       |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.59      |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0107    |
|    std                  | 4.33       |
|    value_loss           | 0.0713     |
----------------------------------------
Eval num_timesteps=2720000, episode_reward=1.69 +/- 0.92
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.69       |
| time/                   |            |
|    total_timesteps      | 2720000    |
| train/                  |            |
|    approx_kl            | 0.14351244 |
|    clip_fraction        | 0.533      |
|    clip_range           | 0.2        |
|    entropy_loss         | -156       |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.55      |
|    n_updates            | 830        |
|    policy_gradient_loss | -0.0117    |
|    std                  | 4.39       |
|    value_loss           | 0.0743     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.62     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 84       |
|    time_elapsed    | 13460    |
|    total_timesteps | 2752512  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.11       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 85         |
|    time_elapsed         | 13620      |
|    total_timesteps      | 2785280    |
| train/                  |            |
|    approx_kl            | 0.15532193 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.2        |
|    entropy_loss         | -157       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.61      |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.0111    |
|    std                  | 4.44       |
|    value_loss           | 0.0844     |
----------------------------------------
Eval num_timesteps=2800000, episode_reward=1.94 +/- 0.82
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.94       |
| time/                   |            |
|    total_timesteps      | 2800000    |
| train/                  |            |
|    approx_kl            | 0.14951065 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.2        |
|    entropy_loss         | -158       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.54      |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.011     |
|    std                  | 4.5        |
|    value_loss           | 0.079      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.08     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 86       |
|    time_elapsed    | 13785    |
|    total_timesteps | 2818048  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.28       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 87         |
|    time_elapsed         | 13946      |
|    total_timesteps      | 2850816    |
| train/                  |            |
|    approx_kl            | 0.14370942 |
|    clip_fraction        | 0.53       |
|    clip_range           | 0.2        |
|    entropy_loss         | -158       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.59      |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.0122    |
|    std                  | 4.56       |
|    value_loss           | 0.0733     |
----------------------------------------
Eval num_timesteps=2880000, episode_reward=1.74 +/- 0.78
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.74       |
| time/                   |            |
|    total_timesteps      | 2880000    |
| train/                  |            |
|    approx_kl            | 0.14524737 |
|    clip_fraction        | 0.537      |
|    clip_range           | 0.2        |
|    entropy_loss         | -159       |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0005     |
|    loss                 | -1.61      |
|    n_updates            | 870        |
|    policy_gradient_loss | -0.00907   |
|    std                  | 4.62       |
|    value_loss           | 0.0759     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.01     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 88       |
|    time_elapsed    | 14110    |
|    total_timesteps | 2883584  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.7        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 89         |
|    time_elapsed         | 14269      |
|    total_timesteps      | 2916352    |
| train/                  |            |
|    approx_kl            | 0.14074618 |
|    clip_fraction        | 0.508      |
|    clip_range           | 0.2        |
|    entropy_loss         | -160       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.61      |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.0117    |
|    std                  | 4.67       |
|    value_loss           | 0.0709     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.75       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 90         |
|    time_elapsed         | 14429      |
|    total_timesteps      | 2949120    |
| train/                  |            |
|    approx_kl            | 0.13998714 |
|    clip_fraction        | 0.517      |
|    clip_range           | 0.2        |
|    entropy_loss         | -160       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.61      |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.0147    |
|    std                  | 4.73       |
|    value_loss           | 0.0715     |
----------------------------------------
Eval num_timesteps=2960000, episode_reward=2.27 +/- 0.99
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.27       |
| time/                   |            |
|    total_timesteps      | 2960000    |
| train/                  |            |
|    approx_kl            | 0.14580877 |
|    clip_fraction        | 0.517      |
|    clip_range           | 0.2        |
|    entropy_loss         | -161       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.54      |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.0113    |
|    std                  | 4.79       |
|    value_loss           | 0.0757     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.57     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 91       |
|    time_elapsed    | 14594    |
|    total_timesteps | 2981888  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.41       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 92         |
|    time_elapsed         | 14752      |
|    total_timesteps      | 3014656    |
| train/                  |            |
|    approx_kl            | 0.13641793 |
|    clip_fraction        | 0.519      |
|    clip_range           | 0.2        |
|    entropy_loss         | -162       |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0005     |
|    loss                 | -1.64      |
|    n_updates            | 910        |
|    policy_gradient_loss | -0.0116    |
|    std                  | 4.85       |
|    value_loss           | 0.0853     |
----------------------------------------
Eval num_timesteps=3040000, episode_reward=1.79 +/- 0.97
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.79       |
| time/                   |            |
|    total_timesteps      | 3040000    |
| train/                  |            |
|    approx_kl            | 0.14574608 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.2        |
|    entropy_loss         | -162       |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.6       |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.0111    |
|    std                  | 4.92       |
|    value_loss           | 0.0776     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.13     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 93       |
|    time_elapsed    | 14915    |
|    total_timesteps | 3047424  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.7        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 94         |
|    time_elapsed         | 15072      |
|    total_timesteps      | 3080192    |
| train/                  |            |
|    approx_kl            | 0.14179492 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.2        |
|    entropy_loss         | -163       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.64      |
|    n_updates            | 930        |
|    policy_gradient_loss | -0.0144    |
|    std                  | 4.98       |
|    value_loss           | 0.0831     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.99       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 95         |
|    time_elapsed         | 15229      |
|    total_timesteps      | 3112960    |
| train/                  |            |
|    approx_kl            | 0.14019838 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.2        |
|    entropy_loss         | -164       |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.6       |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.0112    |
|    std                  | 5.03       |
|    value_loss           | 0.0824     |
----------------------------------------
Eval num_timesteps=3120000, episode_reward=2.91 +/- 0.88
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.91       |
| time/                   |            |
|    total_timesteps      | 3120000    |
| train/                  |            |
|    approx_kl            | 0.13708745 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.2        |
|    entropy_loss         | -164       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.67      |
|    n_updates            | 950        |
|    policy_gradient_loss | -0.0132    |
|    std                  | 5.09       |
|    value_loss           | 0.0886     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.6      |
| time/              |          |
|    fps             | 204      |
|    iterations      | 96       |
|    time_elapsed    | 15393    |
|    total_timesteps | 3145728  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.33       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 97         |
|    time_elapsed         | 15549      |
|    total_timesteps      | 3178496    |
| train/                  |            |
|    approx_kl            | 0.14552808 |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.2        |
|    entropy_loss         | -165       |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.67      |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.0109    |
|    std                  | 5.16       |
|    value_loss           | 0.0776     |
----------------------------------------
Eval num_timesteps=3200000, episode_reward=1.63 +/- 0.58
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.63       |
| time/                   |            |
|    total_timesteps      | 3200000    |
| train/                  |            |
|    approx_kl            | 0.14319786 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.2        |
|    entropy_loss         | -166       |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.69      |
|    n_updates            | 970        |
|    policy_gradient_loss | -0.0123    |
|    std                  | 5.22       |
|    value_loss           | 0.0745     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.14     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 98       |
|    time_elapsed    | 15711    |
|    total_timesteps | 3211264  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.23       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 99         |
|    time_elapsed         | 15870      |
|    total_timesteps      | 3244032    |
| train/                  |            |
|    approx_kl            | 0.13273361 |
|    clip_fraction        | 0.498      |
|    clip_range           | 0.2        |
|    entropy_loss         | -166       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.63      |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.013     |
|    std                  | 5.29       |
|    value_loss           | 0.0735     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.89       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 100        |
|    time_elapsed         | 16029      |
|    total_timesteps      | 3276800    |
| train/                  |            |
|    approx_kl            | 0.13063848 |
|    clip_fraction        | 0.506      |
|    clip_range           | 0.2        |
|    entropy_loss         | -167       |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.74      |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.0126    |
|    std                  | 5.34       |
|    value_loss           | 0.0678     |
----------------------------------------
Eval num_timesteps=3280000, episode_reward=1.25 +/- 0.70
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.25       |
| time/                   |            |
|    total_timesteps      | 3280000    |
| train/                  |            |
|    approx_kl            | 0.13553149 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.2        |
|    entropy_loss         | -168       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.64      |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.0127    |
|    std                  | 5.41       |
|    value_loss           | 0.0746     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.11     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 101      |
|    time_elapsed    | 16191    |
|    total_timesteps | 3309568  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.27       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 102        |
|    time_elapsed         | 16348      |
|    total_timesteps      | 3342336    |
| train/                  |            |
|    approx_kl            | 0.12408486 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.2        |
|    entropy_loss         | -168       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.71      |
|    n_updates            | 1010       |
|    policy_gradient_loss | -0.0118    |
|    std                  | 5.47       |
|    value_loss           | 0.0752     |
----------------------------------------
Eval num_timesteps=3360000, episode_reward=1.56 +/- 0.54
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.56       |
| time/                   |            |
|    total_timesteps      | 3360000    |
| train/                  |            |
|    approx_kl            | 0.12696102 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.2        |
|    entropy_loss         | -169       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.69      |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.0162    |
|    std                  | 5.54       |
|    value_loss           | 0.0796     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.21     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 103      |
|    time_elapsed    | 16509    |
|    total_timesteps | 3375104  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 4.11      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 104       |
|    time_elapsed         | 16667     |
|    total_timesteps      | 3407872   |
| train/                  |           |
|    approx_kl            | 0.1316942 |
|    clip_fraction        | 0.497     |
|    clip_range           | 0.2       |
|    entropy_loss         | -170      |
|    explained_variance   | 0.91      |
|    learning_rate        | 0.0005    |
|    loss                 | -1.67     |
|    n_updates            | 1030      |
|    policy_gradient_loss | -0.0103   |
|    std                  | 5.62      |
|    value_loss           | 0.0934    |
---------------------------------------
Eval num_timesteps=3440000, episode_reward=2.75 +/- 0.92
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.75       |
| time/                   |            |
|    total_timesteps      | 3440000    |
| train/                  |            |
|    approx_kl            | 0.12188867 |
|    clip_fraction        | 0.499      |
|    clip_range           | 0.2        |
|    entropy_loss         | -170       |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.7       |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.0135    |
|    std                  | 5.69       |
|    value_loss           | 0.075      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.04     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 105      |
|    time_elapsed    | 16829    |
|    total_timesteps | 3440640  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.7        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 106        |
|    time_elapsed         | 16986      |
|    total_timesteps      | 3473408    |
| train/                  |            |
|    approx_kl            | 0.12684807 |
|    clip_fraction        | 0.508      |
|    clip_range           | 0.2        |
|    entropy_loss         | -171       |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.69      |
|    n_updates            | 1050       |
|    policy_gradient_loss | -0.0136    |
|    std                  | 5.77       |
|    value_loss           | 0.081      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.66        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 107         |
|    time_elapsed         | 17144       |
|    total_timesteps      | 3506176     |
| train/                  |             |
|    approx_kl            | 0.124951184 |
|    clip_fraction        | 0.501       |
|    clip_range           | 0.2         |
|    entropy_loss         | -172        |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0005      |
|    loss                 | -1.72       |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.0113     |
|    std                  | 5.83        |
|    value_loss           | 0.0786      |
-----------------------------------------
Eval num_timesteps=3520000, episode_reward=1.62 +/- 0.20
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.62        |
| time/                   |             |
|    total_timesteps      | 3520000     |
| train/                  |             |
|    approx_kl            | 0.121712625 |
|    clip_fraction        | 0.483       |
|    clip_range           | 0.2         |
|    entropy_loss         | -172        |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0005      |
|    loss                 | -1.73       |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.0142     |
|    std                  | 5.9         |
|    value_loss           | 0.082       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.13     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 108      |
|    time_elapsed    | 17309    |
|    total_timesteps | 3538944  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.2       |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 109       |
|    time_elapsed         | 17467     |
|    total_timesteps      | 3571712   |
| train/                  |           |
|    approx_kl            | 0.1256364 |
|    clip_fraction        | 0.49      |
|    clip_range           | 0.2       |
|    entropy_loss         | -173      |
|    explained_variance   | 0.925     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.78     |
|    n_updates            | 1080      |
|    policy_gradient_loss | -0.0136   |
|    std                  | 5.97      |
|    value_loss           | 0.0867    |
---------------------------------------
Eval num_timesteps=3600000, episode_reward=2.45 +/- 1.02
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.45       |
| time/                   |            |
|    total_timesteps      | 3600000    |
| train/                  |            |
|    approx_kl            | 0.12236966 |
|    clip_fraction        | 0.489      |
|    clip_range           | 0.2        |
|    entropy_loss         | -173       |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0005     |
|    loss                 | -1.72      |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.0152    |
|    std                  | 6.03       |
|    value_loss           | 0.089      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.79     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 110      |
|    time_elapsed    | 17627    |
|    total_timesteps | 3604480  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.35       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 111        |
|    time_elapsed         | 17782      |
|    total_timesteps      | 3637248    |
| train/                  |            |
|    approx_kl            | 0.11719895 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.2        |
|    entropy_loss         | -174       |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.75      |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.0151    |
|    std                  | 6.1        |
|    value_loss           | 0.0829     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.62       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 112        |
|    time_elapsed         | 17938      |
|    total_timesteps      | 3670016    |
| train/                  |            |
|    approx_kl            | 0.12194075 |
|    clip_fraction        | 0.491      |
|    clip_range           | 0.2        |
|    entropy_loss         | -175       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.69      |
|    n_updates            | 1110       |
|    policy_gradient_loss | -0.0138    |
|    std                  | 6.18       |
|    value_loss           | 0.0843     |
----------------------------------------
Eval num_timesteps=3680000, episode_reward=2.71 +/- 1.16
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.71       |
| time/                   |            |
|    total_timesteps      | 3680000    |
| train/                  |            |
|    approx_kl            | 0.11442714 |
|    clip_fraction        | 0.482      |
|    clip_range           | 0.2        |
|    entropy_loss         | -175       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.73      |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.0158    |
|    std                  | 6.25       |
|    value_loss           | 0.0835     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.82     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 113      |
|    time_elapsed    | 18099    |
|    total_timesteps | 3702784  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.33       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 114        |
|    time_elapsed         | 18258      |
|    total_timesteps      | 3735552    |
| train/                  |            |
|    approx_kl            | 0.11971184 |
|    clip_fraction        | 0.491      |
|    clip_range           | 0.2        |
|    entropy_loss         | -176       |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.77      |
|    n_updates            | 1130       |
|    policy_gradient_loss | -0.0124    |
|    std                  | 6.32       |
|    value_loss           | 0.084      |
----------------------------------------
Eval num_timesteps=3760000, episode_reward=1.75 +/- 0.70
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.75        |
| time/                   |             |
|    total_timesteps      | 3760000     |
| train/                  |             |
|    approx_kl            | 0.116648115 |
|    clip_fraction        | 0.488       |
|    clip_range           | 0.2         |
|    entropy_loss         | -177        |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0005      |
|    loss                 | -1.74       |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.0142     |
|    std                  | 6.4         |
|    value_loss           | 0.0873      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.8      |
| time/              |          |
|    fps             | 204      |
|    iterations      | 115      |
|    time_elapsed    | 18418    |
|    total_timesteps | 3768320  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.87       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 116        |
|    time_elapsed         | 18578      |
|    total_timesteps      | 3801088    |
| train/                  |            |
|    approx_kl            | 0.11293087 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.2        |
|    entropy_loss         | -177       |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0005     |
|    loss                 | -1.8       |
|    n_updates            | 1150       |
|    policy_gradient_loss | -0.0144    |
|    std                  | 6.47       |
|    value_loss           | 0.0871     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.54        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 117         |
|    time_elapsed         | 18737       |
|    total_timesteps      | 3833856     |
| train/                  |             |
|    approx_kl            | 0.118096925 |
|    clip_fraction        | 0.486       |
|    clip_range           | 0.2         |
|    entropy_loss         | -178        |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0005      |
|    loss                 | -1.77       |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.0133     |
|    std                  | 6.55        |
|    value_loss           | 0.0955      |
-----------------------------------------
Eval num_timesteps=3840000, episode_reward=2.24 +/- 1.01
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.24        |
| time/                   |             |
|    total_timesteps      | 3840000     |
| train/                  |             |
|    approx_kl            | 0.120270014 |
|    clip_fraction        | 0.483       |
|    clip_range           | 0.2         |
|    entropy_loss         | -178        |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0005      |
|    loss                 | -1.8        |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0149     |
|    std                  | 6.62        |
|    value_loss           | 0.0834      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.47     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 118      |
|    time_elapsed    | 18901    |
|    total_timesteps | 3866624  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.45       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 119        |
|    time_elapsed         | 19061      |
|    total_timesteps      | 3899392    |
| train/                  |            |
|    approx_kl            | 0.11794985 |
|    clip_fraction        | 0.483      |
|    clip_range           | 0.2        |
|    entropy_loss         | -179       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.8       |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.0136    |
|    std                  | 6.7        |
|    value_loss           | 0.0839     |
----------------------------------------
Eval num_timesteps=3920000, episode_reward=1.69 +/- 1.02
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 1.69      |
| time/                   |           |
|    total_timesteps      | 3920000   |
| train/                  |           |
|    approx_kl            | 0.1136723 |
|    clip_fraction        | 0.472     |
|    clip_range           | 0.2       |
|    entropy_loss         | -180      |
|    explained_variance   | 0.923     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.79     |
|    n_updates            | 1190      |
|    policy_gradient_loss | -0.0154   |
|    std                  | 6.78      |
|    value_loss           | 0.0863    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.24     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 120      |
|    time_elapsed    | 19225    |
|    total_timesteps | 3932160  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.14       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 121        |
|    time_elapsed         | 19385      |
|    total_timesteps      | 3964928    |
| train/                  |            |
|    approx_kl            | 0.11568567 |
|    clip_fraction        | 0.482      |
|    clip_range           | 0.2        |
|    entropy_loss         | -180       |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.78      |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.0138    |
|    std                  | 6.85       |
|    value_loss           | 0.0896     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.21       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 122        |
|    time_elapsed         | 19544      |
|    total_timesteps      | 3997696    |
| train/                  |            |
|    approx_kl            | 0.11216848 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.2        |
|    entropy_loss         | -181       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.82      |
|    n_updates            | 1210       |
|    policy_gradient_loss | -0.0148    |
|    std                  | 6.94       |
|    value_loss           | 0.088      |
----------------------------------------
Eval num_timesteps=4000000, episode_reward=1.46 +/- 1.00
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.46       |
| time/                   |            |
|    total_timesteps      | 4000000    |
| train/                  |            |
|    approx_kl            | 0.11032671 |
|    clip_fraction        | 0.485      |
|    clip_range           | 0.2        |
|    entropy_loss         | -182       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.83      |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.0122    |
|    std                  | 7.03       |
|    value_loss           | 0.0876     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.52     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 123      |
|    time_elapsed    | 19705    |
|    total_timesteps | 4030464  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.26       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 124        |
|    time_elapsed         | 19862      |
|    total_timesteps      | 4063232    |
| train/                  |            |
|    approx_kl            | 0.10376551 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.2        |
|    entropy_loss         | -182       |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.77      |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.013     |
|    std                  | 7.09       |
|    value_loss           | 0.0934     |
----------------------------------------
Eval num_timesteps=4080000, episode_reward=3.01 +/- 1.59
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 3.01        |
| time/                   |             |
|    total_timesteps      | 4080000     |
| train/                  |             |
|    approx_kl            | 0.106378615 |
|    clip_fraction        | 0.464       |
|    clip_range           | 0.2         |
|    entropy_loss         | -183        |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0005      |
|    loss                 | -1.84       |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0131     |
|    std                  | 7.17        |
|    value_loss           | 0.0967      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.13     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 125      |
|    time_elapsed    | 20024    |
|    total_timesteps | 4096000  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.58       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 126        |
|    time_elapsed         | 20185      |
|    total_timesteps      | 4128768    |
| train/                  |            |
|    approx_kl            | 0.10392961 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.2        |
|    entropy_loss         | -183       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.79      |
|    n_updates            | 1250       |
|    policy_gradient_loss | -0.0164    |
|    std                  | 7.25       |
|    value_loss           | 0.0914     |
----------------------------------------
Eval num_timesteps=4160000, episode_reward=2.65 +/- 1.60
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.65       |
| time/                   |            |
|    total_timesteps      | 4160000    |
| train/                  |            |
|    approx_kl            | 0.10069467 |
|    clip_fraction        | 0.463      |
|    clip_range           | 0.2        |
|    entropy_loss         | -184       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.77      |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.0135    |
|    std                  | 7.34       |
|    value_loss           | 0.106      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.63     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 127      |
|    time_elapsed    | 20344    |
|    total_timesteps | 4161536  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.27        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 128         |
|    time_elapsed         | 20503       |
|    total_timesteps      | 4194304     |
| train/                  |             |
|    approx_kl            | 0.107844114 |
|    clip_fraction        | 0.467       |
|    clip_range           | 0.2         |
|    entropy_loss         | -185        |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0005      |
|    loss                 | -1.86       |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.0161     |
|    std                  | 7.43        |
|    value_loss           | 0.0998      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.58       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 129        |
|    time_elapsed         | 20662      |
|    total_timesteps      | 4227072    |
| train/                  |            |
|    approx_kl            | 0.10317713 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.2        |
|    entropy_loss         | -185       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.84      |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.0159    |
|    std                  | 7.51       |
|    value_loss           | 0.0922     |
----------------------------------------
Eval num_timesteps=4240000, episode_reward=2.74 +/- 0.87
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.74       |
| time/                   |            |
|    total_timesteps      | 4240000    |
| train/                  |            |
|    approx_kl            | 0.10235357 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.2        |
|    entropy_loss         | -186       |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.84      |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.0148    |
|    std                  | 7.58       |
|    value_loss           | 0.0939     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.04     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 130      |
|    time_elapsed    | 20827    |
|    total_timesteps | 4259840  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.47       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 131        |
|    time_elapsed         | 20987      |
|    total_timesteps      | 4292608    |
| train/                  |            |
|    approx_kl            | 0.10100323 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.2        |
|    entropy_loss         | -186       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.88      |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.015     |
|    std                  | 7.65       |
|    value_loss           | 0.106      |
----------------------------------------
Eval num_timesteps=4320000, episode_reward=2.17 +/- 0.82
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.17       |
| time/                   |            |
|    total_timesteps      | 4320000    |
| train/                  |            |
|    approx_kl            | 0.10663364 |
|    clip_fraction        | 0.457      |
|    clip_range           | 0.2        |
|    entropy_loss         | -187       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.88      |
|    n_updates            | 1310       |
|    policy_gradient_loss | -0.0172    |
|    std                  | 7.74       |
|    value_loss           | 0.0974     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.65     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 132      |
|    time_elapsed    | 21151    |
|    total_timesteps | 4325376  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.66       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 133        |
|    time_elapsed         | 21308      |
|    total_timesteps      | 4358144    |
| train/                  |            |
|    approx_kl            | 0.10791591 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.2        |
|    entropy_loss         | -187       |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.83      |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.015     |
|    std                  | 7.83       |
|    value_loss           | 0.0967     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.19       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 134        |
|    time_elapsed         | 21465      |
|    total_timesteps      | 4390912    |
| train/                  |            |
|    approx_kl            | 0.10395834 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.2        |
|    entropy_loss         | -188       |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.88      |
|    n_updates            | 1330       |
|    policy_gradient_loss | -0.0181    |
|    std                  | 7.91       |
|    value_loss           | 0.0875     |
----------------------------------------
Eval num_timesteps=4400000, episode_reward=2.44 +/- 1.05
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.44        |
| time/                   |             |
|    total_timesteps      | 4400000     |
| train/                  |             |
|    approx_kl            | 0.099194214 |
|    clip_fraction        | 0.456       |
|    clip_range           | 0.2         |
|    entropy_loss         | -189        |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0005      |
|    loss                 | -1.88       |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.0153     |
|    std                  | 8           |
|    value_loss           | 0.093       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.37     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 135      |
|    time_elapsed    | 21629    |
|    total_timesteps | 4423680  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.33       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 136        |
|    time_elapsed         | 21787      |
|    total_timesteps      | 4456448    |
| train/                  |            |
|    approx_kl            | 0.10408005 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.2        |
|    entropy_loss         | -189       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.83      |
|    n_updates            | 1350       |
|    policy_gradient_loss | -0.0171    |
|    std                  | 8.08       |
|    value_loss           | 0.0922     |
----------------------------------------
Eval num_timesteps=4480000, episode_reward=2.84 +/- 1.56
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.84       |
| time/                   |            |
|    total_timesteps      | 4480000    |
| train/                  |            |
|    approx_kl            | 0.10085268 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.2        |
|    entropy_loss         | -190       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.9       |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.0168    |
|    std                  | 8.2        |
|    value_loss           | 0.0889     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.89     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 137      |
|    time_elapsed    | 21950    |
|    total_timesteps | 4489216  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.41       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 138        |
|    time_elapsed         | 22109      |
|    total_timesteps      | 4521984    |
| train/                  |            |
|    approx_kl            | 0.09728203 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.2        |
|    entropy_loss         | -191       |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.88      |
|    n_updates            | 1370       |
|    policy_gradient_loss | -0.0174    |
|    std                  | 8.28       |
|    value_loss           | 0.0858     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.49       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 139        |
|    time_elapsed         | 22269      |
|    total_timesteps      | 4554752    |
| train/                  |            |
|    approx_kl            | 0.09853893 |
|    clip_fraction        | 0.449      |
|    clip_range           | 0.2        |
|    entropy_loss         | -191       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.85      |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0168    |
|    std                  | 8.38       |
|    value_loss           | 0.0968     |
----------------------------------------
Eval num_timesteps=4560000, episode_reward=2.27 +/- 1.32
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.27        |
| time/                   |             |
|    total_timesteps      | 4560000     |
| train/                  |             |
|    approx_kl            | 0.092650466 |
|    clip_fraction        | 0.432       |
|    clip_range           | 0.2         |
|    entropy_loss         | -192        |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0005      |
|    loss                 | -1.92       |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.015      |
|    std                  | 8.46        |
|    value_loss           | 0.104       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.11     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 140      |
|    time_elapsed    | 22435    |
|    total_timesteps | 4587520  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.6        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 141        |
|    time_elapsed         | 22594      |
|    total_timesteps      | 4620288    |
| train/                  |            |
|    approx_kl            | 0.09271036 |
|    clip_fraction        | 0.439      |
|    clip_range           | 0.2        |
|    entropy_loss         | -192       |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.89      |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.0164    |
|    std                  | 8.56       |
|    value_loss           | 0.0914     |
----------------------------------------
Eval num_timesteps=4640000, episode_reward=1.61 +/- 0.61
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.61       |
| time/                   |            |
|    total_timesteps      | 4640000    |
| train/                  |            |
|    approx_kl            | 0.09234771 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.2        |
|    entropy_loss         | -193       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.91      |
|    n_updates            | 1410       |
|    policy_gradient_loss | -0.0157    |
|    std                  | 8.65       |
|    value_loss           | 0.083      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.55     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 142      |
|    time_elapsed    | 22758    |
|    total_timesteps | 4653056  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.85       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 143        |
|    time_elapsed         | 22918      |
|    total_timesteps      | 4685824    |
| train/                  |            |
|    approx_kl            | 0.09264772 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.2        |
|    entropy_loss         | -193       |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.9       |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.0182    |
|    std                  | 8.75       |
|    value_loss           | 0.092      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.02       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 144        |
|    time_elapsed         | 23075      |
|    total_timesteps      | 4718592    |
| train/                  |            |
|    approx_kl            | 0.09344344 |
|    clip_fraction        | 0.434      |
|    clip_range           | 0.2        |
|    entropy_loss         | -194       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.9       |
|    n_updates            | 1430       |
|    policy_gradient_loss | -0.0167    |
|    std                  | 8.87       |
|    value_loss           | 0.097      |
----------------------------------------
Eval num_timesteps=4720000, episode_reward=1.25 +/- 0.71
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.25       |
| time/                   |            |
|    total_timesteps      | 4720000    |
| train/                  |            |
|    approx_kl            | 0.08970335 |
|    clip_fraction        | 0.432      |
|    clip_range           | 0.2        |
|    entropy_loss         | -195       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.9       |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.0168    |
|    std                  | 8.95       |
|    value_loss           | 0.0964     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.93     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 145      |
|    time_elapsed    | 23238    |
|    total_timesteps | 4751360  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.89       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 146        |
|    time_elapsed         | 23396      |
|    total_timesteps      | 4784128    |
| train/                  |            |
|    approx_kl            | 0.09218745 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.2        |
|    entropy_loss         | -195       |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.93      |
|    n_updates            | 1450       |
|    policy_gradient_loss | -0.0162    |
|    std                  | 9.05       |
|    value_loss           | 0.0981     |
----------------------------------------
Eval num_timesteps=4800000, episode_reward=2.05 +/- 0.98
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.05       |
| time/                   |            |
|    total_timesteps      | 4800000    |
| train/                  |            |
|    approx_kl            | 0.09444666 |
|    clip_fraction        | 0.44       |
|    clip_range           | 0.2        |
|    entropy_loss         | -196       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.95      |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.0162    |
|    std                  | 9.14       |
|    value_loss           | 0.105      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.25     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 147      |
|    time_elapsed    | 23560    |
|    total_timesteps | 4816896  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 4.81        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 148         |
|    time_elapsed         | 23718       |
|    total_timesteps      | 4849664     |
| train/                  |             |
|    approx_kl            | 0.088316366 |
|    clip_fraction        | 0.421       |
|    clip_range           | 0.2         |
|    entropy_loss         | -196        |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0005      |
|    loss                 | -1.99       |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.0172     |
|    std                  | 9.23        |
|    value_loss           | 0.103       |
-----------------------------------------
Eval num_timesteps=4880000, episode_reward=2.76 +/- 0.53
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.76       |
| time/                   |            |
|    total_timesteps      | 4880000    |
| train/                  |            |
|    approx_kl            | 0.09075409 |
|    clip_fraction        | 0.432      |
|    clip_range           | 0.2        |
|    entropy_loss         | -197       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.98      |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.0177    |
|    std                  | 9.33       |
|    value_loss           | 0.101      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.44     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 149      |
|    time_elapsed    | 23883    |
|    total_timesteps | 4882432  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.6        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 150        |
|    time_elapsed         | 24041      |
|    total_timesteps      | 4915200    |
| train/                  |            |
|    approx_kl            | 0.09465321 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.2        |
|    entropy_loss         | -197       |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0005     |
|    loss                 | -2.01      |
|    n_updates            | 1490       |
|    policy_gradient_loss | -0.0173    |
|    std                  | 9.43       |
|    value_loss           | 0.0898     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.47       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 151        |
|    time_elapsed         | 24200      |
|    total_timesteps      | 4947968    |
| train/                  |            |
|    approx_kl            | 0.08646512 |
|    clip_fraction        | 0.432      |
|    clip_range           | 0.2        |
|    entropy_loss         | -198       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | -2         |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.0191    |
|    std                  | 9.53       |
|    value_loss           | 0.0927     |
----------------------------------------
Eval num_timesteps=4960000, episode_reward=1.93 +/- 0.99
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.93       |
| time/                   |            |
|    total_timesteps      | 4960000    |
| train/                  |            |
|    approx_kl            | 0.09020582 |
|    clip_fraction        | 0.431      |
|    clip_range           | 0.2        |
|    entropy_loss         | -199       |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0005     |
|    loss                 | -2.02      |
|    n_updates            | 1510       |
|    policy_gradient_loss | -0.019     |
|    std                  | 9.62       |
|    value_loss           | 0.0904     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5        |
| time/              |          |
|    fps             | 204      |
|    iterations      | 152      |
|    time_elapsed    | 24365    |
|    total_timesteps | 4980736  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.09       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 153        |
|    time_elapsed         | 24522      |
|    total_timesteps      | 5013504    |
| train/                  |            |
|    approx_kl            | 0.08614308 |
|    clip_fraction        | 0.423      |
|    clip_range           | 0.2        |
|    entropy_loss         | -199       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.05      |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.0176    |
|    std                  | 9.72       |
|    value_loss           | 0.0914     |
----------------------------------------
Eval num_timesteps=5040000, episode_reward=2.03 +/- 0.82
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.03       |
| time/                   |            |
|    total_timesteps      | 5040000    |
| train/                  |            |
|    approx_kl            | 0.08783707 |
|    clip_fraction        | 0.422      |
|    clip_range           | 0.2        |
|    entropy_loss         | -200       |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.99      |
|    n_updates            | 1530       |
|    policy_gradient_loss | -0.0187    |
|    std                  | 9.82       |
|    value_loss           | 0.0995     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.58     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 154      |
|    time_elapsed    | 24687    |
|    total_timesteps | 5046272  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 4.99        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 155         |
|    time_elapsed         | 24846       |
|    total_timesteps      | 5079040     |
| train/                  |             |
|    approx_kl            | 0.082545206 |
|    clip_fraction        | 0.417       |
|    clip_range           | 0.2         |
|    entropy_loss         | -200        |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0005      |
|    loss                 | -1.98       |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.0164     |
|    std                  | 9.94        |
|    value_loss           | 0.102       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.78       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 156        |
|    time_elapsed         | 25003      |
|    total_timesteps      | 5111808    |
| train/                  |            |
|    approx_kl            | 0.08782013 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -201       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.02      |
|    n_updates            | 1550       |
|    policy_gradient_loss | -0.0168    |
|    std                  | 10         |
|    value_loss           | 0.0921     |
----------------------------------------
Eval num_timesteps=5120000, episode_reward=1.90 +/- 0.66
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 5120000     |
| train/                  |             |
|    approx_kl            | 0.083199516 |
|    clip_fraction        | 0.423       |
|    clip_range           | 0.2         |
|    entropy_loss         | -201        |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.03       |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 10.1        |
|    value_loss           | 0.0842      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.73     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 157      |
|    time_elapsed    | 25164    |
|    total_timesteps | 5144576  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.24       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 158        |
|    time_elapsed         | 25323      |
|    total_timesteps      | 5177344    |
| train/                  |            |
|    approx_kl            | 0.08492406 |
|    clip_fraction        | 0.413      |
|    clip_range           | 0.2        |
|    entropy_loss         | -202       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.99      |
|    n_updates            | 1570       |
|    policy_gradient_loss | -0.0177    |
|    std                  | 10.2       |
|    value_loss           | 0.0881     |
----------------------------------------
Eval num_timesteps=5200000, episode_reward=1.52 +/- 0.67
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.52       |
| time/                   |            |
|    total_timesteps      | 5200000    |
| train/                  |            |
|    approx_kl            | 0.08440636 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -202       |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.08      |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.0177    |
|    std                  | 10.3       |
|    value_loss           | 0.095      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.77     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 159      |
|    time_elapsed    | 25487    |
|    total_timesteps | 5210112  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.29       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 160        |
|    time_elapsed         | 25646      |
|    total_timesteps      | 5242880    |
| train/                  |            |
|    approx_kl            | 0.08965427 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.2        |
|    entropy_loss         | -203       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -2         |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.0195    |
|    std                  | 10.5       |
|    value_loss           | 0.073      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.17       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 161        |
|    time_elapsed         | 25804      |
|    total_timesteps      | 5275648    |
| train/                  |            |
|    approx_kl            | 0.08331793 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.2        |
|    entropy_loss         | -204       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.06      |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.0184    |
|    std                  | 10.6       |
|    value_loss           | 0.0744     |
----------------------------------------
Eval num_timesteps=5280000, episode_reward=2.53 +/- 0.70
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 2.53      |
| time/                   |           |
|    total_timesteps      | 5280000   |
| train/                  |           |
|    approx_kl            | 0.0818008 |
|    clip_fraction        | 0.41      |
|    clip_range           | 0.2       |
|    entropy_loss         | -204      |
|    explained_variance   | 0.921     |
|    learning_rate        | 0.0005    |
|    loss                 | -2.07     |
|    n_updates            | 1610      |
|    policy_gradient_loss | -0.0181   |
|    std                  | 10.7      |
|    value_loss           | 0.0833    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.08     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 162      |
|    time_elapsed    | 25964    |
|    total_timesteps | 5308416  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.43       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 163        |
|    time_elapsed         | 26123      |
|    total_timesteps      | 5341184    |
| train/                  |            |
|    approx_kl            | 0.08335404 |
|    clip_fraction        | 0.409      |
|    clip_range           | 0.2        |
|    entropy_loss         | -205       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.08      |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.0188    |
|    std                  | 10.8       |
|    value_loss           | 0.0915     |
----------------------------------------
Eval num_timesteps=5360000, episode_reward=2.37 +/- 0.93
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.37        |
| time/                   |             |
|    total_timesteps      | 5360000     |
| train/                  |             |
|    approx_kl            | 0.082526065 |
|    clip_fraction        | 0.406       |
|    clip_range           | 0.2         |
|    entropy_loss         | -205        |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0005      |
|    loss                 | -2.04       |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.0168     |
|    std                  | 10.9        |
|    value_loss           | 0.0988      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.6      |
| time/              |          |
|    fps             | 204      |
|    iterations      | 164      |
|    time_elapsed    | 26285    |
|    total_timesteps | 5373952  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.1        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 165        |
|    time_elapsed         | 26444      |
|    total_timesteps      | 5406720    |
| train/                  |            |
|    approx_kl            | 0.07661806 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.2        |
|    entropy_loss         | -206       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.06      |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.0151    |
|    std                  | 11         |
|    value_loss           | 0.106      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.06       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 166        |
|    time_elapsed         | 26604      |
|    total_timesteps      | 5439488    |
| train/                  |            |
|    approx_kl            | 0.08214561 |
|    clip_fraction        | 0.42       |
|    clip_range           | 0.2        |
|    entropy_loss         | -206       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.97      |
|    n_updates            | 1650       |
|    policy_gradient_loss | -0.017     |
|    std                  | 11.1       |
|    value_loss           | 0.102      |
----------------------------------------
Eval num_timesteps=5440000, episode_reward=2.62 +/- 1.34
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 2.62      |
| time/                   |           |
|    total_timesteps      | 5440000   |
| train/                  |           |
|    approx_kl            | 0.0768121 |
|    clip_fraction        | 0.405     |
|    clip_range           | 0.2       |
|    entropy_loss         | -207      |
|    explained_variance   | 0.916     |
|    learning_rate        | 0.0005    |
|    loss                 | -2.07     |
|    n_updates            | 1660      |
|    policy_gradient_loss | -0.0172   |
|    std                  | 11.2      |
|    value_loss           | 0.104     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.51     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 167      |
|    time_elapsed    | 26768    |
|    total_timesteps | 5472256  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.73       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 168        |
|    time_elapsed         | 26927      |
|    total_timesteps      | 5505024    |
| train/                  |            |
|    approx_kl            | 0.08510361 |
|    clip_fraction        | 0.429      |
|    clip_range           | 0.2        |
|    entropy_loss         | -207       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.98      |
|    n_updates            | 1670       |
|    policy_gradient_loss | -0.0189    |
|    std                  | 11.3       |
|    value_loss           | 0.0999     |
----------------------------------------
Eval num_timesteps=5520000, episode_reward=1.67 +/- 0.66
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.67       |
| time/                   |            |
|    total_timesteps      | 5520000    |
| train/                  |            |
|    approx_kl            | 0.07988919 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.2        |
|    entropy_loss         | -208       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.07      |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.0193    |
|    std                  | 11.4       |
|    value_loss           | 0.0971     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5        |
| time/              |          |
|    fps             | 204      |
|    iterations      | 169      |
|    time_elapsed    | 27090    |
|    total_timesteps | 5537792  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.12       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 170        |
|    time_elapsed         | 27248      |
|    total_timesteps      | 5570560    |
| train/                  |            |
|    approx_kl            | 0.08263818 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.2        |
|    entropy_loss         | -208       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.1       |
|    n_updates            | 1690       |
|    policy_gradient_loss | -0.0184    |
|    std                  | 11.5       |
|    value_loss           | 0.0974     |
----------------------------------------
Eval num_timesteps=5600000, episode_reward=2.25 +/- 1.32
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.25        |
| time/                   |             |
|    total_timesteps      | 5600000     |
| train/                  |             |
|    approx_kl            | 0.076046824 |
|    clip_fraction        | 0.4         |
|    clip_range           | 0.2         |
|    entropy_loss         | -209        |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.08       |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.0186     |
|    std                  | 11.6        |
|    value_loss           | 0.0958      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.56     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 171      |
|    time_elapsed    | 27411    |
|    total_timesteps | 5603328  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.73       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 172        |
|    time_elapsed         | 27569      |
|    total_timesteps      | 5636096    |
| train/                  |            |
|    approx_kl            | 0.07908138 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.2        |
|    entropy_loss         | -209       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.08      |
|    n_updates            | 1710       |
|    policy_gradient_loss | -0.0182    |
|    std                  | 11.8       |
|    value_loss           | 0.0982     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.31       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 173        |
|    time_elapsed         | 27726      |
|    total_timesteps      | 5668864    |
| train/                  |            |
|    approx_kl            | 0.08103599 |
|    clip_fraction        | 0.41       |
|    clip_range           | 0.2        |
|    entropy_loss         | -210       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.08      |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.0168    |
|    std                  | 11.9       |
|    value_loss           | 0.0976     |
----------------------------------------
Eval num_timesteps=5680000, episode_reward=1.61 +/- 1.19
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.61        |
| time/                   |             |
|    total_timesteps      | 5680000     |
| train/                  |             |
|    approx_kl            | 0.075939365 |
|    clip_fraction        | 0.402       |
|    clip_range           | 0.2         |
|    entropy_loss         | -211        |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.08       |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.0185     |
|    std                  | 12          |
|    value_loss           | 0.0938      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.24     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 174      |
|    time_elapsed    | 27889    |
|    total_timesteps | 5701632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 4.38        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 175         |
|    time_elapsed         | 28046       |
|    total_timesteps      | 5734400     |
| train/                  |             |
|    approx_kl            | 0.076718494 |
|    clip_fraction        | 0.399       |
|    clip_range           | 0.2         |
|    entropy_loss         | -211        |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.11       |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.0172     |
|    std                  | 12.1        |
|    value_loss           | 0.0967      |
-----------------------------------------
Eval num_timesteps=5760000, episode_reward=2.77 +/- 1.54
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.77       |
| time/                   |            |
|    total_timesteps      | 5760000    |
| train/                  |            |
|    approx_kl            | 0.07623488 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.2        |
|    entropy_loss         | -212       |
|    explained_variance   | 0.9        |
|    learning_rate        | 0.0005     |
|    loss                 | -2.14      |
|    n_updates            | 1750       |
|    policy_gradient_loss | -0.019     |
|    std                  | 12.2       |
|    value_loss           | 0.0964     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.55     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 176      |
|    time_elapsed    | 28211    |
|    total_timesteps | 5767168  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.3        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 177        |
|    time_elapsed         | 28368      |
|    total_timesteps      | 5799936    |
| train/                  |            |
|    approx_kl            | 0.07987657 |
|    clip_fraction        | 0.401      |
|    clip_range           | 0.2        |
|    entropy_loss         | -212       |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.12      |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.0199    |
|    std                  | 12.3       |
|    value_loss           | 0.0991     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.85       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 178        |
|    time_elapsed         | 28527      |
|    total_timesteps      | 5832704    |
| train/                  |            |
|    approx_kl            | 0.08067422 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.2        |
|    entropy_loss         | -213       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.13      |
|    n_updates            | 1770       |
|    policy_gradient_loss | -0.0192    |
|    std                  | 12.5       |
|    value_loss           | 0.0905     |
----------------------------------------
Eval num_timesteps=5840000, episode_reward=1.88 +/- 0.93
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.88        |
| time/                   |             |
|    total_timesteps      | 5840000     |
| train/                  |             |
|    approx_kl            | 0.075361416 |
|    clip_fraction        | 0.395       |
|    clip_range           | 0.2         |
|    entropy_loss         | -213        |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.12       |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 12.6        |
|    value_loss           | 0.0882      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.59     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 179      |
|    time_elapsed    | 28693    |
|    total_timesteps | 5865472  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.89       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 180        |
|    time_elapsed         | 28853      |
|    total_timesteps      | 5898240    |
| train/                  |            |
|    approx_kl            | 0.07694108 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.2        |
|    entropy_loss         | -214       |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.12      |
|    n_updates            | 1790       |
|    policy_gradient_loss | -0.0186    |
|    std                  | 12.7       |
|    value_loss           | 0.0942     |
----------------------------------------
Eval num_timesteps=5920000, episode_reward=1.32 +/- 0.56
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.32        |
| time/                   |             |
|    total_timesteps      | 5920000     |
| train/                  |             |
|    approx_kl            | 0.074362785 |
|    clip_fraction        | 0.39        |
|    clip_range           | 0.2         |
|    entropy_loss         | -214        |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.18       |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.0186     |
|    std                  | 12.9        |
|    value_loss           | 0.103       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.19     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 181      |
|    time_elapsed    | 29017    |
|    total_timesteps | 5931008  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.99       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 182        |
|    time_elapsed         | 29176      |
|    total_timesteps      | 5963776    |
| train/                  |            |
|    approx_kl            | 0.07508429 |
|    clip_fraction        | 0.401      |
|    clip_range           | 0.2        |
|    entropy_loss         | -215       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.13      |
|    n_updates            | 1810       |
|    policy_gradient_loss | -0.0192    |
|    std                  | 13         |
|    value_loss           | 0.101      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.63       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 183        |
|    time_elapsed         | 29336      |
|    total_timesteps      | 5996544    |
| train/                  |            |
|    approx_kl            | 0.07442279 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.2        |
|    entropy_loss         | -215       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.2       |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.0184    |
|    std                  | 13.1       |
|    value_loss           | 0.0999     |
----------------------------------------
Eval num_timesteps=6000000, episode_reward=2.40 +/- 2.16
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.4         |
| time/                   |             |
|    total_timesteps      | 6000000     |
| train/                  |             |
|    approx_kl            | 0.080906585 |
|    clip_fraction        | 0.4         |
|    clip_range           | 0.2         |
|    entropy_loss         | -216        |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.16       |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0187     |
|    std                  | 13.2        |
|    value_loss           | 0.0997      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.9      |
| time/              |          |
|    fps             | 204      |
|    iterations      | 184      |
|    time_elapsed    | 29500    |
|    total_timesteps | 6029312  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.97       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 185        |
|    time_elapsed         | 29660      |
|    total_timesteps      | 6062080    |
| train/                  |            |
|    approx_kl            | 0.07249838 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.2        |
|    entropy_loss         | -216       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.15      |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.0188    |
|    std                  | 13.3       |
|    value_loss           | 0.103      |
----------------------------------------
Eval num_timesteps=6080000, episode_reward=1.73 +/- 0.62
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 1.73      |
| time/                   |           |
|    total_timesteps      | 6080000   |
| train/                  |           |
|    approx_kl            | 0.0748602 |
|    clip_fraction        | 0.385     |
|    clip_range           | 0.2       |
|    entropy_loss         | -217      |
|    explained_variance   | 0.926     |
|    learning_rate        | 0.0005    |
|    loss                 | -2.13     |
|    n_updates            | 1850      |
|    policy_gradient_loss | -0.0193   |
|    std                  | 13.5      |
|    value_loss           | 0.0939    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.85     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 186      |
|    time_elapsed    | 29826    |
|    total_timesteps | 6094848  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.55       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 187        |
|    time_elapsed         | 29985      |
|    total_timesteps      | 6127616    |
| train/                  |            |
|    approx_kl            | 0.07549466 |
|    clip_fraction        | 0.386      |
|    clip_range           | 0.2        |
|    entropy_loss         | -217       |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.16      |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.0188    |
|    std                  | 13.6       |
|    value_loss           | 0.102      |
----------------------------------------
Eval num_timesteps=6160000, episode_reward=2.18 +/- 1.10
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.18        |
| time/                   |             |
|    total_timesteps      | 6160000     |
| train/                  |             |
|    approx_kl            | 0.070650816 |
|    clip_fraction        | 0.38        |
|    clip_range           | 0.2         |
|    entropy_loss         | -218        |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.16       |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.0183     |
|    std                  | 13.7        |
|    value_loss           | 0.103       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.34     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 188      |
|    time_elapsed    | 30149    |
|    total_timesteps | 6160384  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.05       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 189        |
|    time_elapsed         | 30308      |
|    total_timesteps      | 6193152    |
| train/                  |            |
|    approx_kl            | 0.07627633 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.2        |
|    entropy_loss         | -218       |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.14      |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.0193    |
|    std                  | 13.9       |
|    value_loss           | 0.0934     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.47        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 190         |
|    time_elapsed         | 30465       |
|    total_timesteps      | 6225920     |
| train/                  |             |
|    approx_kl            | 0.071610674 |
|    clip_fraction        | 0.39        |
|    clip_range           | 0.2         |
|    entropy_loss         | -219        |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.13       |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0176     |
|    std                  | 14          |
|    value_loss           | 0.0959      |
-----------------------------------------
Eval num_timesteps=6240000, episode_reward=1.84 +/- 1.02
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.84       |
| time/                   |            |
|    total_timesteps      | 6240000    |
| train/                  |            |
|    approx_kl            | 0.06830194 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.2        |
|    entropy_loss         | -219       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.15      |
|    n_updates            | 1900       |
|    policy_gradient_loss | -0.0182    |
|    std                  | 14.1       |
|    value_loss           | 0.0988     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.38     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 191      |
|    time_elapsed    | 30624    |
|    total_timesteps | 6258688  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.26       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 192        |
|    time_elapsed         | 30781      |
|    total_timesteps      | 6291456    |
| train/                  |            |
|    approx_kl            | 0.06890692 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.2        |
|    entropy_loss         | -220       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.21      |
|    n_updates            | 1910       |
|    policy_gradient_loss | -0.019     |
|    std                  | 14.3       |
|    value_loss           | 0.101      |
----------------------------------------
Eval num_timesteps=6320000, episode_reward=1.34 +/- 0.71
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.34        |
| time/                   |             |
|    total_timesteps      | 6320000     |
| train/                  |             |
|    approx_kl            | 0.072636396 |
|    clip_fraction        | 0.394       |
|    clip_range           | 0.2         |
|    entropy_loss         | -220        |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.21       |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.0188     |
|    std                  | 14.4        |
|    value_loss           | 0.106       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.05     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 193      |
|    time_elapsed    | 30944    |
|    total_timesteps | 6324224  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.45        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 194         |
|    time_elapsed         | 31102       |
|    total_timesteps      | 6356992     |
| train/                  |             |
|    approx_kl            | 0.073718876 |
|    clip_fraction        | 0.383       |
|    clip_range           | 0.2         |
|    entropy_loss         | -221        |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.25       |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.0201     |
|    std                  | 14.5        |
|    value_loss           | 0.0987      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.81       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 195        |
|    time_elapsed         | 31259      |
|    total_timesteps      | 6389760    |
| train/                  |            |
|    approx_kl            | 0.06457089 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.2        |
|    entropy_loss         | -221       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.16      |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.0198    |
|    std                  | 14.7       |
|    value_loss           | 0.0966     |
----------------------------------------
Eval num_timesteps=6400000, episode_reward=2.78 +/- 1.83
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.78       |
| time/                   |            |
|    total_timesteps      | 6400000    |
| train/                  |            |
|    approx_kl            | 0.06891643 |
|    clip_fraction        | 0.38       |
|    clip_range           | 0.2        |
|    entropy_loss         | -222       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.19      |
|    n_updates            | 1950       |
|    policy_gradient_loss | -0.0174    |
|    std                  | 14.8       |
|    value_loss           | 0.0857     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.87     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 196      |
|    time_elapsed    | 31422    |
|    total_timesteps | 6422528  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 4.88        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 197         |
|    time_elapsed         | 31581       |
|    total_timesteps      | 6455296     |
| train/                  |             |
|    approx_kl            | 0.065145366 |
|    clip_fraction        | 0.369       |
|    clip_range           | 0.2         |
|    entropy_loss         | -222        |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.25       |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.0182     |
|    std                  | 14.9        |
|    value_loss           | 0.0937      |
-----------------------------------------
Eval num_timesteps=6480000, episode_reward=1.99 +/- 0.74
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.99       |
| time/                   |            |
|    total_timesteps      | 6480000    |
| train/                  |            |
|    approx_kl            | 0.06800838 |
|    clip_fraction        | 0.37       |
|    clip_range           | 0.2        |
|    entropy_loss         | -223       |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0005     |
|    loss                 | -2.23      |
|    n_updates            | 1970       |
|    policy_gradient_loss | -0.0181    |
|    std                  | 15.1       |
|    value_loss           | 0.0954     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.08     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 198      |
|    time_elapsed    | 31746    |
|    total_timesteps | 6488064  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.57      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 199       |
|    time_elapsed         | 31906     |
|    total_timesteps      | 6520832   |
| train/                  |           |
|    approx_kl            | 0.0664876 |
|    clip_fraction        | 0.362     |
|    clip_range           | 0.2       |
|    entropy_loss         | -223      |
|    explained_variance   | 0.916     |
|    learning_rate        | 0.0005    |
|    loss                 | -2.19     |
|    n_updates            | 1980      |
|    policy_gradient_loss | -0.0205   |
|    std                  | 15.2      |
|    value_loss           | 0.0876    |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.44      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 200       |
|    time_elapsed         | 32067     |
|    total_timesteps      | 6553600   |
| train/                  |           |
|    approx_kl            | 0.0646855 |
|    clip_fraction        | 0.366     |
|    clip_range           | 0.2       |
|    entropy_loss         | -224      |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0005    |
|    loss                 | -2.21     |
|    n_updates            | 1990      |
|    policy_gradient_loss | -0.0185   |
|    std                  | 15.3      |
|    value_loss           | 0.0973    |
---------------------------------------
Eval num_timesteps=6560000, episode_reward=2.88 +/- 0.58
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.88        |
| time/                   |             |
|    total_timesteps      | 6560000     |
| train/                  |             |
|    approx_kl            | 0.065152496 |
|    clip_fraction        | 0.356       |
|    clip_range           | 0.2         |
|    entropy_loss         | -224        |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0005      |
|    loss                 | -2.2        |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.0177     |
|    std                  | 15.5        |
|    value_loss           | 0.104       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.17     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 201      |
|    time_elapsed    | 32232    |
|    total_timesteps | 6586368  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.7        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 202        |
|    time_elapsed         | 32391      |
|    total_timesteps      | 6619136    |
| train/                  |            |
|    approx_kl            | 0.06333914 |
|    clip_fraction        | 0.369      |
|    clip_range           | 0.2        |
|    entropy_loss         | -225       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.21      |
|    n_updates            | 2010       |
|    policy_gradient_loss | -0.0195    |
|    std                  | 15.6       |
|    value_loss           | 0.108      |
----------------------------------------
Eval num_timesteps=6640000, episode_reward=1.99 +/- 1.19
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.99        |
| time/                   |             |
|    total_timesteps      | 6640000     |
| train/                  |             |
|    approx_kl            | 0.067549534 |
|    clip_fraction        | 0.367       |
|    clip_range           | 0.2         |
|    entropy_loss         | -225        |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.25       |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.0184     |
|    std                  | 15.7        |
|    value_loss           | 0.105       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.08     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 203      |
|    time_elapsed    | 32555    |
|    total_timesteps | 6651904  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.67       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 204        |
|    time_elapsed         | 32713      |
|    total_timesteps      | 6684672    |
| train/                  |            |
|    approx_kl            | 0.06897553 |
|    clip_fraction        | 0.38       |
|    clip_range           | 0.2        |
|    entropy_loss         | -226       |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.25      |
|    n_updates            | 2030       |
|    policy_gradient_loss | -0.0186    |
|    std                  | 15.9       |
|    value_loss           | 0.103      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.61       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 205        |
|    time_elapsed         | 32872      |
|    total_timesteps      | 6717440    |
| train/                  |            |
|    approx_kl            | 0.07122977 |
|    clip_fraction        | 0.378      |
|    clip_range           | 0.2        |
|    entropy_loss         | -226       |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.29      |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.0192    |
|    std                  | 16         |
|    value_loss           | 0.103      |
----------------------------------------
Eval num_timesteps=6720000, episode_reward=2.90 +/- 1.30
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.9        |
| time/                   |            |
|    total_timesteps      | 6720000    |
| train/                  |            |
|    approx_kl            | 0.06538341 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.2        |
|    entropy_loss         | -227       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.3       |
|    n_updates            | 2050       |
|    policy_gradient_loss | -0.0205    |
|    std                  | 16.2       |
|    value_loss           | 0.0897     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.63     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 206      |
|    time_elapsed    | 33034    |
|    total_timesteps | 6750208  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.12       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 207        |
|    time_elapsed         | 33190      |
|    total_timesteps      | 6782976    |
| train/                  |            |
|    approx_kl            | 0.06769115 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -227       |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.28      |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.0169    |
|    std                  | 16.3       |
|    value_loss           | 0.101      |
----------------------------------------
Eval num_timesteps=6800000, episode_reward=2.10 +/- 0.93
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.1        |
| time/                   |            |
|    total_timesteps      | 6800000    |
| train/                  |            |
|    approx_kl            | 0.06590573 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.2        |
|    entropy_loss         | -228       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.28      |
|    n_updates            | 2070       |
|    policy_gradient_loss | -0.0195    |
|    std                  | 16.5       |
|    value_loss           | 0.0892     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.52     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 208      |
|    time_elapsed    | 33355    |
|    total_timesteps | 6815744  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.02       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 209        |
|    time_elapsed         | 33512      |
|    total_timesteps      | 6848512    |
| train/                  |            |
|    approx_kl            | 0.06448811 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -228       |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.31      |
|    n_updates            | 2080       |
|    policy_gradient_loss | -0.0189    |
|    std                  | 16.6       |
|    value_loss           | 0.0961     |
----------------------------------------
Eval num_timesteps=6880000, episode_reward=3.08 +/- 0.80
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 3.08        |
| time/                   |             |
|    total_timesteps      | 6880000     |
| train/                  |             |
|    approx_kl            | 0.061155442 |
|    clip_fraction        | 0.357       |
|    clip_range           | 0.2         |
|    entropy_loss         | -229        |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0005      |
|    loss                 | -2.27       |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.0177     |
|    std                  | 16.8        |
|    value_loss           | 0.0973      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.96     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 210      |
|    time_elapsed    | 33672    |
|    total_timesteps | 6881280  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.66       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 211        |
|    time_elapsed         | 33832      |
|    total_timesteps      | 6914048    |
| train/                  |            |
|    approx_kl            | 0.06667392 |
|    clip_fraction        | 0.376      |
|    clip_range           | 0.2        |
|    entropy_loss         | -229       |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.28      |
|    n_updates            | 2100       |
|    policy_gradient_loss | -0.02      |
|    std                  | 17         |
|    value_loss           | 0.0995     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.73        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 212         |
|    time_elapsed         | 33992       |
|    total_timesteps      | 6946816     |
| train/                  |             |
|    approx_kl            | 0.063502096 |
|    clip_fraction        | 0.349       |
|    clip_range           | 0.2         |
|    entropy_loss         | -230        |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.31       |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.019      |
|    std                  | 17.1        |
|    value_loss           | 0.0984      |
-----------------------------------------
Eval num_timesteps=6960000, episode_reward=2.33 +/- 0.54
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.33       |
| time/                   |            |
|    total_timesteps      | 6960000    |
| train/                  |            |
|    approx_kl            | 0.06576574 |
|    clip_fraction        | 0.362      |
|    clip_range           | 0.2        |
|    entropy_loss         | -230       |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.27      |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.0191    |
|    std                  | 17.3       |
|    value_loss           | 0.0971     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.31     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 213      |
|    time_elapsed    | 34157    |
|    total_timesteps | 6979584  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.66       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 214        |
|    time_elapsed         | 34316      |
|    total_timesteps      | 7012352    |
| train/                  |            |
|    approx_kl            | 0.06512462 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.2        |
|    entropy_loss         | -231       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.25      |
|    n_updates            | 2130       |
|    policy_gradient_loss | -0.019     |
|    std                  | 17.4       |
|    value_loss           | 0.0951     |
----------------------------------------
Eval num_timesteps=7040000, episode_reward=3.11 +/- 1.43
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 3.11        |
| time/                   |             |
|    total_timesteps      | 7040000     |
| train/                  |             |
|    approx_kl            | 0.062454186 |
|    clip_fraction        | 0.355       |
|    clip_range           | 0.2         |
|    entropy_loss         | -231        |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.34       |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.0179     |
|    std                  | 17.6        |
|    value_loss           | 0.102       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.42     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 215      |
|    time_elapsed    | 34480    |
|    total_timesteps | 7045120  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.69       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 216        |
|    time_elapsed         | 34641      |
|    total_timesteps      | 7077888    |
| train/                  |            |
|    approx_kl            | 0.06235782 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.2        |
|    entropy_loss         | -232       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.34      |
|    n_updates            | 2150       |
|    policy_gradient_loss | -0.0188    |
|    std                  | 17.7       |
|    value_loss           | 0.111      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.2         |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 217         |
|    time_elapsed         | 34800       |
|    total_timesteps      | 7110656     |
| train/                  |             |
|    approx_kl            | 0.065314025 |
|    clip_fraction        | 0.358       |
|    clip_range           | 0.2         |
|    entropy_loss         | -232        |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.28       |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.0201     |
|    std                  | 17.9        |
|    value_loss           | 0.103       |
-----------------------------------------
Eval num_timesteps=7120000, episode_reward=1.79 +/- 1.15
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.79        |
| time/                   |             |
|    total_timesteps      | 7120000     |
| train/                  |             |
|    approx_kl            | 0.061704487 |
|    clip_fraction        | 0.351       |
|    clip_range           | 0.2         |
|    entropy_loss         | -232        |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.31       |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.0191     |
|    std                  | 18          |
|    value_loss           | 0.103       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.63     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 218      |
|    time_elapsed    | 34963    |
|    total_timesteps | 7143424  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.62        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 219         |
|    time_elapsed         | 35122       |
|    total_timesteps      | 7176192     |
| train/                  |             |
|    approx_kl            | 0.057357438 |
|    clip_fraction        | 0.361       |
|    clip_range           | 0.2         |
|    entropy_loss         | -233        |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.35       |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 18.2        |
|    value_loss           | 0.0888      |
-----------------------------------------
Eval num_timesteps=7200000, episode_reward=2.95 +/- 0.56
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.95        |
| time/                   |             |
|    total_timesteps      | 7200000     |
| train/                  |             |
|    approx_kl            | 0.059156187 |
|    clip_fraction        | 0.342       |
|    clip_range           | 0.2         |
|    entropy_loss         | -233        |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.38       |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.019      |
|    std                  | 18.3        |
|    value_loss           | 0.105       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.73     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 220      |
|    time_elapsed    | 35285    |
|    total_timesteps | 7208960  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.19        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 221         |
|    time_elapsed         | 35442       |
|    total_timesteps      | 7241728     |
| train/                  |             |
|    approx_kl            | 0.056798615 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.2         |
|    entropy_loss         | -234        |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.37       |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.0203     |
|    std                  | 18.5        |
|    value_loss           | 0.106       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.5        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 222        |
|    time_elapsed         | 35599      |
|    total_timesteps      | 7274496    |
| train/                  |            |
|    approx_kl            | 0.06099022 |
|    clip_fraction        | 0.355      |
|    clip_range           | 0.2        |
|    entropy_loss         | -234       |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0005     |
|    loss                 | -2.34      |
|    n_updates            | 2210       |
|    policy_gradient_loss | -0.0173    |
|    std                  | 18.6       |
|    value_loss           | 0.11       |
----------------------------------------
Eval num_timesteps=7280000, episode_reward=2.43 +/- 0.80
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.43       |
| time/                   |            |
|    total_timesteps      | 7280000    |
| train/                  |            |
|    approx_kl            | 0.06242947 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.2        |
|    entropy_loss         | -235       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.27      |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0194    |
|    std                  | 18.8       |
|    value_loss           | 0.107      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.4      |
| time/              |          |
|    fps             | 204      |
|    iterations      | 223      |
|    time_elapsed    | 35760    |
|    total_timesteps | 7307264  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.55        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 224         |
|    time_elapsed         | 35919       |
|    total_timesteps      | 7340032     |
| train/                  |             |
|    approx_kl            | 0.063272186 |
|    clip_fraction        | 0.361       |
|    clip_range           | 0.2         |
|    entropy_loss         | -235        |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.36       |
|    n_updates            | 2230        |
|    policy_gradient_loss | -0.0205     |
|    std                  | 18.9        |
|    value_loss           | 0.0843      |
-----------------------------------------
Eval num_timesteps=7360000, episode_reward=0.92 +/- 0.50
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 0.918       |
| time/                   |             |
|    total_timesteps      | 7360000     |
| train/                  |             |
|    approx_kl            | 0.057535388 |
|    clip_fraction        | 0.352       |
|    clip_range           | 0.2         |
|    entropy_loss         | -236        |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.35       |
|    n_updates            | 2240        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 19.1        |
|    value_loss           | 0.0891      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.52     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 225      |
|    time_elapsed    | 36083    |
|    total_timesteps | 7372800  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 288      |
|    ep_rew_mean          | 5.01     |
| time/                   |          |
|    fps                  | 204      |
|    iterations           | 226      |
|    time_elapsed         | 36242    |
|    total_timesteps      | 7405568  |
| train/                  |          |
|    approx_kl            | 0.059504 |
|    clip_fraction        | 0.34     |
|    clip_range           | 0.2      |
|    entropy_loss         | -236     |
|    explained_variance   | 0.916    |
|    learning_rate        | 0.0005   |
|    loss                 | -2.39    |
|    n_updates            | 2250     |
|    policy_gradient_loss | -0.0196  |
|    std                  | 19.3     |
|    value_loss           | 0.1      |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.79       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 227        |
|    time_elapsed         | 36400      |
|    total_timesteps      | 7438336    |
| train/                  |            |
|    approx_kl            | 0.05949553 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.2        |
|    entropy_loss         | -237       |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.38      |
|    n_updates            | 2260       |
|    policy_gradient_loss | -0.0187    |
|    std                  | 19.5       |
|    value_loss           | 0.0961     |
----------------------------------------
Eval num_timesteps=7440000, episode_reward=2.46 +/- 1.15
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.46        |
| time/                   |             |
|    total_timesteps      | 7440000     |
| train/                  |             |
|    approx_kl            | 0.056152917 |
|    clip_fraction        | 0.338       |
|    clip_range           | 0.2         |
|    entropy_loss         | -237        |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.37       |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.0183     |
|    std                  | 19.6        |
|    value_loss           | 0.101       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.76     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 228      |
|    time_elapsed    | 36564    |
|    total_timesteps | 7471104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 4.86        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 229         |
|    time_elapsed         | 36722       |
|    total_timesteps      | 7503872     |
| train/                  |             |
|    approx_kl            | 0.059728406 |
|    clip_fraction        | 0.356       |
|    clip_range           | 0.2         |
|    entropy_loss         | -237        |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.38       |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.0203     |
|    std                  | 19.7        |
|    value_loss           | 0.0993      |
-----------------------------------------
Eval num_timesteps=7520000, episode_reward=2.71 +/- 0.54
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.71        |
| time/                   |             |
|    total_timesteps      | 7520000     |
| train/                  |             |
|    approx_kl            | 0.056225955 |
|    clip_fraction        | 0.339       |
|    clip_range           | 0.2         |
|    entropy_loss         | -238        |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.41       |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 19.9        |
|    value_loss           | 0.0986      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.21     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 230      |
|    time_elapsed    | 36881    |
|    total_timesteps | 7536640  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 4.76        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 231         |
|    time_elapsed         | 37036       |
|    total_timesteps      | 7569408     |
| train/                  |             |
|    approx_kl            | 0.060161613 |
|    clip_fraction        | 0.361       |
|    clip_range           | 0.2         |
|    entropy_loss         | -238        |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0005      |
|    loss                 | -2.4        |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.0191     |
|    std                  | 20.1        |
|    value_loss           | 0.0977      |
-----------------------------------------
Eval num_timesteps=7600000, episode_reward=2.51 +/- 0.83
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.51       |
| time/                   |            |
|    total_timesteps      | 7600000    |
| train/                  |            |
|    approx_kl            | 0.05772523 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.2        |
|    entropy_loss         | -239       |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0005     |
|    loss                 | -2.38      |
|    n_updates            | 2310       |
|    policy_gradient_loss | -0.0174    |
|    std                  | 20.3       |
|    value_loss           | 0.107      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.22     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 232      |
|    time_elapsed    | 37198    |
|    total_timesteps | 7602176  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.27        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 233         |
|    time_elapsed         | 37357       |
|    total_timesteps      | 7634944     |
| train/                  |             |
|    approx_kl            | 0.057460852 |
|    clip_fraction        | 0.343       |
|    clip_range           | 0.2         |
|    entropy_loss         | -239        |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.42       |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.02       |
|    std                  | 20.4        |
|    value_loss           | 0.106       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.47       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 234        |
|    time_elapsed         | 37517      |
|    total_timesteps      | 7667712    |
| train/                  |            |
|    approx_kl            | 0.05546266 |
|    clip_fraction        | 0.347      |
|    clip_range           | 0.2        |
|    entropy_loss         | -240       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.43      |
|    n_updates            | 2330       |
|    policy_gradient_loss | -0.0196    |
|    std                  | 20.6       |
|    value_loss           | 0.0957     |
----------------------------------------
Eval num_timesteps=7680000, episode_reward=1.93 +/- 0.77
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.93        |
| time/                   |             |
|    total_timesteps      | 7680000     |
| train/                  |             |
|    approx_kl            | 0.058637224 |
|    clip_fraction        | 0.346       |
|    clip_range           | 0.2         |
|    entropy_loss         | -240        |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.4        |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.0226     |
|    std                  | 20.7        |
|    value_loss           | 0.0961      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.12     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 235      |
|    time_elapsed    | 37683    |
|    total_timesteps | 7700480  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.07        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 236         |
|    time_elapsed         | 37837       |
|    total_timesteps      | 7733248     |
| train/                  |             |
|    approx_kl            | 0.057171203 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.2         |
|    entropy_loss         | -240        |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.41       |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.0201     |
|    std                  | 20.9        |
|    value_loss           | 0.0891      |
-----------------------------------------
Eval num_timesteps=7760000, episode_reward=2.03 +/- 0.39
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 2.03      |
| time/                   |           |
|    total_timesteps      | 7760000   |
| train/                  |           |
|    approx_kl            | 0.0584167 |
|    clip_fraction        | 0.348     |
|    clip_range           | 0.2       |
|    entropy_loss         | -241      |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0005    |
|    loss                 | -2.37     |
|    n_updates            | 2360      |
|    policy_gradient_loss | -0.0201   |
|    std                  | 21.1      |
|    value_loss           | 0.0869    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.93     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 237      |
|    time_elapsed    | 37999    |
|    total_timesteps | 7766016  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.74       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 238        |
|    time_elapsed         | 38157      |
|    total_timesteps      | 7798784    |
| train/                  |            |
|    approx_kl            | 0.05542548 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.2        |
|    entropy_loss         | -241       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.41      |
|    n_updates            | 2370       |
|    policy_gradient_loss | -0.0197    |
|    std                  | 21.2       |
|    value_loss           | 0.101      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.54       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 239        |
|    time_elapsed         | 38316      |
|    total_timesteps      | 7831552    |
| train/                  |            |
|    approx_kl            | 0.05652358 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.2        |
|    entropy_loss         | -242       |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.41      |
|    n_updates            | 2380       |
|    policy_gradient_loss | -0.0212    |
|    std                  | 21.4       |
|    value_loss           | 0.0999     |
----------------------------------------
Eval num_timesteps=7840000, episode_reward=2.46 +/- 1.90
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.46        |
| time/                   |             |
|    total_timesteps      | 7840000     |
| train/                  |             |
|    approx_kl            | 0.057994317 |
|    clip_fraction        | 0.345       |
|    clip_range           | 0.2         |
|    entropy_loss         | -242        |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.41       |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.0204     |
|    std                  | 21.6        |
|    value_loss           | 0.106       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.05     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 240      |
|    time_elapsed    | 38480    |
|    total_timesteps | 7864320  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.38        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 241         |
|    time_elapsed         | 38638       |
|    total_timesteps      | 7897088     |
| train/                  |             |
|    approx_kl            | 0.058878876 |
|    clip_fraction        | 0.351       |
|    clip_range           | 0.2         |
|    entropy_loss         | -243        |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.42       |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.0195     |
|    std                  | 21.8        |
|    value_loss           | 0.104       |
-----------------------------------------
Eval num_timesteps=7920000, episode_reward=1.96 +/- 0.65
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.96       |
| time/                   |            |
|    total_timesteps      | 7920000    |
| train/                  |            |
|    approx_kl            | 0.05616326 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.2        |
|    entropy_loss         | -243       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.4       |
|    n_updates            | 2410       |
|    policy_gradient_loss | -0.0182    |
|    std                  | 22         |
|    value_loss           | 0.0945     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.05     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 242      |
|    time_elapsed    | 38799    |
|    total_timesteps | 7929856  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.32        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 243         |
|    time_elapsed         | 38956       |
|    total_timesteps      | 7962624     |
| train/                  |             |
|    approx_kl            | 0.053342775 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.2         |
|    entropy_loss         | -244        |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.44       |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.02       |
|    std                  | 22.2        |
|    value_loss           | 0.0954      |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 4.63      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 244       |
|    time_elapsed         | 39113     |
|    total_timesteps      | 7995392   |
| train/                  |           |
|    approx_kl            | 0.0553058 |
|    clip_fraction        | 0.329     |
|    clip_range           | 0.2       |
|    entropy_loss         | -244      |
|    explained_variance   | 0.916     |
|    learning_rate        | 0.0005    |
|    loss                 | -2.41     |
|    n_updates            | 2430      |
|    policy_gradient_loss | -0.0182   |
|    std                  | 22.4      |
|    value_loss           | 0.0922    |
---------------------------------------
Eval num_timesteps=8000000, episode_reward=1.87 +/- 1.04
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.87       |
| time/                   |            |
|    total_timesteps      | 8000000    |
| train/                  |            |
|    approx_kl            | 0.05479215 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -245       |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.43      |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.0192    |
|    std                  | 22.6       |
|    value_loss           | 0.0913     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.58     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 245      |
|    time_elapsed    | 39276    |
|    total_timesteps | 8028160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.04        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 246         |
|    time_elapsed         | 39429       |
|    total_timesteps      | 8060928     |
| train/                  |             |
|    approx_kl            | 0.056963015 |
|    clip_fraction        | 0.348       |
|    clip_range           | 0.2         |
|    entropy_loss         | -245        |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.47       |
|    n_updates            | 2450        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 22.8        |
|    value_loss           | 0.092       |
-----------------------------------------
Eval num_timesteps=8080000, episode_reward=2.11 +/- 1.12
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.11       |
| time/                   |            |
|    total_timesteps      | 8080000    |
| train/                  |            |
|    approx_kl            | 0.05322659 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.2        |
|    entropy_loss         | -245       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.49      |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.0204    |
|    std                  | 22.9       |
|    value_loss           | 0.099      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.09     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 247      |
|    time_elapsed    | 39590    |
|    total_timesteps | 8093696  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.32       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 248        |
|    time_elapsed         | 39749      |
|    total_timesteps      | 8126464    |
| train/                  |            |
|    approx_kl            | 0.05653879 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.2        |
|    entropy_loss         | -246       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.45      |
|    n_updates            | 2470       |
|    policy_gradient_loss | -0.0179    |
|    std                  | 23.1       |
|    value_loss           | 0.091      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.8        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 249        |
|    time_elapsed         | 39907      |
|    total_timesteps      | 8159232    |
| train/                  |            |
|    approx_kl            | 0.05607366 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -246       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.51      |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.018     |
|    std                  | 23.3       |
|    value_loss           | 0.103      |
----------------------------------------
Eval num_timesteps=8160000, episode_reward=2.11 +/- 0.66
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.11        |
| time/                   |             |
|    total_timesteps      | 8160000     |
| train/                  |             |
|    approx_kl            | 0.054444224 |
|    clip_fraction        | 0.332       |
|    clip_range           | 0.2         |
|    entropy_loss         | -247        |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.46       |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.0204     |
|    std                  | 23.5        |
|    value_loss           | 0.098       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.73     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 250      |
|    time_elapsed    | 40066    |
|    total_timesteps | 8192000  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.33        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 251         |
|    time_elapsed         | 40217       |
|    total_timesteps      | 8224768     |
| train/                  |             |
|    approx_kl            | 0.051455505 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    entropy_loss         | -247        |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.5        |
|    n_updates            | 2500        |
|    policy_gradient_loss | -0.0191     |
|    std                  | 23.7        |
|    value_loss           | 0.0928      |
-----------------------------------------
Eval num_timesteps=8240000, episode_reward=2.72 +/- 1.53
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.72        |
| time/                   |             |
|    total_timesteps      | 8240000     |
| train/                  |             |
|    approx_kl            | 0.056878332 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.2         |
|    entropy_loss         | -248        |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.49       |
|    n_updates            | 2510        |
|    policy_gradient_loss | -0.0205     |
|    std                  | 23.9        |
|    value_loss           | 0.0978      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.97     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 252      |
|    time_elapsed    | 40381    |
|    total_timesteps | 8257536  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 4.73        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 253         |
|    time_elapsed         | 40537       |
|    total_timesteps      | 8290304     |
| train/                  |             |
|    approx_kl            | 0.053356595 |
|    clip_fraction        | 0.324       |
|    clip_range           | 0.2         |
|    entropy_loss         | -248        |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.47       |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 24.2        |
|    value_loss           | 0.0973      |
-----------------------------------------
Eval num_timesteps=8320000, episode_reward=1.52 +/- 0.76
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 1.52      |
| time/                   |           |
|    total_timesteps      | 8320000   |
| train/                  |           |
|    approx_kl            | 0.0514381 |
|    clip_fraction        | 0.325     |
|    clip_range           | 0.2       |
|    entropy_loss         | -249      |
|    explained_variance   | 0.903     |
|    learning_rate        | 0.0005    |
|    loss                 | -2.5      |
|    n_updates            | 2530      |
|    policy_gradient_loss | -0.0182   |
|    std                  | 24.4      |
|    value_loss           | 0.113     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.29     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 254      |
|    time_elapsed    | 40699    |
|    total_timesteps | 8323072  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.68       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 255        |
|    time_elapsed         | 40857      |
|    total_timesteps      | 8355840    |
| train/                  |            |
|    approx_kl            | 0.05039514 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    entropy_loss         | -249       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.5       |
|    n_updates            | 2540       |
|    policy_gradient_loss | -0.0179    |
|    std                  | 24.6       |
|    value_loss           | 0.0979     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.67       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 256        |
|    time_elapsed         | 41014      |
|    total_timesteps      | 8388608    |
| train/                  |            |
|    approx_kl            | 0.05341082 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -250       |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0005     |
|    loss                 | -2.46      |
|    n_updates            | 2550       |
|    policy_gradient_loss | -0.0195    |
|    std                  | 24.8       |
|    value_loss           | 0.112      |
----------------------------------------
Eval num_timesteps=8400000, episode_reward=2.74 +/- 1.03
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.74        |
| time/                   |             |
|    total_timesteps      | 8400000     |
| train/                  |             |
|    approx_kl            | 0.052226014 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -250        |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.52       |
|    n_updates            | 2560        |
|    policy_gradient_loss | -0.0189     |
|    std                  | 25          |
|    value_loss           | 0.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.49     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 257      |
|    time_elapsed    | 41171    |
|    total_timesteps | 8421376  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.24        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 258         |
|    time_elapsed         | 41329       |
|    total_timesteps      | 8454144     |
| train/                  |             |
|    approx_kl            | 0.052692674 |
|    clip_fraction        | 0.324       |
|    clip_range           | 0.2         |
|    entropy_loss         | -251        |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.49       |
|    n_updates            | 2570        |
|    policy_gradient_loss | -0.0187     |
|    std                  | 25.2        |
|    value_loss           | 0.115       |
-----------------------------------------
Eval num_timesteps=8480000, episode_reward=1.58 +/- 0.52
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.58        |
| time/                   |             |
|    total_timesteps      | 8480000     |
| train/                  |             |
|    approx_kl            | 0.056146692 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.2         |
|    entropy_loss         | -251        |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.53       |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 25.4        |
|    value_loss           | 0.107       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.57     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 259      |
|    time_elapsed    | 41495    |
|    total_timesteps | 8486912  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.92       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 260        |
|    time_elapsed         | 41650      |
|    total_timesteps      | 8519680    |
| train/                  |            |
|    approx_kl            | 0.05494118 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.2        |
|    entropy_loss         | -252       |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.5       |
|    n_updates            | 2590       |
|    policy_gradient_loss | -0.0183    |
|    std                  | 25.6       |
|    value_loss           | 0.111      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.21        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 261         |
|    time_elapsed         | 41805       |
|    total_timesteps      | 8552448     |
| train/                  |             |
|    approx_kl            | 0.051443174 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -252        |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.52       |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.0185     |
|    std                  | 25.9        |
|    value_loss           | 0.11        |
-----------------------------------------
Eval num_timesteps=8560000, episode_reward=1.61 +/- 0.46
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.61        |
| time/                   |             |
|    total_timesteps      | 8560000     |
| train/                  |             |
|    approx_kl            | 0.052964244 |
|    clip_fraction        | 0.331       |
|    clip_range           | 0.2         |
|    entropy_loss         | -252        |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.52       |
|    n_updates            | 2610        |
|    policy_gradient_loss | -0.0181     |
|    std                  | 26.1        |
|    value_loss           | 0.102       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.1      |
| time/              |          |
|    fps             | 204      |
|    iterations      | 262      |
|    time_elapsed    | 41968    |
|    total_timesteps | 8585216  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.2         |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 263         |
|    time_elapsed         | 42126       |
|    total_timesteps      | 8617984     |
| train/                  |             |
|    approx_kl            | 0.051246047 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -253        |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.53       |
|    n_updates            | 2620        |
|    policy_gradient_loss | -0.0181     |
|    std                  | 26.3        |
|    value_loss           | 0.0958      |
-----------------------------------------
Eval num_timesteps=8640000, episode_reward=1.88 +/- 0.38
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.88        |
| time/                   |             |
|    total_timesteps      | 8640000     |
| train/                  |             |
|    approx_kl            | 0.051827807 |
|    clip_fraction        | 0.324       |
|    clip_range           | 0.2         |
|    entropy_loss         | -253        |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0005      |
|    loss                 | -2.46       |
|    n_updates            | 2630        |
|    policy_gradient_loss | -0.0193     |
|    std                  | 26.5        |
|    value_loss           | 0.1         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.33     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 264      |
|    time_elapsed    | 42281    |
|    total_timesteps | 8650752  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.11       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 265        |
|    time_elapsed         | 42440      |
|    total_timesteps      | 8683520    |
| train/                  |            |
|    approx_kl            | 0.05209011 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -254       |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.58      |
|    n_updates            | 2640       |
|    policy_gradient_loss | -0.0184    |
|    std                  | 26.7       |
|    value_loss           | 0.101      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.21       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 266        |
|    time_elapsed         | 42600      |
|    total_timesteps      | 8716288    |
| train/                  |            |
|    approx_kl            | 0.05184368 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.2        |
|    entropy_loss         | -254       |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.55      |
|    n_updates            | 2650       |
|    policy_gradient_loss | -0.0192    |
|    std                  | 26.9       |
|    value_loss           | 0.0978     |
----------------------------------------
Eval num_timesteps=8720000, episode_reward=1.46 +/- 0.89
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 8720000     |
| train/                  |             |
|    approx_kl            | 0.052207287 |
|    clip_fraction        | 0.325       |
|    clip_range           | 0.2         |
|    entropy_loss         | -255        |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.49       |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.0189     |
|    std                  | 27.1        |
|    value_loss           | 0.101       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.5      |
| time/              |          |
|    fps             | 204      |
|    iterations      | 267      |
|    time_elapsed    | 42763    |
|    total_timesteps | 8749056  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.36        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 268         |
|    time_elapsed         | 42918       |
|    total_timesteps      | 8781824     |
| train/                  |             |
|    approx_kl            | 0.050956763 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -255        |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.56       |
|    n_updates            | 2670        |
|    policy_gradient_loss | -0.0188     |
|    std                  | 27.3        |
|    value_loss           | 0.101       |
-----------------------------------------
Eval num_timesteps=8800000, episode_reward=1.40 +/- 0.45
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 8800000     |
| train/                  |             |
|    approx_kl            | 0.050467923 |
|    clip_fraction        | 0.322       |
|    clip_range           | 0.2         |
|    entropy_loss         | -255        |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.54       |
|    n_updates            | 2680        |
|    policy_gradient_loss | -0.02       |
|    std                  | 27.6        |
|    value_loss           | 0.105       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.31     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 269      |
|    time_elapsed    | 43081    |
|    total_timesteps | 8814592  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.37        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 270         |
|    time_elapsed         | 43236       |
|    total_timesteps      | 8847360     |
| train/                  |             |
|    approx_kl            | 0.050028764 |
|    clip_fraction        | 0.319       |
|    clip_range           | 0.2         |
|    entropy_loss         | -256        |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.58       |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.0196     |
|    std                  | 27.8        |
|    value_loss           | 0.1         |
-----------------------------------------
Eval num_timesteps=8880000, episode_reward=2.57 +/- 0.62
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.57        |
| time/                   |             |
|    total_timesteps      | 8880000     |
| train/                  |             |
|    approx_kl            | 0.050074004 |
|    clip_fraction        | 0.318       |
|    clip_range           | 0.2         |
|    entropy_loss         | -256        |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.59       |
|    n_updates            | 2700        |
|    policy_gradient_loss | -0.0189     |
|    std                  | 28          |
|    value_loss           | 0.1         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.87     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 271      |
|    time_elapsed    | 43389    |
|    total_timesteps | 8880128  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.39        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 272         |
|    time_elapsed         | 43545       |
|    total_timesteps      | 8912896     |
| train/                  |             |
|    approx_kl            | 0.048215143 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -257        |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.55       |
|    n_updates            | 2710        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 28.1        |
|    value_loss           | 0.0999      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.24       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 273        |
|    time_elapsed         | 43701      |
|    total_timesteps      | 8945664    |
| train/                  |            |
|    approx_kl            | 0.04791791 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -257       |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.61      |
|    n_updates            | 2720       |
|    policy_gradient_loss | -0.0189    |
|    std                  | 28.3       |
|    value_loss           | 0.0994     |
----------------------------------------
Eval num_timesteps=8960000, episode_reward=2.49 +/- 0.91
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.49        |
| time/                   |             |
|    total_timesteps      | 8960000     |
| train/                  |             |
|    approx_kl            | 0.046521097 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    entropy_loss         | -257        |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.57       |
|    n_updates            | 2730        |
|    policy_gradient_loss | -0.0186     |
|    std                  | 28.6        |
|    value_loss           | 0.107       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.48     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 274      |
|    time_elapsed    | 43861    |
|    total_timesteps | 8978432  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 4.43        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 275         |
|    time_elapsed         | 44017       |
|    total_timesteps      | 9011200     |
| train/                  |             |
|    approx_kl            | 0.049908727 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    entropy_loss         | -258        |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.52       |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 28.8        |
|    value_loss           | 0.109       |
-----------------------------------------
Eval num_timesteps=9040000, episode_reward=1.91 +/- 1.12
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.91        |
| time/                   |             |
|    total_timesteps      | 9040000     |
| train/                  |             |
|    approx_kl            | 0.050097097 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.2         |
|    entropy_loss         | -258        |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.58       |
|    n_updates            | 2750        |
|    policy_gradient_loss | -0.0177     |
|    std                  | 29          |
|    value_loss           | 0.095       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.45     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 276      |
|    time_elapsed    | 44166    |
|    total_timesteps | 9043968  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.84       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 277        |
|    time_elapsed         | 44326      |
|    total_timesteps      | 9076736    |
| train/                  |            |
|    approx_kl            | 0.04979039 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -259       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.6       |
|    n_updates            | 2760       |
|    policy_gradient_loss | -0.0195    |
|    std                  | 29.3       |
|    value_loss           | 0.0973     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 4.79        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 278         |
|    time_elapsed         | 44484       |
|    total_timesteps      | 9109504     |
| train/                  |             |
|    approx_kl            | 0.048279013 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -259        |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.6        |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 29.5        |
|    value_loss           | 0.0912      |
-----------------------------------------
Eval num_timesteps=9120000, episode_reward=1.99 +/- 1.16
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.99       |
| time/                   |            |
|    total_timesteps      | 9120000    |
| train/                  |            |
|    approx_kl            | 0.04643143 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -260       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.62      |
|    n_updates            | 2780       |
|    policy_gradient_loss | -0.0189    |
|    std                  | 29.8       |
|    value_loss           | 0.0917     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.46     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 279      |
|    time_elapsed    | 44648    |
|    total_timesteps | 9142272  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.22       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 280        |
|    time_elapsed         | 44805      |
|    total_timesteps      | 9175040    |
| train/                  |            |
|    approx_kl            | 0.04396074 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.2        |
|    entropy_loss         | -260       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.56      |
|    n_updates            | 2790       |
|    policy_gradient_loss | -0.0195    |
|    std                  | 30         |
|    value_loss           | 0.0987     |
----------------------------------------
Eval num_timesteps=9200000, episode_reward=1.52 +/- 1.28
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.52        |
| time/                   |             |
|    total_timesteps      | 9200000     |
| train/                  |             |
|    approx_kl            | 0.046361573 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -260        |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.64       |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.0188     |
|    std                  | 30.2        |
|    value_loss           | 0.0992      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.92     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 281      |
|    time_elapsed    | 44969    |
|    total_timesteps | 9207808  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.43        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 282         |
|    time_elapsed         | 45128       |
|    total_timesteps      | 9240576     |
| train/                  |             |
|    approx_kl            | 0.046780907 |
|    clip_fraction        | 0.306       |
|    clip_range           | 0.2         |
|    entropy_loss         | -261        |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.6        |
|    n_updates            | 2810        |
|    policy_gradient_loss | -0.0196     |
|    std                  | 30.5        |
|    value_loss           | 0.105       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.42        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 283         |
|    time_elapsed         | 45287       |
|    total_timesteps      | 9273344     |
| train/                  |             |
|    approx_kl            | 0.048073106 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -261        |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.59       |
|    n_updates            | 2820        |
|    policy_gradient_loss | -0.0185     |
|    std                  | 30.7        |
|    value_loss           | 0.102       |
-----------------------------------------
Eval num_timesteps=9280000, episode_reward=2.45 +/- 1.91
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 2.45      |
| time/                   |           |
|    total_timesteps      | 9280000   |
| train/                  |           |
|    approx_kl            | 0.0469953 |
|    clip_fraction        | 0.309     |
|    clip_range           | 0.2       |
|    entropy_loss         | -262      |
|    explained_variance   | 0.917     |
|    learning_rate        | 0.0005    |
|    loss                 | -2.6      |
|    n_updates            | 2830      |
|    policy_gradient_loss | -0.0192   |
|    std                  | 30.9      |
|    value_loss           | 0.102     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.28     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 284      |
|    time_elapsed    | 45450    |
|    total_timesteps | 9306112  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.24       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 285        |
|    time_elapsed         | 45611      |
|    total_timesteps      | 9338880    |
| train/                  |            |
|    approx_kl            | 0.04744726 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    entropy_loss         | -262       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.61      |
|    n_updates            | 2840       |
|    policy_gradient_loss | -0.018     |
|    std                  | 31.1       |
|    value_loss           | 0.0988     |
----------------------------------------
Eval num_timesteps=9360000, episode_reward=2.14 +/- 1.12
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 2.14        |
| time/                   |             |
|    total_timesteps      | 9360000     |
| train/                  |             |
|    approx_kl            | 0.048164286 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    entropy_loss         | -262        |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.59       |
|    n_updates            | 2850        |
|    policy_gradient_loss | -0.019      |
|    std                  | 31.3        |
|    value_loss           | 0.1         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.24     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 286      |
|    time_elapsed    | 45774    |
|    total_timesteps | 9371648  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.41        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 287         |
|    time_elapsed         | 45934       |
|    total_timesteps      | 9404416     |
| train/                  |             |
|    approx_kl            | 0.046442997 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -263        |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.6        |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.0196     |
|    std                  | 31.6        |
|    value_loss           | 0.095       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.71        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 288         |
|    time_elapsed         | 46096       |
|    total_timesteps      | 9437184     |
| train/                  |             |
|    approx_kl            | 0.047820274 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -263        |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.62       |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.0189     |
|    std                  | 31.8        |
|    value_loss           | 0.0886      |
-----------------------------------------
Eval num_timesteps=9440000, episode_reward=1.90 +/- 0.70
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 9440000     |
| train/                  |             |
|    approx_kl            | 0.044700325 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -264        |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.65       |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.0185     |
|    std                  | 32.1        |
|    value_loss           | 0.0875      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.36     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 289      |
|    time_elapsed    | 46260    |
|    total_timesteps | 9469952  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.96        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 290         |
|    time_elapsed         | 46420       |
|    total_timesteps      | 9502720     |
| train/                  |             |
|    approx_kl            | 0.047037017 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.2         |
|    entropy_loss         | -264        |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.65       |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 32.4        |
|    value_loss           | 0.0885      |
-----------------------------------------
Eval num_timesteps=9520000, episode_reward=3.51 +/- 0.49
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 3.51       |
| time/                   |            |
|    total_timesteps      | 9520000    |
| train/                  |            |
|    approx_kl            | 0.04511854 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -265       |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.6       |
|    n_updates            | 2900       |
|    policy_gradient_loss | -0.0195    |
|    std                  | 32.7       |
|    value_loss           | 0.091      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.17     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 291      |
|    time_elapsed    | 46584    |
|    total_timesteps | 9535488  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.62       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 292        |
|    time_elapsed         | 46744      |
|    total_timesteps      | 9568256    |
| train/                  |            |
|    approx_kl            | 0.04431486 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.2        |
|    entropy_loss         | -265       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.65      |
|    n_updates            | 2910       |
|    policy_gradient_loss | -0.02      |
|    std                  | 32.9       |
|    value_loss           | 0.0998     |
----------------------------------------
Eval num_timesteps=9600000, episode_reward=1.35 +/- 1.01
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.35        |
| time/                   |             |
|    total_timesteps      | 9600000     |
| train/                  |             |
|    approx_kl            | 0.045986306 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.2         |
|    entropy_loss         | -265        |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0005      |
|    loss                 | -2.66       |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 33.1        |
|    value_loss           | 0.109       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.74     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 293      |
|    time_elapsed    | 46909    |
|    total_timesteps | 9601024  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.27        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 294         |
|    time_elapsed         | 47068       |
|    total_timesteps      | 9633792     |
| train/                  |             |
|    approx_kl            | 0.048220962 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    entropy_loss         | -266        |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.63       |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 33.3        |
|    value_loss           | 0.101       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 6.36        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 295         |
|    time_elapsed         | 47230       |
|    total_timesteps      | 9666560     |
| train/                  |             |
|    approx_kl            | 0.045199096 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -266        |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.6        |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.0189     |
|    std                  | 33.6        |
|    value_loss           | 0.0961      |
-----------------------------------------
Eval num_timesteps=9680000, episode_reward=2.55 +/- 0.78
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.55       |
| time/                   |            |
|    total_timesteps      | 9680000    |
| train/                  |            |
|    approx_kl            | 0.04662507 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.2        |
|    entropy_loss         | -267       |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.71      |
|    n_updates            | 2950       |
|    policy_gradient_loss | -0.0192    |
|    std                  | 33.9       |
|    value_loss           | 0.0992     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.66     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 296      |
|    time_elapsed    | 47392    |
|    total_timesteps | 9699328  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.5        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 297        |
|    time_elapsed         | 47549      |
|    total_timesteps      | 9732096    |
| train/                  |            |
|    approx_kl            | 0.04708893 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -267       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.67      |
|    n_updates            | 2960       |
|    policy_gradient_loss | -0.0208    |
|    std                  | 34.1       |
|    value_loss           | 0.0866     |
----------------------------------------
Eval num_timesteps=9760000, episode_reward=2.27 +/- 0.95
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.27       |
| time/                   |            |
|    total_timesteps      | 9760000    |
| train/                  |            |
|    approx_kl            | 0.04326209 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.2        |
|    entropy_loss         | -267       |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.68      |
|    n_updates            | 2970       |
|    policy_gradient_loss | -0.0172    |
|    std                  | 34.4       |
|    value_loss           | 0.0927     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.53     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 298      |
|    time_elapsed    | 47707    |
|    total_timesteps | 9764864  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.54        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 299         |
|    time_elapsed         | 47864       |
|    total_timesteps      | 9797632     |
| train/                  |             |
|    approx_kl            | 0.042751662 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.2         |
|    entropy_loss         | -268        |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.66       |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.0178     |
|    std                  | 34.6        |
|    value_loss           | 0.0909      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.19        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 300         |
|    time_elapsed         | 48021       |
|    total_timesteps      | 9830400     |
| train/                  |             |
|    approx_kl            | 0.043804843 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -268        |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.65       |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.0201     |
|    std                  | 34.9        |
|    value_loss           | 0.108       |
-----------------------------------------
Eval num_timesteps=9840000, episode_reward=3.22 +/- 1.24
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 3.22        |
| time/                   |             |
|    total_timesteps      | 9840000     |
| train/                  |             |
|    approx_kl            | 0.042467937 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -269        |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.62       |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.0191     |
|    std                  | 35.2        |
|    value_loss           | 0.108       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.12     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 301      |
|    time_elapsed    | 48180    |
|    total_timesteps | 9863168  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.2         |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 302         |
|    time_elapsed         | 48332       |
|    total_timesteps      | 9895936     |
| train/                  |             |
|    approx_kl            | 0.044958875 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    entropy_loss         | -269        |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.67       |
|    n_updates            | 3010        |
|    policy_gradient_loss | -0.0185     |
|    std                  | 35.5        |
|    value_loss           | 0.12        |
-----------------------------------------
Eval num_timesteps=9920000, episode_reward=2.02 +/- 0.90
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.02       |
| time/                   |            |
|    total_timesteps      | 9920000    |
| train/                  |            |
|    approx_kl            | 0.04614245 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.2        |
|    entropy_loss         | -269       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.71      |
|    n_updates            | 3020       |
|    policy_gradient_loss | -0.018     |
|    std                  | 35.7       |
|    value_loss           | 0.107      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.07     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 303      |
|    time_elapsed    | 48494    |
|    total_timesteps | 9928704  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.5         |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 304         |
|    time_elapsed         | 48652       |
|    total_timesteps      | 9961472     |
| train/                  |             |
|    approx_kl            | 0.044431422 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -270        |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.69       |
|    n_updates            | 3030        |
|    policy_gradient_loss | -0.0185     |
|    std                  | 36.1        |
|    value_loss           | 0.0991      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 5.4         |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 305         |
|    time_elapsed         | 48810       |
|    total_timesteps      | 9994240     |
| train/                  |             |
|    approx_kl            | 0.043882802 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -270        |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.71       |
|    n_updates            | 3040        |
|    policy_gradient_loss | -0.0186     |
|    std                  | 36.4        |
|    value_loss           | 0.105       |
-----------------------------------------
Eval num_timesteps=10000000, episode_reward=3.69 +/- 1.94
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 3.69        |
| time/                   |             |
|    total_timesteps      | 10000000    |
| train/                  |             |
|    approx_kl            | 0.042790174 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -271        |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.67       |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.0193     |
|    std                  | 36.6        |
|    value_loss           | 0.115       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.55     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 306      |
|    time_elapsed    | 48974    |
|    total_timesteps | 10027008 |
---------------------------------
Training complete. Model saved to logs/final_model/ppo_evcharging_final
Evaluating model...
Mean reward: 2.17 +/- 1.22
