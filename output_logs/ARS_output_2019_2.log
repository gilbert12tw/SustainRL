Configuration: 
algo: ARS
timesteps: 10000000
log_dir: logs
seed: 0
num_envs: 16
num_eval_envs: 2
checkpoint_freq: 50000
eval_freq: 5000
eval_episodes_during_training: 5
eval_episodes: 100
test_episodes: 10
PPO:
  learning_rate: 0.0005
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
RPPO:
  learning_rate: 0.0005
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  clip_range_vf: null
SAC:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  ent_coef: auto
  target_update_interval: 1
  target_entropy: auto
TD3:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 100
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  policy_delay: 2
  target_policy_noise: 0.2
  target_noise_clip: 0.5
DDPG:
  learning_rate: 0.001
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 100
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
CrossQ:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  ent_coef: 0.1
  sigma: 0.5
ARS:
  n_delta: 8
  delta_std: 0.05
  n_top: 4
  learning_rate: 0.01
  zero_policy: false
TQC:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  policy_kwargs: null
  top_quantiles_to_drop_per_net: 2
  ent_coef: auto
  target_entropy: auto

Observation space: Box([   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0. -288. -288. -288. -288. -288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.
 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.
 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.
 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 288. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.   1.   1.   1.   1.
   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
   1.   1.   1.   1.   1.   1.], (146,), float32)
Action space: Box(0.0, 1.0, (54,), float32)
Initializing ARS with parameters: {'n_delta': 8, 'delta_std': 0.05, 'n_top': 4, 'learning_rate': 0.01, 'zero_policy': False}
Using cpu device
Starting training with ARS...
Logging to logs/tensorboard/ARS_3
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.18     |
|    return_std      | 2.56     |
| time/              |          |
|    fps             | 310      |
|    time_elapsed    | 237      |
|    total_timesteps | 73728    |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 0        |
|    learning_rate   | 0.01     |
|    step_size       | 0.000976 |
---------------------------------
Eval num_timesteps=80000, episode_reward=-0.08 +/- 0.53
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.0789  |
| time/              |          |
|    total_timesteps | 80000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.1      |
|    return_std      | 2.29     |
| time/              |          |
|    fps             | 307      |
|    time_elapsed    | 479      |
|    total_timesteps | 147456   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1        |
|    learning_rate   | 0.01     |
|    step_size       | 0.00109  |
---------------------------------
Eval num_timesteps=160000, episode_reward=1.25 +/- 1.17
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 1.25     |
| time/              |          |
|    total_timesteps | 160000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.02     |
|    return_std      | 2.7      |
| time/              |          |
|    fps             | 306      |
|    time_elapsed    | 722      |
|    total_timesteps | 221184   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 2        |
|    learning_rate   | 0.01     |
|    step_size       | 0.000925 |
---------------------------------
Eval num_timesteps=240000, episode_reward=-0.00 +/- 0.94
Episode length: 288.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 288       |
|    mean_reward     | -0.000955 |
| time/              |           |
|    total_timesteps | 240000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.53     |
|    return_std      | 3.31     |
| time/              |          |
|    fps             | 305      |
|    time_elapsed    | 964      |
|    total_timesteps | 294912   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 3        |
|    learning_rate   | 0.01     |
|    step_size       | 0.000756 |
---------------------------------
Eval num_timesteps=320000, episode_reward=0.43 +/- 1.84
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.429    |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.6      |
|    return_std      | 2.75     |
| time/              |          |
|    fps             | 306      |
|    time_elapsed    | 1203     |
|    total_timesteps | 368640   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 4        |
|    learning_rate   | 0.01     |
|    step_size       | 0.00091  |
---------------------------------
Eval num_timesteps=400000, episode_reward=0.37 +/- 0.47
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.369    |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.28     |
|    return_std      | 3.51     |
| time/              |          |
|    fps             | 305      |
|    time_elapsed    | 1446     |
|    total_timesteps | 442368   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 5        |
|    learning_rate   | 0.01     |
|    step_size       | 0.000711 |
---------------------------------
Eval num_timesteps=480000, episode_reward=-0.07 +/- 0.68
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.0714  |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.72     |
|    return_std      | 3.27     |
| time/              |          |
|    fps             | 309      |
|    time_elapsed    | 1669     |
|    total_timesteps | 516096   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 6        |
|    learning_rate   | 0.01     |
|    step_size       | 0.000765 |
---------------------------------
Eval num_timesteps=560000, episode_reward=-0.04 +/- 0.68
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.0372  |
| time/              |          |
|    total_timesteps | 560000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.61     |
|    return_std      | 2.59     |
| time/              |          |
|    fps             | 312      |
|    time_elapsed    | 1886     |
|    total_timesteps | 589824   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 7        |
|    learning_rate   | 0.01     |
|    step_size       | 0.000966 |
---------------------------------
Eval num_timesteps=640000, episode_reward=-0.84 +/- 0.40
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.838   |
| time/              |          |
|    total_timesteps | 640000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.73     |
|    return_std      | 2.99     |
| time/              |          |
|    fps             | 315      |
|    time_elapsed    | 2105     |
|    total_timesteps | 663552   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 8        |
|    learning_rate   | 0.01     |
|    step_size       | 0.000836 |
---------------------------------
Eval num_timesteps=720000, episode_reward=-0.40 +/- 0.77
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.402   |
| time/              |          |
|    total_timesteps | 720000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.67     |
|    return_std      | 2.55     |
| time/              |          |
|    fps             | 317      |
|    time_elapsed    | 2323     |
|    total_timesteps | 737280   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 9        |
|    learning_rate   | 0.01     |
|    step_size       | 0.000981 |
---------------------------------
Eval num_timesteps=800000, episode_reward=-0.11 +/- 0.85
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.109   |
| time/              |          |
|    total_timesteps | 800000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.14     |
|    return_std      | 3.37     |
| time/              |          |
|    fps             | 317      |
|    time_elapsed    | 2553     |
|    total_timesteps | 811008   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 10       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000741 |
---------------------------------
Eval num_timesteps=880000, episode_reward=-0.44 +/- 1.08
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.442   |
| time/              |          |
|    total_timesteps | 880000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.65     |
|    return_std      | 2.87     |
| time/              |          |
|    fps             | 319      |
|    time_elapsed    | 2771     |
|    total_timesteps | 884736   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 11       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000871 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.64     |
|    return_std      | 3.08     |
| time/              |          |
|    fps             | 320      |
|    time_elapsed    | 2987     |
|    total_timesteps | 958464   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 12       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000813 |
---------------------------------
Eval num_timesteps=960000, episode_reward=0.04 +/- 1.56
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.0375   |
| time/              |          |
|    total_timesteps | 960000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.54     |
|    return_std      | 3.37     |
| time/              |          |
|    fps             | 321      |
|    time_elapsed    | 3205     |
|    total_timesteps | 1032192  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 13       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000742 |
---------------------------------
Eval num_timesteps=1040000, episode_reward=0.09 +/- 0.46
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.0919   |
| time/              |          |
|    total_timesteps | 1040000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.96     |
|    return_std      | 2.54     |
| time/              |          |
|    fps             | 322      |
|    time_elapsed    | 3429     |
|    total_timesteps | 1105920  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 14       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000983 |
---------------------------------
Eval num_timesteps=1120000, episode_reward=-0.13 +/- 1.56
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.132   |
| time/              |          |
|    total_timesteps | 1120000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.12     |
|    return_std      | 1.43     |
| time/              |          |
|    fps             | 322      |
|    time_elapsed    | 3654     |
|    total_timesteps | 1179648  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 15       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00175  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=-0.20 +/- 0.82
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.199   |
| time/              |          |
|    total_timesteps | 1200000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.86     |
|    return_std      | 1.53     |
| time/              |          |
|    fps             | 323      |
|    time_elapsed    | 3869     |
|    total_timesteps | 1253376  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 16       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00163  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=-0.43 +/- 0.79
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.432   |
| time/              |          |
|    total_timesteps | 1280000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.23     |
|    return_std      | 4.08     |
| time/              |          |
|    fps             | 323      |
|    time_elapsed    | 4096     |
|    total_timesteps | 1327104  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 17       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000612 |
---------------------------------
Eval num_timesteps=1360000, episode_reward=0.71 +/- 1.15
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.706    |
| time/              |          |
|    total_timesteps | 1360000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.23     |
|    return_std      | 3.77     |
| time/              |          |
|    fps             | 324      |
|    time_elapsed    | 4319     |
|    total_timesteps | 1400832  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 18       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000663 |
---------------------------------
Eval num_timesteps=1440000, episode_reward=0.14 +/- 0.53
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.144    |
| time/              |          |
|    total_timesteps | 1440000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.74     |
|    return_std      | 4.43     |
| time/              |          |
|    fps             | 324      |
|    time_elapsed    | 4548     |
|    total_timesteps | 1474560  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 19       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000564 |
---------------------------------
Eval num_timesteps=1520000, episode_reward=0.04 +/- 1.84
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.04     |
| time/              |          |
|    total_timesteps | 1520000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.5      |
|    return_std      | 3.12     |
| time/              |          |
|    fps             | 325      |
|    time_elapsed    | 4758     |
|    total_timesteps | 1548288  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 20       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000801 |
---------------------------------
Eval num_timesteps=1600000, episode_reward=0.45 +/- 0.78
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.452    |
| time/              |          |
|    total_timesteps | 1600000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.37     |
|    return_std      | 3.33     |
| time/              |          |
|    fps             | 326      |
|    time_elapsed    | 4974     |
|    total_timesteps | 1622016  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 21       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000751 |
---------------------------------
Eval num_timesteps=1680000, episode_reward=-0.45 +/- 0.81
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.448   |
| time/              |          |
|    total_timesteps | 1680000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.55     |
|    return_std      | 4.06     |
| time/              |          |
|    fps             | 326      |
|    time_elapsed    | 5189     |
|    total_timesteps | 1695744  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 22       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000616 |
---------------------------------
Eval num_timesteps=1760000, episode_reward=0.17 +/- 0.83
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.167    |
| time/              |          |
|    total_timesteps | 1760000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.1      |
|    return_std      | 4.4      |
| time/              |          |
|    fps             | 327      |
|    time_elapsed    | 5405     |
|    total_timesteps | 1769472  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 23       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000568 |
---------------------------------
Eval num_timesteps=1840000, episode_reward=-0.21 +/- 0.72
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.211   |
| time/              |          |
|    total_timesteps | 1840000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.1      |
|    return_std      | 3.74     |
| time/              |          |
|    fps             | 328      |
|    time_elapsed    | 5618     |
|    total_timesteps | 1843200  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 24       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000669 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.15     |
|    return_std      | 2.86     |
| time/              |          |
|    fps             | 328      |
|    time_elapsed    | 5828     |
|    total_timesteps | 1916928  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 25       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000873 |
---------------------------------
Eval num_timesteps=1920000, episode_reward=0.83 +/- 1.59
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.829    |
| time/              |          |
|    total_timesteps | 1920000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.17     |
|    return_std      | 3.42     |
| time/              |          |
|    fps             | 329      |
|    time_elapsed    | 6048     |
|    total_timesteps | 1990656  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 26       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000731 |
---------------------------------
Eval num_timesteps=2000000, episode_reward=0.29 +/- 0.40
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.291    |
| time/              |          |
|    total_timesteps | 2000000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.46     |
|    return_std      | 2.9      |
| time/              |          |
|    fps             | 329      |
|    time_elapsed    | 6264     |
|    total_timesteps | 2064384  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 27       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000861 |
---------------------------------
Eval num_timesteps=2080000, episode_reward=0.08 +/- 1.13
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.0752   |
| time/              |          |
|    total_timesteps | 2080000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.19     |
|    return_std      | 4.23     |
| time/              |          |
|    fps             | 329      |
|    time_elapsed    | 6490     |
|    total_timesteps | 2138112  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 28       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000591 |
---------------------------------
Eval num_timesteps=2160000, episode_reward=0.20 +/- 1.15
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.199    |
| time/              |          |
|    total_timesteps | 2160000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.01     |
|    return_std      | 3.3      |
| time/              |          |
|    fps             | 329      |
|    time_elapsed    | 6710     |
|    total_timesteps | 2211840  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 29       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000757 |
---------------------------------
Eval num_timesteps=2240000, episode_reward=-0.01 +/- 0.67
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.00771 |
| time/              |          |
|    total_timesteps | 2240000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.03     |
|    return_std      | 3.44     |
| time/              |          |
|    fps             | 329      |
|    time_elapsed    | 6935     |
|    total_timesteps | 2285568  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 30       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000726 |
---------------------------------
Eval num_timesteps=2320000, episode_reward=-0.40 +/- 1.30
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.401   |
| time/              |          |
|    total_timesteps | 2320000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.08     |
|    return_std      | 1.71     |
| time/              |          |
|    fps             | 329      |
|    time_elapsed    | 7165     |
|    total_timesteps | 2359296  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 31       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00146  |
---------------------------------
Eval num_timesteps=2400000, episode_reward=0.05 +/- 0.85
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.0505   |
| time/              |          |
|    total_timesteps | 2400000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.03     |
|    return_std      | 2.82     |
| time/              |          |
|    fps             | 329      |
|    time_elapsed    | 7381     |
|    total_timesteps | 2433024  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 32       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000886 |
---------------------------------
Eval num_timesteps=2480000, episode_reward=1.56 +/- 1.60
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 2480000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.6      |
|    return_std      | 2.12     |
| time/              |          |
|    fps             | 329      |
|    time_elapsed    | 7596     |
|    total_timesteps | 2506752  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 33       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00118  |
---------------------------------
Eval num_timesteps=2560000, episode_reward=1.16 +/- 1.59
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 1.16     |
| time/              |          |
|    total_timesteps | 2560000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.28     |
|    return_std      | 2.57     |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 7811     |
|    total_timesteps | 2580480  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 34       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000973 |
---------------------------------
Eval num_timesteps=2640000, episode_reward=0.68 +/- 1.12
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.676    |
| time/              |          |
|    total_timesteps | 2640000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.8      |
|    return_std      | 2.78     |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 8025     |
|    total_timesteps | 2654208  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 35       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000901 |
---------------------------------
Eval num_timesteps=2720000, episode_reward=0.90 +/- 1.69
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.899    |
| time/              |          |
|    total_timesteps | 2720000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.67     |
|    return_std      | 2.39     |
| time/              |          |
|    fps             | 331      |
|    time_elapsed    | 8238     |
|    total_timesteps | 2727936  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 36       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00105  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=-0.22 +/- 1.07
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.217   |
| time/              |          |
|    total_timesteps | 2800000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.98     |
|    return_std      | 3.69     |
| time/              |          |
|    fps             | 331      |
|    time_elapsed    | 8451     |
|    total_timesteps | 2801664  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 37       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000677 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.54     |
|    return_std      | 4.48     |
| time/              |          |
|    fps             | 331      |
|    time_elapsed    | 8660     |
|    total_timesteps | 2875392  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 38       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000559 |
---------------------------------
Eval num_timesteps=2880000, episode_reward=0.63 +/- 0.95
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.633    |
| time/              |          |
|    total_timesteps | 2880000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.07     |
|    return_std      | 3.46     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 8876     |
|    total_timesteps | 2949120  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 39       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000722 |
---------------------------------
Eval num_timesteps=2960000, episode_reward=0.00 +/- 1.25
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.0047   |
| time/              |          |
|    total_timesteps | 2960000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.56     |
|    return_std      | 2.93     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 9091     |
|    total_timesteps | 3022848  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 40       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000855 |
---------------------------------
Eval num_timesteps=3040000, episode_reward=-0.24 +/- 0.69
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.242   |
| time/              |          |
|    total_timesteps | 3040000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.12     |
|    return_std      | 3.22     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 9312     |
|    total_timesteps | 3096576  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 41       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000776 |
---------------------------------
Eval num_timesteps=3120000, episode_reward=-0.38 +/- 0.73
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.375   |
| time/              |          |
|    total_timesteps | 3120000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.49     |
|    return_std      | 3.13     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 9533     |
|    total_timesteps | 3170304  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 42       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000797 |
---------------------------------
Eval num_timesteps=3200000, episode_reward=0.19 +/- 0.76
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.194    |
| time/              |          |
|    total_timesteps | 3200000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.73     |
|    return_std      | 3.54     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 9749     |
|    total_timesteps | 3244032  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 43       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000707 |
---------------------------------
Eval num_timesteps=3280000, episode_reward=-0.02 +/- 0.72
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.0246  |
| time/              |          |
|    total_timesteps | 3280000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.53     |
|    return_std      | 2.87     |
| time/              |          |
|    fps             | 333      |
|    time_elapsed    | 9962     |
|    total_timesteps | 3317760  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 44       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000872 |
---------------------------------
Eval num_timesteps=3360000, episode_reward=0.06 +/- 0.97
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.0627   |
| time/              |          |
|    total_timesteps | 3360000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.03     |
|    return_std      | 1.52     |
| time/              |          |
|    fps             | 333      |
|    time_elapsed    | 10175    |
|    total_timesteps | 3391488  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 45       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00164  |
---------------------------------
Eval num_timesteps=3440000, episode_reward=0.04 +/- 1.18
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.0447   |
| time/              |          |
|    total_timesteps | 3440000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.57     |
|    return_std      | 3.46     |
| time/              |          |
|    fps             | 333      |
|    time_elapsed    | 10389    |
|    total_timesteps | 3465216  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 46       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000722 |
---------------------------------
Eval num_timesteps=3520000, episode_reward=0.22 +/- 0.42
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.216    |
| time/              |          |
|    total_timesteps | 3520000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.09     |
|    return_std      | 1.97     |
| time/              |          |
|    fps             | 333      |
|    time_elapsed    | 10603    |
|    total_timesteps | 3538944  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 47       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00127  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=-0.29 +/- 1.56
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.292   |
| time/              |          |
|    total_timesteps | 3600000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.46     |
|    return_std      | 4.22     |
| time/              |          |
|    fps             | 334      |
|    time_elapsed    | 10816    |
|    total_timesteps | 3612672  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 48       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000592 |
---------------------------------
Eval num_timesteps=3680000, episode_reward=0.21 +/- 1.48
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.207    |
| time/              |          |
|    total_timesteps | 3680000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.85     |
|    return_std      | 3.23     |
| time/              |          |
|    fps             | 334      |
|    time_elapsed    | 11029    |
|    total_timesteps | 3686400  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 49       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000774 |
---------------------------------
Eval num_timesteps=3760000, episode_reward=-0.02 +/- 1.44
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.0207  |
| time/              |          |
|    total_timesteps | 3760000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.85     |
|    return_std      | 4.61     |
| time/              |          |
|    fps             | 334      |
|    time_elapsed    | 11242    |
|    total_timesteps | 3760128  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 50       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000542 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.7      |
|    return_std      | 3.7      |
| time/              |          |
|    fps             | 334      |
|    time_elapsed    | 11450    |
|    total_timesteps | 3833856  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 51       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000676 |
---------------------------------
Eval num_timesteps=3840000, episode_reward=0.76 +/- 0.80
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.757    |
| time/              |          |
|    total_timesteps | 3840000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.85     |
|    return_std      | 3.26     |
| time/              |          |
|    fps             | 334      |
|    time_elapsed    | 11665    |
|    total_timesteps | 3907584  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 52       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000766 |
---------------------------------
Eval num_timesteps=3920000, episode_reward=0.63 +/- 0.94
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.626    |
| time/              |          |
|    total_timesteps | 3920000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.9      |
|    return_std      | 1.25     |
| time/              |          |
|    fps             | 335      |
|    time_elapsed    | 11878    |
|    total_timesteps | 3981312  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 53       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00201  |
---------------------------------
Eval num_timesteps=4000000, episode_reward=0.48 +/- 1.16
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.483    |
| time/              |          |
|    total_timesteps | 4000000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.59     |
|    return_std      | 3.74     |
| time/              |          |
|    fps             | 335      |
|    time_elapsed    | 12091    |
|    total_timesteps | 4055040  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 54       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000669 |
---------------------------------
Eval num_timesteps=4080000, episode_reward=-0.49 +/- 1.21
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.486   |
| time/              |          |
|    total_timesteps | 4080000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.79     |
|    return_std      | 3.89     |
| time/              |          |
|    fps             | 335      |
|    time_elapsed    | 12305    |
|    total_timesteps | 4128768  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 55       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000643 |
---------------------------------
Eval num_timesteps=4160000, episode_reward=0.11 +/- 1.14
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.107    |
| time/              |          |
|    total_timesteps | 4160000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.15     |
|    return_std      | 3.5      |
| time/              |          |
|    fps             | 335      |
|    time_elapsed    | 12519    |
|    total_timesteps | 4202496  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 56       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000713 |
---------------------------------
Eval num_timesteps=4240000, episode_reward=-0.18 +/- 0.64
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.184   |
| time/              |          |
|    total_timesteps | 4240000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.77     |
|    return_std      | 1.19     |
| time/              |          |
|    fps             | 335      |
|    time_elapsed    | 12732    |
|    total_timesteps | 4276224  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 57       |
|    learning_rate   | 0.01     |
|    step_size       | 0.0021   |
---------------------------------
Eval num_timesteps=4320000, episode_reward=-0.47 +/- 1.08
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.468   |
| time/              |          |
|    total_timesteps | 4320000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.08     |
|    return_std      | 3.09     |
| time/              |          |
|    fps             | 335      |
|    time_elapsed    | 12946    |
|    total_timesteps | 4349952  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 58       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00081  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=0.44 +/- 1.02
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.436    |
| time/              |          |
|    total_timesteps | 4400000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.58     |
|    return_std      | 2.36     |
| time/              |          |
|    fps             | 336      |
|    time_elapsed    | 13161    |
|    total_timesteps | 4423680  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 59       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00106  |
---------------------------------
Eval num_timesteps=4480000, episode_reward=0.19 +/- 0.91
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.194    |
| time/              |          |
|    total_timesteps | 4480000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.16     |
|    return_std      | 3.35     |
| time/              |          |
|    fps             | 336      |
|    time_elapsed    | 13375    |
|    total_timesteps | 4497408  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 60       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000747 |
---------------------------------
Eval num_timesteps=4560000, episode_reward=-0.50 +/- 1.12
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.499   |
| time/              |          |
|    total_timesteps | 4560000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.64     |
|    return_std      | 4.14     |
| time/              |          |
|    fps             | 336      |
|    time_elapsed    | 13588    |
|    total_timesteps | 4571136  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 61       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000603 |
---------------------------------
Eval num_timesteps=4640000, episode_reward=-0.28 +/- 0.55
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.279   |
| time/              |          |
|    total_timesteps | 4640000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.51     |
|    return_std      | 3.95     |
| time/              |          |
|    fps             | 336      |
|    time_elapsed    | 13811    |
|    total_timesteps | 4644864  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 62       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000633 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.09     |
|    return_std      | 3.52     |
| time/              |          |
|    fps             | 336      |
|    time_elapsed    | 14043    |
|    total_timesteps | 4718592  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 63       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000711 |
---------------------------------
Eval num_timesteps=4720000, episode_reward=0.62 +/- 1.29
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.62     |
| time/              |          |
|    total_timesteps | 4720000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.1      |
|    return_std      | 3.86     |
| time/              |          |
|    fps             | 335      |
|    time_elapsed    | 14277    |
|    total_timesteps | 4792320  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 64       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000648 |
---------------------------------
Eval num_timesteps=4800000, episode_reward=-0.49 +/- 1.05
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.491   |
| time/              |          |
|    total_timesteps | 4800000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.09     |
|    return_std      | 4.42     |
| time/              |          |
|    fps             | 335      |
|    time_elapsed    | 14515    |
|    total_timesteps | 4866048  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 65       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000566 |
---------------------------------
Eval num_timesteps=4880000, episode_reward=-0.52 +/- 0.58
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.522   |
| time/              |          |
|    total_timesteps | 4880000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.51     |
|    return_std      | 2.01     |
| time/              |          |
|    fps             | 334      |
|    time_elapsed    | 14753    |
|    total_timesteps | 4939776  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 66       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00124  |
---------------------------------
Eval num_timesteps=4960000, episode_reward=-0.48 +/- 0.74
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.483   |
| time/              |          |
|    total_timesteps | 4960000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.71     |
|    return_std      | 3.56     |
| time/              |          |
|    fps             | 334      |
|    time_elapsed    | 14981    |
|    total_timesteps | 5013504  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 67       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000702 |
---------------------------------
Eval num_timesteps=5040000, episode_reward=-0.25 +/- 0.39
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.25    |
| time/              |          |
|    total_timesteps | 5040000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.72     |
|    return_std      | 3.29     |
| time/              |          |
|    fps             | 334      |
|    time_elapsed    | 15211    |
|    total_timesteps | 5087232  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 68       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000761 |
---------------------------------
Eval num_timesteps=5120000, episode_reward=-0.43 +/- 0.49
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.434   |
| time/              |          |
|    total_timesteps | 5120000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.75     |
|    return_std      | 2.98     |
| time/              |          |
|    fps             | 334      |
|    time_elapsed    | 15439    |
|    total_timesteps | 5160960  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 69       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000839 |
---------------------------------
Eval num_timesteps=5200000, episode_reward=0.16 +/- 0.92
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.161    |
| time/              |          |
|    total_timesteps | 5200000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.02     |
|    return_std      | 4.08     |
| time/              |          |
|    fps             | 334      |
|    time_elapsed    | 15667    |
|    total_timesteps | 5234688  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 70       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000613 |
---------------------------------
Eval num_timesteps=5280000, episode_reward=0.23 +/- 0.57
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.226    |
| time/              |          |
|    total_timesteps | 5280000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.91     |
|    return_std      | 2.04     |
| time/              |          |
|    fps             | 333      |
|    time_elapsed    | 15895    |
|    total_timesteps | 5308416  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 71       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00122  |
---------------------------------
Eval num_timesteps=5360000, episode_reward=0.10 +/- 1.43
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 5360000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.68     |
|    return_std      | 3.31     |
| time/              |          |
|    fps             | 333      |
|    time_elapsed    | 16123    |
|    total_timesteps | 5382144  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 72       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000755 |
---------------------------------
Eval num_timesteps=5440000, episode_reward=-0.36 +/- 1.18
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.364   |
| time/              |          |
|    total_timesteps | 5440000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.54     |
|    return_std      | 4.92     |
| time/              |          |
|    fps             | 333      |
|    time_elapsed    | 16349    |
|    total_timesteps | 5455872  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 73       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000508 |
---------------------------------
Eval num_timesteps=5520000, episode_reward=-0.26 +/- 0.84
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.264   |
| time/              |          |
|    total_timesteps | 5520000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.94     |
|    return_std      | 4.79     |
| time/              |          |
|    fps             | 333      |
|    time_elapsed    | 16577    |
|    total_timesteps | 5529600  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 74       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000522 |
---------------------------------
Eval num_timesteps=5600000, episode_reward=-0.52 +/- 0.60
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.525   |
| time/              |          |
|    total_timesteps | 5600000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.07     |
|    return_std      | 1.28     |
| time/              |          |
|    fps             | 333      |
|    time_elapsed    | 16804    |
|    total_timesteps | 5603328  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 75       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00196  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.85     |
|    return_std      | 2.94     |
| time/              |          |
|    fps             | 333      |
|    time_elapsed    | 17026    |
|    total_timesteps | 5677056  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 76       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00085  |
---------------------------------
Eval num_timesteps=5680000, episode_reward=-0.19 +/- 0.41
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.19    |
| time/              |          |
|    total_timesteps | 5680000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.01     |
|    return_std      | 2.84     |
| time/              |          |
|    fps             | 333      |
|    time_elapsed    | 17250    |
|    total_timesteps | 5750784  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 77       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00088  |
---------------------------------
Eval num_timesteps=5760000, episode_reward=-0.76 +/- 0.68
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.758   |
| time/              |          |
|    total_timesteps | 5760000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.97     |
|    return_std      | 3.38     |
| time/              |          |
|    fps             | 333      |
|    time_elapsed    | 17489    |
|    total_timesteps | 5824512  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 78       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000739 |
---------------------------------
Eval num_timesteps=5840000, episode_reward=-0.21 +/- 1.31
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.206   |
| time/              |          |
|    total_timesteps | 5840000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.13     |
|    return_std      | 2.59     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 17721    |
|    total_timesteps | 5898240  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 79       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000965 |
---------------------------------
Eval num_timesteps=5920000, episode_reward=0.07 +/- 1.24
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.0727   |
| time/              |          |
|    total_timesteps | 5920000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.94     |
|    return_std      | 3.86     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 17952    |
|    total_timesteps | 5971968  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 80       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000648 |
---------------------------------
Eval num_timesteps=6000000, episode_reward=0.13 +/- 1.25
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.13     |
| time/              |          |
|    total_timesteps | 6000000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.64     |
|    return_std      | 3.2      |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 18174    |
|    total_timesteps | 6045696  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 81       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000782 |
---------------------------------
Eval num_timesteps=6080000, episode_reward=-1.16 +/- 0.68
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -1.16    |
| time/              |          |
|    total_timesteps | 6080000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.08     |
|    return_std      | 3.49     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 18404    |
|    total_timesteps | 6119424  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 82       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000716 |
---------------------------------
Eval num_timesteps=6160000, episode_reward=0.04 +/- 0.78
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.0377   |
| time/              |          |
|    total_timesteps | 6160000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.18     |
|    return_std      | 1.47     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 18632    |
|    total_timesteps | 6193152  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 83       |
|    learning_rate   | 0.01     |
|    step_size       | 0.0017   |
---------------------------------
Eval num_timesteps=6240000, episode_reward=0.29 +/- 0.92
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.289    |
| time/              |          |
|    total_timesteps | 6240000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.29     |
|    return_std      | 2.15     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 18858    |
|    total_timesteps | 6266880  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 84       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00116  |
---------------------------------
Eval num_timesteps=6320000, episode_reward=-0.36 +/- 0.63
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.357   |
| time/              |          |
|    total_timesteps | 6320000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.42     |
|    return_std      | 2.02     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 19085    |
|    total_timesteps | 6340608  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 85       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00124  |
---------------------------------
Eval num_timesteps=6400000, episode_reward=-0.50 +/- 0.83
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.497   |
| time/              |          |
|    total_timesteps | 6400000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.45     |
|    return_std      | 2.39     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 19310    |
|    total_timesteps | 6414336  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 86       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00105  |
---------------------------------
Eval num_timesteps=6480000, episode_reward=0.60 +/- 0.82
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.603    |
| time/              |          |
|    total_timesteps | 6480000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.51     |
|    return_std      | 3.26     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 19540    |
|    total_timesteps | 6488064  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 87       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000766 |
---------------------------------
Eval num_timesteps=6560000, episode_reward=0.75 +/- 1.17
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.746    |
| time/              |          |
|    total_timesteps | 6560000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.27     |
|    return_std      | 2.28     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 19763    |
|    total_timesteps | 6561792  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 88       |
|    learning_rate   | 0.01     |
|    step_size       | 0.0011   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.71     |
|    return_std      | 4.45     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 19976    |
|    total_timesteps | 6635520  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 89       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000562 |
---------------------------------
Eval num_timesteps=6640000, episode_reward=0.25 +/- 0.52
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.248    |
| time/              |          |
|    total_timesteps | 6640000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.33     |
|    return_std      | 1.7      |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 20194    |
|    total_timesteps | 6709248  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 90       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00147  |
---------------------------------
Eval num_timesteps=6720000, episode_reward=0.26 +/- 1.13
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.261    |
| time/              |          |
|    total_timesteps | 6720000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.69     |
|    return_std      | 3.77     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 20409    |
|    total_timesteps | 6782976  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 91       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000663 |
---------------------------------
Eval num_timesteps=6800000, episode_reward=0.91 +/- 0.93
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.908    |
| time/              |          |
|    total_timesteps | 6800000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.01     |
|    return_std      | 2.9      |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 20622    |
|    total_timesteps | 6856704  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 92       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000862 |
---------------------------------
Eval num_timesteps=6880000, episode_reward=0.99 +/- 1.48
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.986    |
| time/              |          |
|    total_timesteps | 6880000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.58     |
|    return_std      | 2.48     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 20836    |
|    total_timesteps | 6930432  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 93       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00101  |
---------------------------------
Eval num_timesteps=6960000, episode_reward=-0.09 +/- 1.22
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.0902  |
| time/              |          |
|    total_timesteps | 6960000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.09     |
|    return_std      | 4        |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 21050    |
|    total_timesteps | 7004160  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 94       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000626 |
---------------------------------
Eval num_timesteps=7040000, episode_reward=-0.52 +/- 0.45
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.516   |
| time/              |          |
|    total_timesteps | 7040000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.31     |
|    return_std      | 3.12     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 21265    |
|    total_timesteps | 7077888  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 95       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000802 |
---------------------------------
Eval num_timesteps=7120000, episode_reward=0.00 +/- 0.57
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.000227 |
| time/              |          |
|    total_timesteps | 7120000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.85     |
|    return_std      | 4.09     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 21485    |
|    total_timesteps | 7151616  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 96       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000612 |
---------------------------------
Eval num_timesteps=7200000, episode_reward=0.07 +/- 0.66
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.0666   |
| time/              |          |
|    total_timesteps | 7200000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.35     |
|    return_std      | 3.3      |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 21712    |
|    total_timesteps | 7225344  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 97       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000758 |
---------------------------------
Eval num_timesteps=7280000, episode_reward=-0.24 +/- 1.48
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.24    |
| time/              |          |
|    total_timesteps | 7280000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.46     |
|    return_std      | 3.53     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 21942    |
|    total_timesteps | 7299072  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 98       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000708 |
---------------------------------
Eval num_timesteps=7360000, episode_reward=0.59 +/- 0.56
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.592    |
| time/              |          |
|    total_timesteps | 7360000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.34     |
|    return_std      | 2.93     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 22172    |
|    total_timesteps | 7372800  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 99       |
|    learning_rate   | 0.01     |
|    step_size       | 0.000853 |
---------------------------------
Eval num_timesteps=7440000, episode_reward=0.53 +/- 1.73
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.527    |
| time/              |          |
|    total_timesteps | 7440000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.82     |
|    return_std      | 0.959    |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 22406    |
|    total_timesteps | 7446528  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 100      |
|    learning_rate   | 0.01     |
|    step_size       | 0.00261  |
---------------------------------
Eval num_timesteps=7520000, episode_reward=0.28 +/- 0.85
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.277    |
| time/              |          |
|    total_timesteps | 7520000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.55     |
|    return_std      | 2.34     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 22640    |
|    total_timesteps | 7520256  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 101      |
|    learning_rate   | 0.01     |
|    step_size       | 0.00107  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.64     |
|    return_std      | 2.95     |
| time/              |          |
|    fps             | 332      |
|    time_elapsed    | 22871    |
|    total_timesteps | 7593984  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 102      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000847 |
---------------------------------
Eval num_timesteps=7600000, episode_reward=1.30 +/- 1.24
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 7600000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.11     |
|    return_std      | 4.06     |
| time/              |          |
|    fps             | 331      |
|    time_elapsed    | 23102    |
|    total_timesteps | 7667712  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 103      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000616 |
---------------------------------
Eval num_timesteps=7680000, episode_reward=1.14 +/- 1.42
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 1.14     |
| time/              |          |
|    total_timesteps | 7680000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.95     |
|    return_std      | 2.69     |
| time/              |          |
|    fps             | 331      |
|    time_elapsed    | 23332    |
|    total_timesteps | 7741440  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 104      |
|    learning_rate   | 0.01     |
|    step_size       | 0.00093  |
---------------------------------
Eval num_timesteps=7760000, episode_reward=-0.49 +/- 0.67
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.494   |
| time/              |          |
|    total_timesteps | 7760000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.74     |
|    return_std      | 3.27     |
| time/              |          |
|    fps             | 331      |
|    time_elapsed    | 23558    |
|    total_timesteps | 7815168  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 105      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000764 |
---------------------------------
Eval num_timesteps=7840000, episode_reward=-0.39 +/- 0.65
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.39    |
| time/              |          |
|    total_timesteps | 7840000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.05     |
|    return_std      | 3.9      |
| time/              |          |
|    fps             | 331      |
|    time_elapsed    | 23786    |
|    total_timesteps | 7888896  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 106      |
|    learning_rate   | 0.01     |
|    step_size       | 0.00064  |
---------------------------------
Eval num_timesteps=7920000, episode_reward=0.24 +/- 1.10
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.244    |
| time/              |          |
|    total_timesteps | 7920000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.49     |
|    return_std      | 1.69     |
| time/              |          |
|    fps             | 331      |
|    time_elapsed    | 24016    |
|    total_timesteps | 7962624  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 107      |
|    learning_rate   | 0.01     |
|    step_size       | 0.00148  |
---------------------------------
Eval num_timesteps=8000000, episode_reward=-0.16 +/- 0.79
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.161   |
| time/              |          |
|    total_timesteps | 8000000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.42     |
|    return_std      | 2.92     |
| time/              |          |
|    fps             | 331      |
|    time_elapsed    | 24242    |
|    total_timesteps | 8036352  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 108      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000855 |
---------------------------------
Eval num_timesteps=8080000, episode_reward=-0.29 +/- 0.31
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.289   |
| time/              |          |
|    total_timesteps | 8080000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.23     |
|    return_std      | 2.07     |
| time/              |          |
|    fps             | 331      |
|    time_elapsed    | 24469    |
|    total_timesteps | 8110080  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 109      |
|    learning_rate   | 0.01     |
|    step_size       | 0.00121  |
---------------------------------
Eval num_timesteps=8160000, episode_reward=-0.52 +/- 1.19
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.523   |
| time/              |          |
|    total_timesteps | 8160000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.85     |
|    return_std      | 3.95     |
| time/              |          |
|    fps             | 331      |
|    time_elapsed    | 24696    |
|    total_timesteps | 8183808  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 110      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000634 |
---------------------------------
Eval num_timesteps=8240000, episode_reward=0.32 +/- 0.79
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.323    |
| time/              |          |
|    total_timesteps | 8240000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.84     |
|    return_std      | 2.89     |
| time/              |          |
|    fps             | 331      |
|    time_elapsed    | 24924    |
|    total_timesteps | 8257536  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 111      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000865 |
---------------------------------
Eval num_timesteps=8320000, episode_reward=-0.48 +/- 0.29
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.475   |
| time/              |          |
|    total_timesteps | 8320000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.19     |
|    return_std      | 3.91     |
| time/              |          |
|    fps             | 331      |
|    time_elapsed    | 25152    |
|    total_timesteps | 8331264  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 112      |
|    learning_rate   | 0.01     |
|    step_size       | 0.00064  |
---------------------------------
Eval num_timesteps=8400000, episode_reward=-0.28 +/- 1.22
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.28    |
| time/              |          |
|    total_timesteps | 8400000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.37     |
|    return_std      | 3.16     |
| time/              |          |
|    fps             | 331      |
|    time_elapsed    | 25381    |
|    total_timesteps | 8404992  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 113      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000791 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.81     |
|    return_std      | 3.81     |
| time/              |          |
|    fps             | 331      |
|    time_elapsed    | 25609    |
|    total_timesteps | 8478720  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 114      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000657 |
---------------------------------
Eval num_timesteps=8480000, episode_reward=-1.06 +/- 0.36
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -1.06    |
| time/              |          |
|    total_timesteps | 8480000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.68     |
|    return_std      | 1.09     |
| time/              |          |
|    fps             | 331      |
|    time_elapsed    | 25836    |
|    total_timesteps | 8552448  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 115      |
|    learning_rate   | 0.01     |
|    step_size       | 0.00229  |
---------------------------------
Eval num_timesteps=8560000, episode_reward=0.93 +/- 1.36
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.929    |
| time/              |          |
|    total_timesteps | 8560000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.47     |
|    return_std      | 3.58     |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 26063    |
|    total_timesteps | 8626176  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 116      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000697 |
---------------------------------
Eval num_timesteps=8640000, episode_reward=0.01 +/- 1.08
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.00836  |
| time/              |          |
|    total_timesteps | 8640000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.62     |
|    return_std      | 2.5      |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 26291    |
|    total_timesteps | 8699904  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 117      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000998 |
---------------------------------
Eval num_timesteps=8720000, episode_reward=0.41 +/- 2.05
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.409    |
| time/              |          |
|    total_timesteps | 8720000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.26     |
|    return_std      | 1.99     |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 26519    |
|    total_timesteps | 8773632  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 118      |
|    learning_rate   | 0.01     |
|    step_size       | 0.00126  |
---------------------------------
Eval num_timesteps=8800000, episode_reward=-0.82 +/- 0.53
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.818   |
| time/              |          |
|    total_timesteps | 8800000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.79     |
|    return_std      | 4.69     |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 26746    |
|    total_timesteps | 8847360  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 119      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000533 |
---------------------------------
Eval num_timesteps=8880000, episode_reward=-0.10 +/- 1.02
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.102   |
| time/              |          |
|    total_timesteps | 8880000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.46     |
|    return_std      | 2.87     |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 26973    |
|    total_timesteps | 8921088  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 120      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000872 |
---------------------------------
Eval num_timesteps=8960000, episode_reward=0.44 +/- 0.60
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.44     |
| time/              |          |
|    total_timesteps | 8960000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.38     |
|    return_std      | 1.82     |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 27200    |
|    total_timesteps | 8994816  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 121      |
|    learning_rate   | 0.01     |
|    step_size       | 0.00137  |
---------------------------------
Eval num_timesteps=9040000, episode_reward=-0.30 +/- 0.65
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.298   |
| time/              |          |
|    total_timesteps | 9040000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.2      |
|    return_std      | 2.88     |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 27429    |
|    total_timesteps | 9068544  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 122      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000868 |
---------------------------------
Eval num_timesteps=9120000, episode_reward=-1.11 +/- 0.72
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -1.11    |
| time/              |          |
|    total_timesteps | 9120000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.16     |
|    return_std      | 4.04     |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 27657    |
|    total_timesteps | 9142272  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 123      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000618 |
---------------------------------
Eval num_timesteps=9200000, episode_reward=-0.34 +/- 0.89
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.341   |
| time/              |          |
|    total_timesteps | 9200000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.81     |
|    return_std      | 4.33     |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 27885    |
|    total_timesteps | 9216000  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 124      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000577 |
---------------------------------
Eval num_timesteps=9280000, episode_reward=-1.03 +/- 0.19
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -1.03    |
| time/              |          |
|    total_timesteps | 9280000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.92     |
|    return_std      | 3.41     |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 28113    |
|    total_timesteps | 9289728  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 125      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000733 |
---------------------------------
Eval num_timesteps=9360000, episode_reward=-0.33 +/- 1.17
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.326   |
| time/              |          |
|    total_timesteps | 9360000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.42     |
|    return_std      | 2.82     |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 28342    |
|    total_timesteps | 9363456  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 126      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000885 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.19     |
|    return_std      | 3.41     |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 28564    |
|    total_timesteps | 9437184  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 127      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000732 |
---------------------------------
Eval num_timesteps=9440000, episode_reward=-0.47 +/- 0.84
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.468   |
| time/              |          |
|    total_timesteps | 9440000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4.46     |
|    return_std      | 3.04     |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 28792    |
|    total_timesteps | 9510912  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 128      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000824 |
---------------------------------
Eval num_timesteps=9520000, episode_reward=0.11 +/- 1.80
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.108    |
| time/              |          |
|    total_timesteps | 9520000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.75     |
|    return_std      | 3.8      |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 29021    |
|    total_timesteps | 9584640  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 129      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000658 |
---------------------------------
Eval num_timesteps=9600000, episode_reward=-0.48 +/- 1.07
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.483   |
| time/              |          |
|    total_timesteps | 9600000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.91     |
|    return_std      | 2.53     |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 29249    |
|    total_timesteps | 9658368  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 130      |
|    learning_rate   | 0.01     |
|    step_size       | 0.00099  |
---------------------------------
Eval num_timesteps=9680000, episode_reward=-0.77 +/- 0.61
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.769   |
| time/              |          |
|    total_timesteps | 9680000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 4        |
|    return_std      | 3.37     |
| time/              |          |
|    fps             | 330      |
|    time_elapsed    | 29484    |
|    total_timesteps | 9732096  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 131      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000743 |
---------------------------------
Eval num_timesteps=9760000, episode_reward=-0.81 +/- 0.57
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.812   |
| time/              |          |
|    total_timesteps | 9760000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 3.34     |
|    return_std      | 4.66     |
| time/              |          |
|    fps             | 329      |
|    time_elapsed    | 29720    |
|    total_timesteps | 9805824  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 132      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000537 |
---------------------------------
Eval num_timesteps=9840000, episode_reward=-0.05 +/- 1.28
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.0469  |
| time/              |          |
|    total_timesteps | 9840000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.47     |
|    return_std      | 3.61     |
| time/              |          |
|    fps             | 329      |
|    time_elapsed    | 29955    |
|    total_timesteps | 9879552  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 133      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000693 |
---------------------------------
Eval num_timesteps=9920000, episode_reward=-0.37 +/- 1.19
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.375   |
| time/              |          |
|    total_timesteps | 9920000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.03     |
|    return_std      | 3.57     |
| time/              |          |
|    fps             | 329      |
|    time_elapsed    | 30190    |
|    total_timesteps | 9953280  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 134      |
|    learning_rate   | 0.01     |
|    step_size       | 0.0007   |
---------------------------------
Eval num_timesteps=10000000, episode_reward=-0.70 +/- 0.89
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.699   |
| time/              |          |
|    total_timesteps | 10000000 |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.77     |
|    return_std      | 4.24     |
| time/              |          |
|    fps             | 329      |
|    time_elapsed    | 30422    |
|    total_timesteps | 10027008 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 135      |
|    learning_rate   | 0.01     |
|    step_size       | 0.000589 |
---------------------------------
Training complete. Model saved to logs/final_model/ars_evcharging_final
Evaluating model...
Mean reward: -0.22 +/- 1.01
