Configuration: 
algo: ARS
timesteps: 10000000
log_dir: logs
seed: 0
num_envs: 64
num_eval_envs: 2
checkpoint_freq: 50000
eval_freq: 5000
eval_episodes_during_training: 5
eval_episodes: 100
test_episodes: 10
PPO:
  learning_rate: 0.0005
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
RPPO:
  learning_rate: 0.0005
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  clip_range_vf: null
SAC:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  ent_coef: auto
  target_update_interval: 1
  target_entropy: auto
TD3:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 100
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  policy_delay: 2
  target_policy_noise: 0.2
  target_noise_clip: 0.5
DDPG:
  learning_rate: 0.001
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 100
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
CrossQ:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  ent_coef: 0.1
  sigma: 0.5
ARS:
  n_delta: 8
  delta_std: 0.05
  n_top: 4
  learning_rate: 0.01
  zero_policy: false
TQC:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  policy_kwargs: null
  top_quantiles_to_drop_per_net: 2
  n_critics: 5
  n_quantiles: 25
  ent_coef: auto
  target_entropy: auto

Observation space: Box([   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0. -288. -288. -288. -288. -288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.
 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.
 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.
 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 288. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.   1.   1.   1.   1.
   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
   1.   1.   1.   1.   1.   1.], (146,), float32)
Action space: Box(0.0, 1.0, (54,), float32)
Initializing ARS with parameters: {'n_delta': 8, 'delta_std': 0.05, 'n_top': 4, 'learning_rate': 0.01, 'zero_policy': False}
Using cpu device
Starting training with ARS...
Logging to logs/tensorboard/ARS_1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.351    |
|    return_std      | 0.963    |
| time/              |          |
|    fps             | 358      |
|    time_elapsed    | 823      |
|    total_timesteps | 294912   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 0        |
|    learning_rate   | 0.01     |
|    step_size       | 0.0026   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-0.27 +/- 0.50
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.271   |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | -0.3     |
|    return_std      | 0.888    |
| time/              |          |
|    fps             | 353      |
|    time_elapsed    | 1666     |
|    total_timesteps | 589824   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 1        |
|    learning_rate   | 0.01     |
|    step_size       | 0.00282  |
---------------------------------
Eval num_timesteps=640000, episode_reward=0.16 +/- 1.06
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.159    |
| time/              |          |
|    total_timesteps | 640000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.218    |
|    return_std      | 1.24     |
| time/              |          |
|    fps             | 352      |
|    time_elapsed    | 2508     |
|    total_timesteps | 884736   |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 2        |
|    learning_rate   | 0.01     |
|    step_size       | 0.00202  |
---------------------------------
Eval num_timesteps=960000, episode_reward=-0.40 +/- 0.98
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.4     |
| time/              |          |
|    total_timesteps | 960000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.402    |
|    return_std      | 0.682    |
| time/              |          |
|    fps             | 351      |
|    time_elapsed    | 3359     |
|    total_timesteps | 1179648  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 3        |
|    learning_rate   | 0.01     |
|    step_size       | 0.00366  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=-0.07 +/- 1.16
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.0709  |
| time/              |          |
|    total_timesteps | 1280000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.0354   |
|    return_std      | 1.91     |
| time/              |          |
|    fps             | 350      |
|    time_elapsed    | 4211     |
|    total_timesteps | 1474560  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 4        |
|    learning_rate   | 0.01     |
|    step_size       | 0.00131  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=0.26 +/- 1.35
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.261    |
| time/              |          |
|    total_timesteps | 1600000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.336    |
|    return_std      | 0.958    |
| time/              |          |
|    fps             | 349      |
|    time_elapsed    | 5058     |
|    total_timesteps | 1769472  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 5        |
|    learning_rate   | 0.01     |
|    step_size       | 0.00261  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=0.53 +/- 1.03
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.531    |
| time/              |          |
|    total_timesteps | 1920000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.293    |
|    return_std      | 1.02     |
| time/              |          |
|    fps             | 349      |
|    time_elapsed    | 5898     |
|    total_timesteps | 2064384  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 6        |
|    learning_rate   | 0.01     |
|    step_size       | 0.00244  |
---------------------------------
Eval num_timesteps=2240000, episode_reward=0.14 +/- 0.89
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.139    |
| time/              |          |
|    total_timesteps | 2240000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.372    |
|    return_std      | 1.23     |
| time/              |          |
|    fps             | 349      |
|    time_elapsed    | 6749     |
|    total_timesteps | 2359296  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 7        |
|    learning_rate   | 0.01     |
|    step_size       | 0.00203  |
---------------------------------
Eval num_timesteps=2560000, episode_reward=1.23 +/- 1.01
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 1.23     |
| time/              |          |
|    total_timesteps | 2560000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.265    |
|    return_std      | 0.913    |
| time/              |          |
|    fps             | 349      |
|    time_elapsed    | 7603     |
|    total_timesteps | 2654208  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 8        |
|    learning_rate   | 0.01     |
|    step_size       | 0.00274  |
---------------------------------
Eval num_timesteps=2880000, episode_reward=1.64 +/- 1.64
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 2880000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.233    |
|    return_std      | 1.22     |
| time/              |          |
|    fps             | 349      |
|    time_elapsed    | 8448     |
|    total_timesteps | 2949120  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 9        |
|    learning_rate   | 0.01     |
|    step_size       | 0.00205  |
---------------------------------
Eval num_timesteps=3200000, episode_reward=1.05 +/- 0.84
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 1.05     |
| time/              |          |
|    total_timesteps | 3200000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.0923   |
|    return_std      | 1.88     |
| time/              |          |
|    fps             | 348      |
|    time_elapsed    | 9295     |
|    total_timesteps | 3244032  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 10       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00133  |
---------------------------------
Eval num_timesteps=3520000, episode_reward=0.32 +/- 1.24
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.325    |
| time/              |          |
|    total_timesteps | 3520000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.314    |
|    return_std      | 1.37     |
| time/              |          |
|    fps             | 348      |
|    time_elapsed    | 10141    |
|    total_timesteps | 3538944  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 11       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00183  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.0124   |
|    return_std      | 0.717    |
| time/              |          |
|    fps             | 348      |
|    time_elapsed    | 10992    |
|    total_timesteps | 3833856  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 12       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00349  |
---------------------------------
Eval num_timesteps=3840000, episode_reward=-0.07 +/- 0.42
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.0712  |
| time/              |          |
|    total_timesteps | 3840000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | -0.152   |
|    return_std      | 1.47     |
| time/              |          |
|    fps             | 348      |
|    time_elapsed    | 11845    |
|    total_timesteps | 4128768  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 13       |
|    learning_rate   | 0.01     |
|    step_size       | 0.0017   |
---------------------------------
Eval num_timesteps=4160000, episode_reward=1.88 +/- 1.58
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 4160000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | -0.223   |
|    return_std      | 1.15     |
| time/              |          |
|    fps             | 348      |
|    time_elapsed    | 12682    |
|    total_timesteps | 4423680  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 14       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00216  |
---------------------------------
Eval num_timesteps=4480000, episode_reward=0.28 +/- 0.83
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.277    |
| time/              |          |
|    total_timesteps | 4480000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.618    |
|    return_std      | 1.4      |
| time/              |          |
|    fps             | 349      |
|    time_elapsed    | 13502    |
|    total_timesteps | 4718592  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 15       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00179  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=-0.41 +/- 0.52
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -0.406   |
| time/              |          |
|    total_timesteps | 4800000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | -0.124   |
|    return_std      | 1.54     |
| time/              |          |
|    fps             | 350      |
|    time_elapsed    | 14321    |
|    total_timesteps | 5013504  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 16       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00163  |
---------------------------------
Eval num_timesteps=5120000, episode_reward=1.07 +/- 1.12
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 1.07     |
| time/              |          |
|    total_timesteps | 5120000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | -0.0608  |
|    return_std      | 1.29     |
| time/              |          |
|    fps             | 350      |
|    time_elapsed    | 15143    |
|    total_timesteps | 5308416  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 17       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00193  |
---------------------------------
Eval num_timesteps=5440000, episode_reward=0.57 +/- 0.64
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.567    |
| time/              |          |
|    total_timesteps | 5440000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.249    |
|    return_std      | 1.5      |
| time/              |          |
|    fps             | 350      |
|    time_elapsed    | 15964    |
|    total_timesteps | 5603328  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 18       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00167  |
---------------------------------
Eval num_timesteps=5760000, episode_reward=0.39 +/- 1.03
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.395    |
| time/              |          |
|    total_timesteps | 5760000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.608    |
|    return_std      | 1.24     |
| time/              |          |
|    fps             | 351      |
|    time_elapsed    | 16789    |
|    total_timesteps | 5898240  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 19       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00202  |
---------------------------------
Eval num_timesteps=6080000, episode_reward=1.65 +/- 1.03
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 1.65     |
| time/              |          |
|    total_timesteps | 6080000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.343    |
|    return_std      | 1.99     |
| time/              |          |
|    fps             | 351      |
|    time_elapsed    | 17621    |
|    total_timesteps | 6193152  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 20       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00125  |
---------------------------------
Eval num_timesteps=6400000, episode_reward=1.10 +/- 1.80
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 6400000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.477    |
|    return_std      | 1.93     |
| time/              |          |
|    fps             | 350      |
|    time_elapsed    | 18508    |
|    total_timesteps | 6488064  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 21       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00129  |
---------------------------------
Eval num_timesteps=6720000, episode_reward=0.28 +/- 1.19
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.278    |
| time/              |          |
|    total_timesteps | 6720000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.468    |
|    return_std      | 1.73     |
| time/              |          |
|    fps             | 349      |
|    time_elapsed    | 19424    |
|    total_timesteps | 6782976  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 22       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00145  |
---------------------------------
Eval num_timesteps=7040000, episode_reward=1.78 +/- 1.68
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 7040000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.288    |
|    return_std      | 1.74     |
| time/              |          |
|    fps             | 347      |
|    time_elapsed    | 20340    |
|    total_timesteps | 7077888  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 23       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00144  |
---------------------------------
Eval num_timesteps=7360000, episode_reward=1.13 +/- 0.95
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 1.13     |
| time/              |          |
|    total_timesteps | 7360000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.772    |
|    return_std      | 1.44     |
| time/              |          |
|    fps             | 346      |
|    time_elapsed    | 21256    |
|    total_timesteps | 7372800  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 24       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00174  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | -0.183   |
|    return_std      | 1.1      |
| time/              |          |
|    fps             | 346      |
|    time_elapsed    | 22116    |
|    total_timesteps | 7667712  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 25       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00227  |
---------------------------------
Eval num_timesteps=7680000, episode_reward=1.38 +/- 1.13
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 1.38     |
| time/              |          |
|    total_timesteps | 7680000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.413    |
|    return_std      | 1.63     |
| time/              |          |
|    fps             | 347      |
|    time_elapsed    | 22940    |
|    total_timesteps | 7962624  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 26       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00153  |
---------------------------------
Eval num_timesteps=8000000, episode_reward=0.53 +/- 0.81
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.532    |
| time/              |          |
|    total_timesteps | 8000000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.624    |
|    return_std      | 1.3      |
| time/              |          |
|    fps             | 347      |
|    time_elapsed    | 23761    |
|    total_timesteps | 8257536  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 27       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00193  |
---------------------------------
Eval num_timesteps=8320000, episode_reward=0.82 +/- 0.99
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.823    |
| time/              |          |
|    total_timesteps | 8320000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.686    |
|    return_std      | 1.55     |
| time/              |          |
|    fps             | 347      |
|    time_elapsed    | 24587    |
|    total_timesteps | 8552448  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 28       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00161  |
---------------------------------
Eval num_timesteps=8640000, episode_reward=-1.01 +/- 0.67
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | -1.01    |
| time/              |          |
|    total_timesteps | 8640000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.00827  |
|    return_std      | 1.36     |
| time/              |          |
|    fps             | 347      |
|    time_elapsed    | 25432    |
|    total_timesteps | 8847360  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 29       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00183  |
---------------------------------
Eval num_timesteps=8960000, episode_reward=0.29 +/- 1.06
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.292    |
| time/              |          |
|    total_timesteps | 8960000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.02     |
|    return_std      | 1.47     |
| time/              |          |
|    fps             | 348      |
|    time_elapsed    | 26251    |
|    total_timesteps | 9142272  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 30       |
|    learning_rate   | 0.01     |
|    step_size       | 0.0017   |
---------------------------------
Eval num_timesteps=9280000, episode_reward=0.40 +/- 1.80
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.404    |
| time/              |          |
|    total_timesteps | 9280000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.572    |
|    return_std      | 1.35     |
| time/              |          |
|    fps             | 348      |
|    time_elapsed    | 27083    |
|    total_timesteps | 9437184  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 31       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00185  |
---------------------------------
Eval num_timesteps=9600000, episode_reward=0.26 +/- 0.95
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.262    |
| time/              |          |
|    total_timesteps | 9600000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.32     |
|    return_std      | 1.02     |
| time/              |          |
|    fps             | 348      |
|    time_elapsed    | 27904    |
|    total_timesteps | 9732096  |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 32       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00245  |
---------------------------------
Eval num_timesteps=9920000, episode_reward=1.02 +/- 1.00
Episode length: 288.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 1.02     |
| time/              |          |
|    total_timesteps | 9920000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.833    |
|    return_std      | 0.95     |
| time/              |          |
|    fps             | 349      |
|    time_elapsed    | 28730    |
|    total_timesteps | 10027008 |
| train/             |          |
|    delta_std       | 0.05     |
|    iterations      | 33       |
|    learning_rate   | 0.01     |
|    step_size       | 0.00263  |
---------------------------------
Training complete. Model saved to logs/final_model/ars_evcharging_final
Evaluating model...
Mean reward: 0.97 +/- 1.38
