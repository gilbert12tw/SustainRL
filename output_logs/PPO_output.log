❯ python3 train.py algo=PPO num_envs=128
/home/llm/SustainRL/train.py:36: UserWarning:
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="conf", config_name="config")
/home/llm/miniconda3/envs/torch_env/lib/python3.10/site-packag
es/hydra/_internal/hydra.py:119: UserWarning: Future Hydra ver
sions will no longer change working directory at job runtime b
y default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_j
ob_working_dir/ for more information.
  ret = run_job(
Configuration:
algo: PPO
timesteps: 10000000
log_dir: logs
seed: 0
num_envs: 128
num_eval_envs: 2
checkpoint_freq: 1000
eval_freq: 5000
eval_episodes_during_training: 5
eval_episodes: 100
test_episodes: 10
PPO:
  learning_rate: 0.0005
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
RPPO:
  learning_rate: 0.0005
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  clip_range_vf: null
SAC:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  ent_coef: auto
  target_update_interval: 1
  target_entropy: auto
TD3:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 100
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  policy_delay: 2
  target_policy_noise: 0.2
  target_noise_clip: 0.5
DDPG:
  learning_rate: 0.001
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 100
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
CrossQ:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  ent_coef: 0.1
  sigma: 0.5
ARS:
  n_delta: 8
  step_size: 0.02
  delta_std: 0.05
  rollout_steps: 1000
  n_top: 4
  learning_rate: 0.01
  delta_max: null
  zero_policy: false
  eval_episodes: 1
  scale_reward: true
TQC:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  policy_kwargs: null
  top_quantiles_to_drop_per_net: 2
  n_critics: 5
  n_quantiles: 25
  ent_coef: auto
  target_entropy: auto

/home/llm/miniconda3/envs/torch_env/lib/python3.10/site-packag
es/sklearn/base.py:329: UserWarning: Trying to unpickle estima
tor GaussianMixture from version 1.1.1 when using version 1.1.
3. This might lead to breaking code or invalid results. Use at
 your own risk. For more info please refer to:
https://scikit-learn.org/stable/model_persistence.html#securit
y-maintainability-limitations
  warnings.warn(
Observation space: Box([   0.    0.    0.    0.    0.    0.
 0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.
    0.    0.    0.    0.    0.    0. -288. -288. -288. -288. -
288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -
288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -
288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -
288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -
288. -288.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
  0.    0.
    0.    0.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 1
00. 100. 100. 100. 100.
 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 1
00. 100.
 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 1
00. 100.
 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 2
88. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 2
88. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 2
88. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 2
88. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.   1.   1.
 1.   1.
   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
 1.   1.
   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
 1.   1.
   1.   1.   1.   1.   1.   1.], (146,), float32)
Action space: Box(0.0, 1.0, (54,), float32)
Initializing PPO with parameters: {'learning_rate': 0.0005, 'n
_steps': 2048, 'batch_size': 64, 'n_epochs': 10, 'gamma': 0.99
, 'gae_lambda': 0.95, 'clip_range': 0.2, 'ent_coef': 0.01, 'vf
_coef': 0.5, 'max_grad_norm': 0.5}
Using cuda device
Starting training with PPO...
Logging to logs/tensorboard/PPO_3
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.62     |
| time/              |          |
|    fps             | 385      |
|    iterations      | 1        |
|    time_elapsed    | 679      |
|    total_timesteps | 262144   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.68        |
| time/                   |             |
|    fps                  | 274         |
|    iterations           | 2           |
|    time_elapsed         | 1907        |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.013621962 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -77         |
|    explained_variance   | -0.634      |
|    learning_rate        | 0.0005      |
|    loss                 | -0.724      |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.0159      |
|    std                  | 1.01        |
|    value_loss           | 0.0317      |
-----------------------------------------
Eval num_timesteps=640000, episode_reward=0.00 +/- 0.00
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 0          |
| time/                   |            |
|    total_timesteps      | 640000     |
| train/                  |            |
|    approx_kl            | 0.01508388 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    entropy_loss         | -77.3      |
|    explained_variance   | 0.264      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.793     |
|    n_updates            | 20         |
|    policy_gradient_loss | 0.00693    |
|    std                  | 1.01       |
|    value_loss           | 0.0112     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.83     |
| time/              |          |
|    fps             | 250      |
|    iterations      | 3        |
|    time_elapsed    | 3139     |
|    total_timesteps | 786432   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.8         |
| time/                   |             |
|    fps                  | 240         |
|    iterations           | 4           |
|    time_elapsed         | 4368        |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.015454315 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -77.5       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.747      |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.00586     |
|    std                  | 1.02        |
|    value_loss           | 0.00911     |
-----------------------------------------
Eval num_timesteps=1280000, episode_reward=0.02 +/- 0.02
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 0.015       |
| time/                   |             |
|    total_timesteps      | 1280000     |
| train/                  |             |
|    approx_kl            | 0.016394045 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -77.9       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.745      |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.00601     |
|    std                  | 1.02        |
|    value_loss           | 0.00957     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.81     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 5        |
|    time_elapsed    | 5603     |
|    total_timesteps | 1310720  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.62        |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 6           |
|    time_elapsed         | 6832        |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.017530825 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -78.2       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.702      |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.00666     |
|    std                  | 1.03        |
|    value_loss           | 0.0107      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.69        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 7           |
|    time_elapsed         | 8069        |
|    total_timesteps      | 1835008     |
| train/                  |             |
|    approx_kl            | 0.018617919 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -78.6       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.778      |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00697     |
|    std                  | 1.04        |
|    value_loss           | 0.0103      |
-----------------------------------------
Eval num_timesteps=1920000, episode_reward=0.02 +/- 0.03
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 0.0176      |
| time/                   |             |
|    total_timesteps      | 1920000     |
| train/                  |             |
|    approx_kl            | 0.019862132 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -79         |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.799      |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.00759     |
|    std                  | 1.04        |
|    value_loss           | 0.00982     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.6      |
| time/              |          |
|    fps             | 225      |
|    iterations      | 8        |
|    time_elapsed    | 9303     |
|    total_timesteps | 2097152  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 1.83      |
| time/                   |           |
|    fps                  | 223       |
|    iterations           | 9         |
|    time_elapsed         | 10543     |
|    total_timesteps      | 2359296   |
| train/                  |           |
|    approx_kl            | 0.0204915 |
|    clip_fraction        | 0.254     |
|    clip_range           | 0.2       |
|    entropy_loss         | -79.3     |
|    explained_variance   | 0.908     |
|    learning_rate        | 0.0005    |
|    loss                 | -0.799    |
|    n_updates            | 80        |
|    policy_gradient_loss | 0.00759   |
|    std                  | 1.05      |
|    value_loss           | 0.00907   |
---------------------------------------
Eval num_timesteps=2560000, episode_reward=0.02 +/- 0.02
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 0.0173     |
| time/                   |            |
|    total_timesteps      | 2560000    |
| train/                  |            |
|    approx_kl            | 0.02063924 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.2        |
|    entropy_loss         | -79.7      |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.787     |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.00767    |
|    std                  | 1.06       |
|    value_loss           | 0.00936    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.87     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 10       |
|    time_elapsed    | 11777    |
|    total_timesteps | 2621440  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 1.82       |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 11         |
|    time_elapsed         | 13009      |
|    total_timesteps      | 2883584    |
| train/                  |            |
|    approx_kl            | 0.02109247 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.2        |
|    entropy_loss         | -80.1      |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.799     |
|    n_updates            | 100        |
|    policy_gradient_loss | 0.00799    |
|    std                  | 1.07       |
|    value_loss           | 0.00948    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.85        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 12          |
|    time_elapsed         | 14240       |
|    total_timesteps      | 3145728     |
| train/                  |             |
|    approx_kl            | 0.020814262 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -80.5       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.778      |
|    n_updates            | 110         |
|    policy_gradient_loss | 0.00866     |
|    std                  | 1.08        |
|    value_loss           | 0.00953     |
-----------------------------------------
Eval num_timesteps=3200000, episode_reward=0.16 +/- 0.11
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 0.158       |
| time/                   |             |
|    total_timesteps      | 3200000     |
| train/                  |             |
|    approx_kl            | 0.021398094 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -81         |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.816      |
|    n_updates            | 120         |
|    policy_gradient_loss | 0.00865     |
|    std                  | 1.08        |
|    value_loss           | 0.00955     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.8      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 13       |
|    time_elapsed    | 15474    |
|    total_timesteps | 3407872  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 2.04       |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 14         |
|    time_elapsed         | 16704      |
|    total_timesteps      | 3670016    |
| train/                  |            |
|    approx_kl            | 0.02090216 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.2        |
|    entropy_loss         | -81.4      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.814     |
|    n_updates            | 130        |
|    policy_gradient_loss | 0.00859    |
|    std                  | 1.09       |
|    value_loss           | 0.00969    |
----------------------------------------
Eval num_timesteps=3840000, episode_reward=0.21 +/- 0.07
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 0.211      |
| time/                   |            |
|    total_timesteps      | 3840000    |
| train/                  |            |
|    approx_kl            | 0.02169681 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.2        |
|    entropy_loss         | -81.8      |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.813     |
|    n_updates            | 140        |
|    policy_gradient_loss | 0.00874    |
|    std                  | 1.1        |
|    value_loss           | 0.01       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2        |
| time/              |          |
|    fps             | 219      |
|    iterations      | 15       |
|    time_elapsed    | 17939    |
|    total_timesteps | 3932160  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.84        |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 16          |
|    time_elapsed         | 19167       |
|    total_timesteps      | 4194304     |
| train/                  |             |
|    approx_kl            | 0.022319004 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -82.2       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.82       |
|    n_updates            | 150         |
|    policy_gradient_loss | 0.00901     |
|    std                  | 1.11        |
|    value_loss           | 0.0104      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.75        |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 17          |
|    time_elapsed         | 20393       |
|    total_timesteps      | 4456448     |
| train/                  |             |
|    approx_kl            | 0.022870308 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -82.7       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0005      |
|    loss                 | -0.829      |
|    n_updates            | 160         |
|    policy_gradient_loss | 0.00903     |
|    std                  | 1.12        |
|    value_loss           | 0.0109      |
-----------------------------------------
Eval num_timesteps=4480000, episode_reward=0.35 +/- 0.18
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 0.347       |
| time/                   |             |
|    total_timesteps      | 4480000     |
| train/                  |             |
|    approx_kl            | 0.022535253 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -83.1       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.796      |
|    n_updates            | 170         |
|    policy_gradient_loss | 0.00952     |
|    std                  | 1.13        |
|    value_loss           | 0.0101      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.02     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 18       |
|    time_elapsed    | 21633    |
|    total_timesteps | 4718592  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.82        |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 19          |
|    time_elapsed         | 22859       |
|    total_timesteps      | 4980736     |
| train/                  |             |
|    approx_kl            | 0.023120303 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -83.5       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.779      |
|    n_updates            | 180         |
|    policy_gradient_loss | 0.0097      |
|    std                  | 1.14        |
|    value_loss           | 0.0105      |
-----------------------------------------
Eval num_timesteps=5120000, episode_reward=0.44 +/- 0.20
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 0.44       |
| time/                   |            |
|    total_timesteps      | 5120000    |
| train/                  |            |
|    approx_kl            | 0.02255787 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.2        |
|    entropy_loss         | -83.9      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.847     |
|    n_updates            | 190        |
|    policy_gradient_loss | 0.00959    |
|    std                  | 1.14       |
|    value_loss           | 0.0104     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.74     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 20       |
|    time_elapsed    | 24095    |
|    total_timesteps | 5242880  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.87        |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 21          |
|    time_elapsed         | 25327       |
|    total_timesteps      | 5505024     |
| train/                  |             |
|    approx_kl            | 0.022658192 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -84.3       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.856      |
|    n_updates            | 200         |
|    policy_gradient_loss | 0.00957     |
|    std                  | 1.15        |
|    value_loss           | 0.0105      |
-----------------------------------------
Eval num_timesteps=5760000, episode_reward=0.58 +/- 0.16
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 0.585       |
| time/                   |             |
|    total_timesteps      | 5760000     |
| train/                  |             |
|    approx_kl            | 0.023440149 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    entropy_loss         | -84.7       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.814      |
|    n_updates            | 210         |
|    policy_gradient_loss | 0.00979     |
|    std                  | 1.16        |
|    value_loss           | 0.0107      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2        |
| time/              |          |
|    fps             | 217      |
|    iterations      | 22       |
|    time_elapsed    | 26556    |
|    total_timesteps | 5767168  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 2.1         |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 23          |
|    time_elapsed         | 27786       |
|    total_timesteps      | 6029312     |
| train/                  |             |
|    approx_kl            | 0.023694295 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -85.1       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.871      |
|    n_updates            | 220         |
|    policy_gradient_loss | 0.00951     |
|    std                  | 1.17        |
|    value_loss           | 0.0114      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.79        |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 24          |
|    time_elapsed         | 29095       |
|    total_timesteps      | 6291456     |
| train/                  |             |
|    approx_kl            | 0.024222258 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -85.5       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.853      |
|    n_updates            | 230         |
|    policy_gradient_loss | 0.0102      |
|    std                  | 1.18        |
|    value_loss           | 0.0111      |
-----------------------------------------
Eval num_timesteps=6400000, episode_reward=0.83 +/- 0.32
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 0.83        |
| time/                   |             |
|    total_timesteps      | 6400000     |
| train/                  |             |
|    approx_kl            | 0.024236146 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -85.9       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.843      |
|    n_updates            | 240         |
|    policy_gradient_loss | 0.00956     |
|    std                  | 1.19        |
|    value_loss           | 0.0109      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.04     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 25       |
|    time_elapsed    | 30411    |
|    total_timesteps | 6553600  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.91        |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 26          |
|    time_elapsed         | 31633       |
|    total_timesteps      | 6815744     |
| train/                  |             |
|    approx_kl            | 0.024063973 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -86.3       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.757      |
|    n_updates            | 250         |
|    policy_gradient_loss | 0.0101      |
|    std                  | 1.2         |
|    value_loss           | 0.0107      |
-----------------------------------------
Eval num_timesteps=7040000, episode_reward=0.80 +/- 0.28
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 0.797       |
| time/                   |             |
|    total_timesteps      | 7040000     |
| train/                  |             |
|    approx_kl            | 0.024242625 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -86.7       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.823      |
|    n_updates            | 260         |
|    policy_gradient_loss | 0.00999     |
|    std                  | 1.21        |
|    value_loss           | 0.011       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.97     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 27       |
|    time_elapsed    | 32952    |
|    total_timesteps | 7077888  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 2.12        |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 28          |
|    time_elapsed         | 34264       |
|    total_timesteps      | 7340032     |
| train/                  |             |
|    approx_kl            | 0.025026496 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -87.2       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.842      |
|    n_updates            | 270         |
|    policy_gradient_loss | 0.0107      |
|    std                  | 1.22        |
|    value_loss           | 0.012       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.86        |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 29          |
|    time_elapsed         | 35496       |
|    total_timesteps      | 7602176     |
| train/                  |             |
|    approx_kl            | 0.025553938 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    entropy_loss         | -87.6       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.891      |
|    n_updates            | 280         |
|    policy_gradient_loss | 0.0105      |
|    std                  | 1.23        |
|    value_loss           | 0.0115      |
-----------------------------------------
Eval num_timesteps=7680000, episode_reward=1.01 +/- 0.44
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.01        |
| time/                   |             |
|    total_timesteps      | 7680000     |
| train/                  |             |
|    approx_kl            | 0.025486272 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -88         |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.888      |
|    n_updates            | 290         |
|    policy_gradient_loss | 0.0107      |
|    std                  | 1.23        |
|    value_loss           | 0.0111      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.32     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 30       |
|    time_elapsed    | 36813    |
|    total_timesteps | 7864320  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.99        |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 31          |
|    time_elapsed         | 38046       |
|    total_timesteps      | 8126464     |
| train/                  |             |
|    approx_kl            | 0.025622416 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -88.4       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.855      |
|    n_updates            | 300         |
|    policy_gradient_loss | 0.0108      |
|    std                  | 1.24        |
|    value_loss           | 0.0115      |
-----------------------------------------
Eval num_timesteps=8320000, episode_reward=0.90 +/- 0.29
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 0.902       |
| time/                   |             |
|    total_timesteps      | 8320000     |
| train/                  |             |
|    approx_kl            | 0.025615793 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    entropy_loss         | -88.8       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.889      |
|    n_updates            | 310         |
|    policy_gradient_loss | 0.0112      |
|    std                  | 1.25        |
|    value_loss           | 0.011       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.07     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 32       |
|    time_elapsed    | 39369    |
|    total_timesteps | 8388608  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.88        |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 33          |
|    time_elapsed         | 40679       |
|    total_timesteps      | 8650752     |
| train/                  |             |
|    approx_kl            | 0.025186446 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -89.2       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0005      |
|    loss                 | -0.868      |
|    n_updates            | 320         |
|    policy_gradient_loss | 0.0106      |
|    std                  | 1.26        |
|    value_loss           | 0.0116      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 2.18        |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 34          |
|    time_elapsed         | 41909       |
|    total_timesteps      | 8912896     |
| train/                  |             |
|    approx_kl            | 0.025547285 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    entropy_loss         | -89.6       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.837      |
|    n_updates            | 330         |
|    policy_gradient_loss | 0.0106      |
|    std                  | 1.27        |
|    value_loss           | 0.0118      |
-----------------------------------------
Eval num_timesteps=8960000, episode_reward=1.51 +/- 0.56
Episode length: 288.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.51        |
| time/                   |             |
|    total_timesteps      | 8960000     |
| train/                  |             |
|    approx_kl            | 0.025443006 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -90         |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0005      |
|    loss                 | -0.889      |
|    n_updates            | 340         |
|    policy_gradient_loss | 0.0109      |
|    std                  | 1.28        |
|    value_loss           | 0.0124      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 1.94     |
| time/              |          |
|    fps             | 212      |
|    iterations      | 35       |
|    time_elapsed    | 43150    |
|    total_timesteps | 9175040  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 1.97       |
| time/                   |            |
|    fps                  | 212        |
|    iterations           | 36         |
|    time_elapsed         | 44388      |
|    total_timesteps      | 9437184    |
| train/                  |            |
|    approx_kl            | 0.02604986 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -90.3      |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.908     |
|    n_updates            | 350        |
|    policy_gradient_loss | 0.0106     |
|    std                  | 1.29       |
|    value_loss           | 0.0123     |
----------------------------------------
Eval num_timesteps=9600000, episode_reward=1.21 +/- 0.46
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.21       |
| time/                   |            |
|    total_timesteps      | 9600000    |
| train/                  |            |
|    approx_kl            | 0.02572371 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -90.7      |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.879     |
|    n_updates            | 360        |
|    policy_gradient_loss | 0.0106     |
|    std                  | 1.3        |
|    value_loss           | 0.0126     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 2.1      |
| time/              |          |
|    fps             | 212      |
|    iterations      | 37       |
|    time_elapsed    | 45622    |
|    total_timesteps | 9699328  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.93        |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 38          |
|    time_elapsed         | 46856       |
|    total_timesteps      | 9961472     |
| train/                  |             |
|    approx_kl            | 0.025577545 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -91.2       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.893      |
|    n_updates            | 370         |
|    policy_gradient_loss | 0.0109      |
|    std                  | 1.31        |
|    value_loss           | 0.0124      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 1.99        |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 39          |
|    time_elapsed         | 48167       |
|    total_timesteps      | 10223616    |
| train/                  |             |
|    approx_kl            | 0.025624018 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -91.6       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0005      |
|    loss                 | -0.883      |
|    n_updates            | 380         |
|    policy_gradient_loss | 0.0106      |
|    std                  | 1.32        |
|    value_loss           | 0.0127      |
-----------------------------------------
Training complete. Model saved to logs/final_model/ppo_evcharging_final
Evaluating model...
100%|███████████████████████| 100/100 [01:32<00:00,  1.08it/s]
測試平均獎勵: 1.52 +/- 0.71
測試平均利潤: 1.98 +/- 0.86