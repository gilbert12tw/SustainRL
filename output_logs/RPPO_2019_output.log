Configuration: 
algo: RPPO
timesteps: 10000000
log_dir: logs
seed: 0
num_envs: 16
num_eval_envs: 2
checkpoint_freq: 50000
eval_freq: 5000
eval_episodes_during_training: 5
eval_episodes: 100
test_episodes: 10
PPO:
  learning_rate: 0.0005
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
RPPO:
  learning_rate: 0.0005
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  clip_range_vf: null
SAC:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  ent_coef: auto
  target_update_interval: 1
  target_entropy: auto
TD3:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 100
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  policy_delay: 2
  target_policy_noise: 0.2
  target_noise_clip: 0.5
DDPG:
  learning_rate: 0.001
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 100
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
CrossQ:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  ent_coef: 0.1
  sigma: 0.5
ARS:
  n_delta: 8
  delta_std: 0.05
  n_top: 4
  learning_rate: 0.01
  zero_policy: false
TQC:
  learning_rate: 0.0003
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  action_noise: null
  policy_kwargs: null
  top_quantiles_to_drop_per_net: 2
  ent_coef: auto
  target_entropy: auto

Observation space: Box([   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0. -288. -288. -288. -288. -288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288.
 -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288. -288.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
    0.    0.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.
 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.
 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.
 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 288. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.
 288. 288. 288. 288. 288. 288. 288. 288. 288. 288.   1.   1.   1.   1.
   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.
   1.   1.   1.   1.   1.   1.], (146,), float32)
Action space: Box(0.0, 1.0, (54,), float32)
Initializing RPPO with parameters: {'learning_rate': 0.0005, 'n_steps': 2048, 'batch_size': 64, 'n_epochs': 10, 'gamma': 0.99, 'gae_lambda': 0.95, 'clip_range': 0.2, 'ent_coef': 0.01, 'vf_coef': 0.5, 'max_grad_norm': 0.5, 'clip_range_vf': None}
Using cuda device
Starting training with RPPO...
Logging to logs/tensorboard/RecurrentPPO_3
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.39     |
| time/              |          |
|    fps             | 446      |
|    iterations      | 1        |
|    time_elapsed    | 73       |
|    total_timesteps | 32768    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.52       |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 2          |
|    time_elapsed         | 227        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.07262424 |
|    clip_fraction        | 0.402      |
|    clip_range           | 0.2        |
|    entropy_loss         | -77        |
|    explained_variance   | -0.046     |
|    learning_rate        | 0.0005     |
|    loss                 | -0.865     |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0224    |
|    std                  | 1.01       |
|    value_loss           | 0.0405     |
----------------------------------------
Eval num_timesteps=80000, episode_reward=0.02 +/- 0.02
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 0.0151     |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.10297437 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.2        |
|    entropy_loss         | -77.4      |
|    explained_variance   | 0.749      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.767     |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.016     |
|    std                  | 1.02       |
|    value_loss           | 0.049      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.77     |
| time/              |          |
|    fps             | 253      |
|    iterations      | 3        |
|    time_elapsed    | 387      |
|    total_timesteps | 98304    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.44      |
| time/                   |           |
|    fps                  | 241       |
|    iterations           | 4         |
|    time_elapsed         | 542       |
|    total_timesteps      | 131072    |
| train/                  |           |
|    approx_kl            | 0.1688877 |
|    clip_fraction        | 0.495     |
|    clip_range           | 0.2       |
|    entropy_loss         | -77.9     |
|    explained_variance   | 0.845     |
|    learning_rate        | 0.0005    |
|    loss                 | -0.728    |
|    n_updates            | 30        |
|    policy_gradient_loss | -0.0291   |
|    std                  | 1.03      |
|    value_loss           | 0.0827    |
---------------------------------------
Eval num_timesteps=160000, episode_reward=0.17 +/- 0.12
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 0.166      |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.19522932 |
|    clip_fraction        | 0.528      |
|    clip_range           | 0.2        |
|    entropy_loss         | -78.4      |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0005     |
|    loss                 | -0.794     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0209    |
|    std                  | 1.04       |
|    value_loss           | 0.0696     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.13     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 5        |
|    time_elapsed    | 702      |
|    total_timesteps | 163840   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.42      |
| time/                   |           |
|    fps                  | 229       |
|    iterations           | 6         |
|    time_elapsed         | 856       |
|    total_timesteps      | 196608    |
| train/                  |           |
|    approx_kl            | 0.2400797 |
|    clip_fraction        | 0.541     |
|    clip_range           | 0.2       |
|    entropy_loss         | -79       |
|    explained_variance   | 0.881     |
|    learning_rate        | 0.0005    |
|    loss                 | -0.776    |
|    n_updates            | 50        |
|    policy_gradient_loss | -0.0256   |
|    std                  | 1.05      |
|    value_loss           | 0.065     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.11       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 7          |
|    time_elapsed         | 1014       |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.25220692 |
|    clip_fraction        | 0.548      |
|    clip_range           | 0.2        |
|    entropy_loss         | -79.5      |
|    explained_variance   | 0.894      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.827     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0179    |
|    std                  | 1.06       |
|    value_loss           | 0.0605     |
----------------------------------------
Eval num_timesteps=240000, episode_reward=0.18 +/- 0.06
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 0.178      |
| time/                   |            |
|    total_timesteps      | 240000     |
| train/                  |            |
|    approx_kl            | 0.28636897 |
|    clip_fraction        | 0.545      |
|    clip_range           | 0.2        |
|    entropy_loss         | -80.1      |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.795     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0198    |
|    std                  | 1.07       |
|    value_loss           | 0.0673     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.43     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 8        |
|    time_elapsed    | 1179     |
|    total_timesteps | 262144   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.12       |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 9          |
|    time_elapsed         | 1338       |
|    total_timesteps      | 294912     |
| train/                  |            |
|    approx_kl            | 0.30840218 |
|    clip_fraction        | 0.553      |
|    clip_range           | 0.2        |
|    entropy_loss         | -80.6      |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.893     |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0196    |
|    std                  | 1.08       |
|    value_loss           | 0.0647     |
----------------------------------------
Eval num_timesteps=320000, episode_reward=0.30 +/- 0.21
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 0.298     |
| time/                   |           |
|    total_timesteps      | 320000    |
| train/                  |           |
|    approx_kl            | 0.3126876 |
|    clip_fraction        | 0.542     |
|    clip_range           | 0.2       |
|    entropy_loss         | -81.1     |
|    explained_variance   | 0.906     |
|    learning_rate        | 0.0005    |
|    loss                 | -0.824    |
|    n_updates            | 90        |
|    policy_gradient_loss | -0.0161   |
|    std                  | 1.09      |
|    value_loss           | 0.0584    |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.94     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 10       |
|    time_elapsed    | 1500     |
|    total_timesteps | 327680   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.45       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 11         |
|    time_elapsed         | 1655       |
|    total_timesteps      | 360448     |
| train/                  |            |
|    approx_kl            | 0.35150886 |
|    clip_fraction        | 0.561      |
|    clip_range           | 0.2        |
|    entropy_loss         | -81.7      |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.797     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0156    |
|    std                  | 1.1        |
|    value_loss           | 0.0578     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.54       |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 12         |
|    time_elapsed         | 1815       |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.33779177 |
|    clip_fraction        | 0.561      |
|    clip_range           | 0.2        |
|    entropy_loss         | -82.2      |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.89      |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0076    |
|    std                  | 1.11       |
|    value_loss           | 0.0637     |
----------------------------------------
Eval num_timesteps=400000, episode_reward=0.60 +/- 0.36
Episode length: 288.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 288      |
|    mean_reward          | 0.597    |
| time/                   |          |
|    total_timesteps      | 400000   |
| train/                  |          |
|    approx_kl            | 0.346797 |
|    clip_fraction        | 0.565    |
|    clip_range           | 0.2      |
|    entropy_loss         | -82.8    |
|    explained_variance   | 0.916    |
|    learning_rate        | 0.0005   |
|    loss                 | -0.87    |
|    n_updates            | 120      |
|    policy_gradient_loss | -0.00863 |
|    std                  | 1.12     |
|    value_loss           | 0.0557   |
--------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.42     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 13       |
|    time_elapsed    | 1985     |
|    total_timesteps | 425984   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.9       |
| time/                   |           |
|    fps                  | 213       |
|    iterations           | 14        |
|    time_elapsed         | 2151      |
|    total_timesteps      | 458752    |
| train/                  |           |
|    approx_kl            | 0.3448953 |
|    clip_fraction        | 0.574     |
|    clip_range           | 0.2       |
|    entropy_loss         | -83.4     |
|    explained_variance   | 0.914     |
|    learning_rate        | 0.0005    |
|    loss                 | -0.808    |
|    n_updates            | 130       |
|    policy_gradient_loss | -0.00747  |
|    std                  | 1.14      |
|    value_loss           | 0.0479    |
---------------------------------------
Eval num_timesteps=480000, episode_reward=0.50 +/- 0.16
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 0.497     |
| time/                   |           |
|    total_timesteps      | 480000    |
| train/                  |           |
|    approx_kl            | 0.3332627 |
|    clip_fraction        | 0.559     |
|    clip_range           | 0.2       |
|    entropy_loss         | -83.9     |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0005    |
|    loss                 | -0.813    |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.00561  |
|    std                  | 1.15      |
|    value_loss           | 0.0493    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.22     |
| time/              |          |
|    fps             | 211      |
|    iterations      | 15       |
|    time_elapsed    | 2320     |
|    total_timesteps | 491520   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.66       |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 16         |
|    time_elapsed         | 2481       |
|    total_timesteps      | 524288     |
| train/                  |            |
|    approx_kl            | 0.35935062 |
|    clip_fraction        | 0.572      |
|    clip_range           | 0.2        |
|    entropy_loss         | -84.5      |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.764     |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0127    |
|    std                  | 1.16       |
|    value_loss           | 0.0557     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.66       |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 17         |
|    time_elapsed         | 2636       |
|    total_timesteps      | 557056     |
| train/                  |            |
|    approx_kl            | 0.37667912 |
|    clip_fraction        | 0.568      |
|    clip_range           | 0.2        |
|    entropy_loss         | -85.1      |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.844     |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0151    |
|    std                  | 1.17       |
|    value_loss           | 0.0546     |
----------------------------------------
Eval num_timesteps=560000, episode_reward=1.01 +/- 0.54
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.01       |
| time/                   |            |
|    total_timesteps      | 560000     |
| train/                  |            |
|    approx_kl            | 0.38239086 |
|    clip_fraction        | 0.572      |
|    clip_range           | 0.2        |
|    entropy_loss         | -85.7      |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.873     |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0127    |
|    std                  | 1.18       |
|    value_loss           | 0.047      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.6      |
| time/              |          |
|    fps             | 210      |
|    iterations      | 18       |
|    time_elapsed    | 2801     |
|    total_timesteps | 589824   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.33       |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 19         |
|    time_elapsed         | 2965       |
|    total_timesteps      | 622592     |
| train/                  |            |
|    approx_kl            | 0.38451067 |
|    clip_fraction        | 0.565      |
|    clip_range           | 0.2        |
|    entropy_loss         | -86.2      |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.858     |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0104    |
|    std                  | 1.2        |
|    value_loss           | 0.0453     |
----------------------------------------
Eval num_timesteps=640000, episode_reward=0.86 +/- 0.40
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 0.861      |
| time/                   |            |
|    total_timesteps      | 640000     |
| train/                  |            |
|    approx_kl            | 0.37946713 |
|    clip_fraction        | 0.559      |
|    clip_range           | 0.2        |
|    entropy_loss         | -86.8      |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.839     |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0092    |
|    std                  | 1.21       |
|    value_loss           | 0.0446     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.54     |
| time/              |          |
|    fps             | 208      |
|    iterations      | 20       |
|    time_elapsed    | 3135     |
|    total_timesteps | 655360   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.7        |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 21         |
|    time_elapsed         | 3299       |
|    total_timesteps      | 688128     |
| train/                  |            |
|    approx_kl            | 0.35240406 |
|    clip_fraction        | 0.552      |
|    clip_range           | 0.2        |
|    entropy_loss         | -87.3      |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.887     |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.00693   |
|    std                  | 1.22       |
|    value_loss           | 0.0451     |
----------------------------------------
Eval num_timesteps=720000, episode_reward=0.76 +/- 0.33
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 0.763     |
| time/                   |           |
|    total_timesteps      | 720000    |
| train/                  |           |
|    approx_kl            | 0.3702638 |
|    clip_fraction        | 0.565     |
|    clip_range           | 0.2       |
|    entropy_loss         | -87.8     |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0005    |
|    loss                 | -0.882    |
|    n_updates            | 210       |
|    policy_gradient_loss | -0.00631  |
|    std                  | 1.23      |
|    value_loss           | 0.052     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.81     |
| time/              |          |
|    fps             | 207      |
|    iterations      | 22       |
|    time_elapsed    | 3467     |
|    total_timesteps | 720896   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.54       |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 23         |
|    time_elapsed         | 3624       |
|    total_timesteps      | 753664     |
| train/                  |            |
|    approx_kl            | 0.39437783 |
|    clip_fraction        | 0.573      |
|    clip_range           | 0.2        |
|    entropy_loss         | -88.4      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.839     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.00932   |
|    std                  | 1.25       |
|    value_loss           | 0.0456     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.52       |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 24         |
|    time_elapsed         | 3780       |
|    total_timesteps      | 786432     |
| train/                  |            |
|    approx_kl            | 0.38286656 |
|    clip_fraction        | 0.565      |
|    clip_range           | 0.2        |
|    entropy_loss         | -88.9      |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.914     |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.00568   |
|    std                  | 1.26       |
|    value_loss           | 0.0547     |
----------------------------------------
Eval num_timesteps=800000, episode_reward=0.90 +/- 0.27
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 0.905     |
| time/                   |           |
|    total_timesteps      | 800000    |
| train/                  |           |
|    approx_kl            | 0.4184789 |
|    clip_fraction        | 0.576     |
|    clip_range           | 0.2       |
|    entropy_loss         | -89.5     |
|    explained_variance   | 0.918     |
|    learning_rate        | 0.0005    |
|    loss                 | -0.929    |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.00552  |
|    std                  | 1.27      |
|    value_loss           | 0.0542    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.85     |
| time/              |          |
|    fps             | 207      |
|    iterations      | 25       |
|    time_elapsed    | 3946     |
|    total_timesteps | 819200   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.7        |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 26         |
|    time_elapsed         | 4104       |
|    total_timesteps      | 851968     |
| train/                  |            |
|    approx_kl            | 0.40252417 |
|    clip_fraction        | 0.575      |
|    clip_range           | 0.2        |
|    entropy_loss         | -90.2      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.98      |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0102    |
|    std                  | 1.29       |
|    value_loss           | 0.0564     |
----------------------------------------
Eval num_timesteps=880000, episode_reward=1.10 +/- 0.48
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.1        |
| time/                   |            |
|    total_timesteps      | 880000     |
| train/                  |            |
|    approx_kl            | 0.39539036 |
|    clip_fraction        | 0.569      |
|    clip_range           | 0.2        |
|    entropy_loss         | -90.7      |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.984     |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0124    |
|    std                  | 1.3        |
|    value_loss           | 0.0478     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6        |
| time/              |          |
|    fps             | 207      |
|    iterations      | 27       |
|    time_elapsed    | 4265     |
|    total_timesteps | 884736   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.56      |
| time/                   |           |
|    fps                  | 207       |
|    iterations           | 28        |
|    time_elapsed         | 4423      |
|    total_timesteps      | 917504    |
| train/                  |           |
|    approx_kl            | 0.4109348 |
|    clip_fraction        | 0.571     |
|    clip_range           | 0.2       |
|    entropy_loss         | -91.2     |
|    explained_variance   | 0.927     |
|    learning_rate        | 0.0005    |
|    loss                 | -0.973    |
|    n_updates            | 270       |
|    policy_gradient_loss | -0.0173   |
|    std                  | 1.31      |
|    value_loss           | 0.0509    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.3        |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 29         |
|    time_elapsed         | 4589       |
|    total_timesteps      | 950272     |
| train/                  |            |
|    approx_kl            | 0.41520542 |
|    clip_fraction        | 0.569      |
|    clip_range           | 0.2        |
|    entropy_loss         | -91.8      |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.967     |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.00623   |
|    std                  | 1.33       |
|    value_loss           | 0.0477     |
----------------------------------------
Eval num_timesteps=960000, episode_reward=1.04 +/- 0.34
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.04       |
| time/                   |            |
|    total_timesteps      | 960000     |
| train/                  |            |
|    approx_kl            | 0.41594684 |
|    clip_fraction        | 0.57       |
|    clip_range           | 0.2        |
|    entropy_loss         | -92.3      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.922     |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.00661   |
|    std                  | 1.34       |
|    value_loss           | 0.0492     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.54     |
| time/              |          |
|    fps             | 206      |
|    iterations      | 30       |
|    time_elapsed    | 4756     |
|    total_timesteps | 983040   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.59       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 31         |
|    time_elapsed         | 4917       |
|    total_timesteps      | 1015808    |
| train/                  |            |
|    approx_kl            | 0.42590132 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -92.9      |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.941     |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.00555   |
|    std                  | 1.35       |
|    value_loss           | 0.0453     |
----------------------------------------
Eval num_timesteps=1040000, episode_reward=1.56 +/- 0.67
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.56       |
| time/                   |            |
|    total_timesteps      | 1040000    |
| train/                  |            |
|    approx_kl            | 0.41446573 |
|    clip_fraction        | 0.571      |
|    clip_range           | 0.2        |
|    entropy_loss         | -93.4      |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0005     |
|    loss                 | -0.893     |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.00859   |
|    std                  | 1.37       |
|    value_loss           | 0.0549     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.24     |
| time/              |          |
|    fps             | 206      |
|    iterations      | 32       |
|    time_elapsed    | 5085     |
|    total_timesteps | 1048576  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.17       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 33         |
|    time_elapsed         | 5250       |
|    total_timesteps      | 1081344    |
| train/                  |            |
|    approx_kl            | 0.42656755 |
|    clip_fraction        | 0.589      |
|    clip_range           | 0.2        |
|    entropy_loss         | -94        |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.03      |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0109    |
|    std                  | 1.38       |
|    value_loss           | 0.0528     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.34       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 34         |
|    time_elapsed         | 5411       |
|    total_timesteps      | 1114112    |
| train/                  |            |
|    approx_kl            | 0.45190156 |
|    clip_fraction        | 0.594      |
|    clip_range           | 0.2        |
|    entropy_loss         | -94.5      |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.966     |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.00811   |
|    std                  | 1.4        |
|    value_loss           | 0.0551     |
----------------------------------------
Eval num_timesteps=1120000, episode_reward=0.97 +/- 0.25
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 0.97      |
| time/                   |           |
|    total_timesteps      | 1120000   |
| train/                  |           |
|    approx_kl            | 0.4582577 |
|    clip_fraction        | 0.584     |
|    clip_range           | 0.2       |
|    entropy_loss         | -95.1     |
|    explained_variance   | 0.935     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.01     |
|    n_updates            | 340       |
|    policy_gradient_loss | -0.00441  |
|    std                  | 1.41      |
|    value_loss           | 0.0551    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.13     |
| time/              |          |
|    fps             | 205      |
|    iterations      | 35       |
|    time_elapsed    | 5576     |
|    total_timesteps | 1146880  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.22       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 36         |
|    time_elapsed         | 5740       |
|    total_timesteps      | 1179648    |
| train/                  |            |
|    approx_kl            | 0.43139035 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.2        |
|    entropy_loss         | -95.7      |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.91      |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.00181   |
|    std                  | 1.42       |
|    value_loss           | 0.0563     |
----------------------------------------
Eval num_timesteps=1200000, episode_reward=1.37 +/- 0.48
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.37       |
| time/                   |            |
|    total_timesteps      | 1200000    |
| train/                  |            |
|    approx_kl            | 0.46001384 |
|    clip_fraction        | 0.576      |
|    clip_range           | 0.2        |
|    entropy_loss         | -96.2      |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.03      |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.00298   |
|    std                  | 1.44       |
|    value_loss           | 0.0605     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.97     |
| time/              |          |
|    fps             | 205      |
|    iterations      | 37       |
|    time_elapsed    | 5904     |
|    total_timesteps | 1212416  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.83       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 38         |
|    time_elapsed         | 6068       |
|    total_timesteps      | 1245184    |
| train/                  |            |
|    approx_kl            | 0.42915982 |
|    clip_fraction        | 0.57       |
|    clip_range           | 0.2        |
|    entropy_loss         | -96.7      |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.962     |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.00744   |
|    std                  | 1.45       |
|    value_loss           | 0.0512     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.79       |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 39         |
|    time_elapsed         | 6230       |
|    total_timesteps      | 1277952    |
| train/                  |            |
|    approx_kl            | 0.45910352 |
|    clip_fraction        | 0.577      |
|    clip_range           | 0.2        |
|    entropy_loss         | -97.3      |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.954     |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.00927   |
|    std                  | 1.47       |
|    value_loss           | 0.05       |
----------------------------------------
Eval num_timesteps=1280000, episode_reward=1.49 +/- 0.29
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.49       |
| time/                   |            |
|    total_timesteps      | 1280000    |
| train/                  |            |
|    approx_kl            | 0.45076334 |
|    clip_fraction        | 0.579      |
|    clip_range           | 0.2        |
|    entropy_loss         | -97.8      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.0005     |
|    loss                 | -1         |
|    n_updates            | 390        |
|    policy_gradient_loss | 0.00017    |
|    std                  | 1.48       |
|    value_loss           | 0.0454     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.47     |
| time/              |          |
|    fps             | 205      |
|    iterations      | 40       |
|    time_elapsed    | 6393     |
|    total_timesteps | 1310720  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.33       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 41         |
|    time_elapsed         | 6554       |
|    total_timesteps      | 1343488    |
| train/                  |            |
|    approx_kl            | 0.44972074 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -98.3      |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.924     |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.00352   |
|    std                  | 1.5        |
|    value_loss           | 0.051      |
----------------------------------------
Eval num_timesteps=1360000, episode_reward=1.15 +/- 0.42
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.15       |
| time/                   |            |
|    total_timesteps      | 1360000    |
| train/                  |            |
|    approx_kl            | 0.45536083 |
|    clip_fraction        | 0.589      |
|    clip_range           | 0.2        |
|    entropy_loss         | -98.9      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.01      |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.0011    |
|    std                  | 1.51       |
|    value_loss           | 0.0545     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.94     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 42       |
|    time_elapsed    | 6722     |
|    total_timesteps | 1376256  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.25       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 43         |
|    time_elapsed         | 6884       |
|    total_timesteps      | 1409024    |
| train/                  |            |
|    approx_kl            | 0.47063935 |
|    clip_fraction        | 0.592      |
|    clip_range           | 0.2        |
|    entropy_loss         | -99.5      |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.08      |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.00614   |
|    std                  | 1.53       |
|    value_loss           | 0.056      |
----------------------------------------
Eval num_timesteps=1440000, episode_reward=1.72 +/- 0.42
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.72       |
| time/                   |            |
|    total_timesteps      | 1440000    |
| train/                  |            |
|    approx_kl            | 0.47402614 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.2        |
|    entropy_loss         | -100       |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0005     |
|    loss                 | -1         |
|    n_updates            | 430        |
|    policy_gradient_loss | -0.00914   |
|    std                  | 1.55       |
|    value_loss           | 0.0565     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.32     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 44       |
|    time_elapsed    | 7053     |
|    total_timesteps | 1441792  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6         |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 45        |
|    time_elapsed         | 7214      |
|    total_timesteps      | 1474560   |
| train/                  |           |
|    approx_kl            | 0.4716863 |
|    clip_fraction        | 0.595     |
|    clip_range           | 0.2       |
|    entropy_loss         | -101      |
|    explained_variance   | 0.935     |
|    learning_rate        | 0.0005    |
|    loss                 | -0.969    |
|    n_updates            | 440       |
|    policy_gradient_loss | -0.00592  |
|    std                  | 1.56      |
|    value_loss           | 0.0538    |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.79      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 46        |
|    time_elapsed         | 7372      |
|    total_timesteps      | 1507328   |
| train/                  |           |
|    approx_kl            | 0.4559465 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.2       |
|    entropy_loss         | -101      |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.08     |
|    n_updates            | 450       |
|    policy_gradient_loss | -0.00309  |
|    std                  | 1.58      |
|    value_loss           | 0.054     |
---------------------------------------
Eval num_timesteps=1520000, episode_reward=1.74 +/- 0.55
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.74       |
| time/                   |            |
|    total_timesteps      | 1520000    |
| train/                  |            |
|    approx_kl            | 0.44715226 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -102       |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.969     |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.00507   |
|    std                  | 1.6        |
|    value_loss           | 0.0521     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.29     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 47       |
|    time_elapsed    | 7540     |
|    total_timesteps | 1540096  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.42       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 48         |
|    time_elapsed         | 7701       |
|    total_timesteps      | 1572864    |
| train/                  |            |
|    approx_kl            | 0.43878466 |
|    clip_fraction        | 0.567      |
|    clip_range           | 0.2        |
|    entropy_loss         | -102       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.11      |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.00368   |
|    std                  | 1.61       |
|    value_loss           | 0.0535     |
----------------------------------------
Eval num_timesteps=1600000, episode_reward=1.84 +/- 0.68
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 1.84      |
| time/                   |           |
|    total_timesteps      | 1600000   |
| train/                  |           |
|    approx_kl            | 0.4173105 |
|    clip_fraction        | 0.574     |
|    clip_range           | 0.2       |
|    entropy_loss         | -103      |
|    explained_variance   | 0.923     |
|    learning_rate        | 0.0005    |
|    loss                 | -0.882    |
|    n_updates            | 480       |
|    policy_gradient_loss | -0.00487  |
|    std                  | 1.63      |
|    value_loss           | 0.0555    |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.46     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 49       |
|    time_elapsed    | 7866     |
|    total_timesteps | 1605632  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.89       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 50         |
|    time_elapsed         | 8029       |
|    total_timesteps      | 1638400    |
| train/                  |            |
|    approx_kl            | 0.44921473 |
|    clip_fraction        | 0.588      |
|    clip_range           | 0.2        |
|    entropy_loss         | -103       |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.882     |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.00722   |
|    std                  | 1.65       |
|    value_loss           | 0.0588     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.57       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 51         |
|    time_elapsed         | 8191       |
|    total_timesteps      | 1671168    |
| train/                  |            |
|    approx_kl            | 0.43669444 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.2        |
|    entropy_loss         | -104       |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.05      |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.00309   |
|    std                  | 1.67       |
|    value_loss           | 0.057      |
----------------------------------------
Eval num_timesteps=1680000, episode_reward=1.24 +/- 0.68
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.24       |
| time/                   |            |
|    total_timesteps      | 1680000    |
| train/                  |            |
|    approx_kl            | 0.45327765 |
|    clip_fraction        | 0.586      |
|    clip_range           | 0.2        |
|    entropy_loss         | -105       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.02      |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.00587   |
|    std                  | 1.68       |
|    value_loss           | 0.0596     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.23     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 52       |
|    time_elapsed    | 8357     |
|    total_timesteps | 1703936  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.66       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 53         |
|    time_elapsed         | 8518       |
|    total_timesteps      | 1736704    |
| train/                  |            |
|    approx_kl            | 0.46791556 |
|    clip_fraction        | 0.586      |
|    clip_range           | 0.2        |
|    entropy_loss         | -105       |
|    explained_variance   | 0.937      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.04      |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.00842   |
|    std                  | 1.7        |
|    value_loss           | 0.0532     |
----------------------------------------
Eval num_timesteps=1760000, episode_reward=1.73 +/- 0.69
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.73       |
| time/                   |            |
|    total_timesteps      | 1760000    |
| train/                  |            |
|    approx_kl            | 0.42585415 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.2        |
|    entropy_loss         | -106       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.09      |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.00531   |
|    std                  | 1.72       |
|    value_loss           | 0.0625     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.68     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 54       |
|    time_elapsed    | 8682     |
|    total_timesteps | 1769472  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 288      |
|    ep_rew_mean          | 5.68     |
| time/                   |          |
|    fps                  | 203      |
|    iterations           | 55       |
|    time_elapsed         | 8839     |
|    total_timesteps      | 1802240  |
| train/                  |          |
|    approx_kl            | 0.426081 |
|    clip_fraction        | 0.573    |
|    clip_range           | 0.2      |
|    entropy_loss         | -106     |
|    explained_variance   | 0.929    |
|    learning_rate        | 0.0005   |
|    loss                 | -1.07    |
|    n_updates            | 540      |
|    policy_gradient_loss | -0.00848 |
|    std                  | 1.74     |
|    value_loss           | 0.053    |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.26       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 56         |
|    time_elapsed         | 8999       |
|    total_timesteps      | 1835008    |
| train/                  |            |
|    approx_kl            | 0.41514367 |
|    clip_fraction        | 0.565      |
|    clip_range           | 0.2        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.12      |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.00635   |
|    std                  | 1.76       |
|    value_loss           | 0.0598     |
----------------------------------------
Eval num_timesteps=1840000, episode_reward=1.27 +/- 0.55
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.27       |
| time/                   |            |
|    total_timesteps      | 1840000    |
| train/                  |            |
|    approx_kl            | 0.42355442 |
|    clip_fraction        | 0.569      |
|    clip_range           | 0.2        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.05      |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.00894   |
|    std                  | 1.78       |
|    value_loss           | 0.0567     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.99     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 57       |
|    time_elapsed    | 9161     |
|    total_timesteps | 1867776  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.03       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 58         |
|    time_elapsed         | 9314       |
|    total_timesteps      | 1900544    |
| train/                  |            |
|    approx_kl            | 0.42833883 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.2        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0005     |
|    loss                 | -1.02      |
|    n_updates            | 570        |
|    policy_gradient_loss | -0.00448   |
|    std                  | 1.8        |
|    value_loss           | 0.053      |
----------------------------------------
Eval num_timesteps=1920000, episode_reward=2.08 +/- 0.80
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.08       |
| time/                   |            |
|    total_timesteps      | 1920000    |
| train/                  |            |
|    approx_kl            | 0.41758358 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.2        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.07      |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.00905   |
|    std                  | 1.82       |
|    value_loss           | 0.054      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.5      |
| time/              |          |
|    fps             | 204      |
|    iterations      | 59       |
|    time_elapsed    | 9472     |
|    total_timesteps | 1933312  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.6        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 60         |
|    time_elapsed         | 9633       |
|    total_timesteps      | 1966080    |
| train/                  |            |
|    approx_kl            | 0.41251165 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.2        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.12      |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.00695   |
|    std                  | 1.84       |
|    value_loss           | 0.0603     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.46      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 61        |
|    time_elapsed         | 9796      |
|    total_timesteps      | 1998848   |
| train/                  |           |
|    approx_kl            | 0.4088278 |
|    clip_fraction        | 0.597     |
|    clip_range           | 0.2       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.91      |
|    learning_rate        | 0.0005    |
|    loss                 | -1.14     |
|    n_updates            | 600       |
|    policy_gradient_loss | -0.00685  |
|    std                  | 1.86      |
|    value_loss           | 0.0603    |
---------------------------------------
Eval num_timesteps=2000000, episode_reward=1.54 +/- 0.45
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 1.54      |
| time/                   |           |
|    total_timesteps      | 2000000   |
| train/                  |           |
|    approx_kl            | 0.4334405 |
|    clip_fraction        | 0.596     |
|    clip_range           | 0.2       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.924     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.12     |
|    n_updates            | 610       |
|    policy_gradient_loss | -0.0115   |
|    std                  | 1.88      |
|    value_loss           | 0.0593    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.72     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 62       |
|    time_elapsed    | 9957     |
|    total_timesteps | 2031616  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.02       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 63         |
|    time_elapsed         | 10112      |
|    total_timesteps      | 2064384    |
| train/                  |            |
|    approx_kl            | 0.43777627 |
|    clip_fraction        | 0.592      |
|    clip_range           | 0.2        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.06      |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0121    |
|    std                  | 1.9        |
|    value_loss           | 0.0533     |
----------------------------------------
Eval num_timesteps=2080000, episode_reward=1.82 +/- 0.66
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 1.82      |
| time/                   |           |
|    total_timesteps      | 2080000   |
| train/                  |           |
|    approx_kl            | 0.4107068 |
|    clip_fraction        | 0.585     |
|    clip_range           | 0.2       |
|    entropy_loss         | -112      |
|    explained_variance   | 0.923     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.19     |
|    n_updates            | 630       |
|    policy_gradient_loss | -0.0062   |
|    std                  | 1.92      |
|    value_loss           | 0.0544    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.02     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 64       |
|    time_elapsed    | 10268    |
|    total_timesteps | 2097152  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.83       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 65         |
|    time_elapsed         | 10425      |
|    total_timesteps      | 2129920    |
| train/                  |            |
|    approx_kl            | 0.41201836 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -112       |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0005     |
|    loss                 | -1.13      |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.00649   |
|    std                  | 1.94       |
|    value_loss           | 0.0572     |
----------------------------------------
Eval num_timesteps=2160000, episode_reward=1.88 +/- 0.56
Episode length: 288.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 288      |
|    mean_reward          | 1.88     |
| time/                   |          |
|    total_timesteps      | 2160000  |
| train/                  |          |
|    approx_kl            | 0.418289 |
|    clip_fraction        | 0.578    |
|    clip_range           | 0.2      |
|    entropy_loss         | -113     |
|    explained_variance   | 0.921    |
|    learning_rate        | 0.0005   |
|    loss                 | -1.21    |
|    n_updates            | 650      |
|    policy_gradient_loss | -0.00384 |
|    std                  | 1.96     |
|    value_loss           | 0.0526   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.58     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 66       |
|    time_elapsed    | 10590    |
|    total_timesteps | 2162688  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.5        |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 67         |
|    time_elapsed         | 10753      |
|    total_timesteps      | 2195456    |
| train/                  |            |
|    approx_kl            | 0.43952703 |
|    clip_fraction        | 0.577      |
|    clip_range           | 0.2        |
|    entropy_loss         | -113       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.17      |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0107    |
|    std                  | 1.98       |
|    value_loss           | 0.0574     |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 288      |
|    ep_rew_mean          | 5.78     |
| time/                   |          |
|    fps                  | 204      |
|    iterations           | 68       |
|    time_elapsed         | 10916    |
|    total_timesteps      | 2228224  |
| train/                  |          |
|    approx_kl            | 0.422931 |
|    clip_fraction        | 0.594    |
|    clip_range           | 0.2      |
|    entropy_loss         | -114     |
|    explained_variance   | 0.923    |
|    learning_rate        | 0.0005   |
|    loss                 | -1.18    |
|    n_updates            | 670      |
|    policy_gradient_loss | -0.00862 |
|    std                  | 2        |
|    value_loss           | 0.0537   |
--------------------------------------
Eval num_timesteps=2240000, episode_reward=2.45 +/- 0.72
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.45       |
| time/                   |            |
|    total_timesteps      | 2240000    |
| train/                  |            |
|    approx_kl            | 0.41876537 |
|    clip_fraction        | 0.582      |
|    clip_range           | 0.2        |
|    entropy_loss         | -115       |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.17      |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.0116    |
|    std                  | 2.03       |
|    value_loss           | 0.0565     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.72     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 69       |
|    time_elapsed    | 11080    |
|    total_timesteps | 2260992  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.89       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 70         |
|    time_elapsed         | 11231      |
|    total_timesteps      | 2293760    |
| train/                  |            |
|    approx_kl            | 0.41483253 |
|    clip_fraction        | 0.583      |
|    clip_range           | 0.2        |
|    entropy_loss         | -115       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.15      |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.00943   |
|    std                  | 2.04       |
|    value_loss           | 0.0621     |
----------------------------------------
Eval num_timesteps=2320000, episode_reward=2.38 +/- 1.13
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.38       |
| time/                   |            |
|    total_timesteps      | 2320000    |
| train/                  |            |
|    approx_kl            | 0.41948444 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.2        |
|    entropy_loss         | -116       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.19      |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.00671   |
|    std                  | 2.07       |
|    value_loss           | 0.0593     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.15     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 71       |
|    time_elapsed    | 11391    |
|    total_timesteps | 2326528  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.39      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 72        |
|    time_elapsed         | 11553     |
|    total_timesteps      | 2359296   |
| train/                  |           |
|    approx_kl            | 0.4257142 |
|    clip_fraction        | 0.586     |
|    clip_range           | 0.2       |
|    entropy_loss         | -116      |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.16     |
|    n_updates            | 710       |
|    policy_gradient_loss | -0.0121   |
|    std                  | 2.09      |
|    value_loss           | 0.0594    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.69       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 73         |
|    time_elapsed         | 11712      |
|    total_timesteps      | 2392064    |
| train/                  |            |
|    approx_kl            | 0.40836832 |
|    clip_fraction        | 0.589      |
|    clip_range           | 0.2        |
|    entropy_loss         | -117       |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.27      |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.00733   |
|    std                  | 2.11       |
|    value_loss           | 0.0646     |
----------------------------------------
Eval num_timesteps=2400000, episode_reward=3.24 +/- 0.83
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 3.24       |
| time/                   |            |
|    total_timesteps      | 2400000    |
| train/                  |            |
|    approx_kl            | 0.43234015 |
|    clip_fraction        | 0.577      |
|    clip_range           | 0.2        |
|    entropy_loss         | -117       |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.19      |
|    n_updates            | 730        |
|    policy_gradient_loss | -0.00749   |
|    std                  | 2.14       |
|    value_loss           | 0.0602     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.22     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 74       |
|    time_elapsed    | 11879    |
|    total_timesteps | 2424832  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.21       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 75         |
|    time_elapsed         | 12040      |
|    total_timesteps      | 2457600    |
| train/                  |            |
|    approx_kl            | 0.41584763 |
|    clip_fraction        | 0.572      |
|    clip_range           | 0.2        |
|    entropy_loss         | -118       |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.17      |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0113    |
|    std                  | 2.16       |
|    value_loss           | 0.0503     |
----------------------------------------
Eval num_timesteps=2480000, episode_reward=2.41 +/- 0.81
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.41       |
| time/                   |            |
|    total_timesteps      | 2480000    |
| train/                  |            |
|    approx_kl            | 0.40275598 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.2        |
|    entropy_loss         | -119       |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.23      |
|    n_updates            | 750        |
|    policy_gradient_loss | -0.00585   |
|    std                  | 2.19       |
|    value_loss           | 0.0589     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.51     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 76       |
|    time_elapsed    | 12201    |
|    total_timesteps | 2490368  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.85       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 77         |
|    time_elapsed         | 12361      |
|    total_timesteps      | 2523136    |
| train/                  |            |
|    approx_kl            | 0.41423935 |
|    clip_fraction        | 0.587      |
|    clip_range           | 0.2        |
|    entropy_loss         | -119       |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.29      |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.00987   |
|    std                  | 2.21       |
|    value_loss           | 0.0593     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.75       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 78         |
|    time_elapsed         | 12522      |
|    total_timesteps      | 2555904    |
| train/                  |            |
|    approx_kl            | 0.41190878 |
|    clip_fraction        | 0.596      |
|    clip_range           | 0.2        |
|    entropy_loss         | -120       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.22      |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.00882   |
|    std                  | 2.23       |
|    value_loss           | 0.0621     |
----------------------------------------
Eval num_timesteps=2560000, episode_reward=1.66 +/- 0.95
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.66       |
| time/                   |            |
|    total_timesteps      | 2560000    |
| train/                  |            |
|    approx_kl            | 0.40732527 |
|    clip_fraction        | 0.582      |
|    clip_range           | 0.2        |
|    entropy_loss         | -120       |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.22      |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.00681   |
|    std                  | 2.25       |
|    value_loss           | 0.0579     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.33     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 79       |
|    time_elapsed    | 12685    |
|    total_timesteps | 2588672  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.32      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 80        |
|    time_elapsed         | 12844     |
|    total_timesteps      | 2621440   |
| train/                  |           |
|    approx_kl            | 0.4246701 |
|    clip_fraction        | 0.603     |
|    clip_range           | 0.2       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.92      |
|    learning_rate        | 0.0005    |
|    loss                 | -1.21     |
|    n_updates            | 790       |
|    policy_gradient_loss | -0.012    |
|    std                  | 2.28      |
|    value_loss           | 0.0579    |
---------------------------------------
Eval num_timesteps=2640000, episode_reward=2.14 +/- 0.63
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.14       |
| time/                   |            |
|    total_timesteps      | 2640000    |
| train/                  |            |
|    approx_kl            | 0.40650368 |
|    clip_fraction        | 0.589      |
|    clip_range           | 0.2        |
|    entropy_loss         | -122       |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.25      |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.00916   |
|    std                  | 2.31       |
|    value_loss           | 0.0613     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.37     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 81       |
|    time_elapsed    | 13009    |
|    total_timesteps | 2654208  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.01       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 82         |
|    time_elapsed         | 13165      |
|    total_timesteps      | 2686976    |
| train/                  |            |
|    approx_kl            | 0.41358888 |
|    clip_fraction        | 0.583      |
|    clip_range           | 0.2        |
|    entropy_loss         | -122       |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.21      |
|    n_updates            | 810        |
|    policy_gradient_loss | -0.00737   |
|    std                  | 2.33       |
|    value_loss           | 0.0615     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.29       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 83         |
|    time_elapsed         | 13328      |
|    total_timesteps      | 2719744    |
| train/                  |            |
|    approx_kl            | 0.44407013 |
|    clip_fraction        | 0.584      |
|    clip_range           | 0.2        |
|    entropy_loss         | -123       |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.36      |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0163    |
|    std                  | 2.36       |
|    value_loss           | 0.0612     |
----------------------------------------
Eval num_timesteps=2720000, episode_reward=1.87 +/- 0.32
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.87       |
| time/                   |            |
|    total_timesteps      | 2720000    |
| train/                  |            |
|    approx_kl            | 0.45130143 |
|    clip_fraction        | 0.597      |
|    clip_range           | 0.2        |
|    entropy_loss         | -123       |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.28      |
|    n_updates            | 830        |
|    policy_gradient_loss | -0.00695   |
|    std                  | 2.39       |
|    value_loss           | 0.0529     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.11     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 84       |
|    time_elapsed    | 13492    |
|    total_timesteps | 2752512  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.99       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 85         |
|    time_elapsed         | 13654      |
|    total_timesteps      | 2785280    |
| train/                  |            |
|    approx_kl            | 0.40183333 |
|    clip_fraction        | 0.572      |
|    clip_range           | 0.2        |
|    entropy_loss         | -124       |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.36      |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.00874   |
|    std                  | 2.41       |
|    value_loss           | 0.0544     |
----------------------------------------
Eval num_timesteps=2800000, episode_reward=2.76 +/- 1.12
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.76       |
| time/                   |            |
|    total_timesteps      | 2800000    |
| train/                  |            |
|    approx_kl            | 0.42115015 |
|    clip_fraction        | 0.587      |
|    clip_range           | 0.2        |
|    entropy_loss         | -125       |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.28      |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.00714   |
|    std                  | 2.44       |
|    value_loss           | 0.0635     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.46     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 86       |
|    time_elapsed    | 13819    |
|    total_timesteps | 2818048  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.05      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 87        |
|    time_elapsed         | 13974     |
|    total_timesteps      | 2850816   |
| train/                  |           |
|    approx_kl            | 0.4052621 |
|    clip_fraction        | 0.598     |
|    clip_range           | 0.2       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.93      |
|    learning_rate        | 0.0005    |
|    loss                 | -1.23     |
|    n_updates            | 860       |
|    policy_gradient_loss | -0.0099   |
|    std                  | 2.47      |
|    value_loss           | 0.0556    |
---------------------------------------
Eval num_timesteps=2880000, episode_reward=1.83 +/- 0.77
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.83       |
| time/                   |            |
|    total_timesteps      | 2880000    |
| train/                  |            |
|    approx_kl            | 0.42664203 |
|    clip_fraction        | 0.584      |
|    clip_range           | 0.2        |
|    entropy_loss         | -126       |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.31      |
|    n_updates            | 870        |
|    policy_gradient_loss | -0.0139    |
|    std                  | 2.49       |
|    value_loss           | 0.0554     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.05     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 88       |
|    time_elapsed    | 14131    |
|    total_timesteps | 2883584  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.83       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 89         |
|    time_elapsed         | 14284      |
|    total_timesteps      | 2916352    |
| train/                  |            |
|    approx_kl            | 0.42170203 |
|    clip_fraction        | 0.594      |
|    clip_range           | 0.2        |
|    entropy_loss         | -126       |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.22      |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.0122    |
|    std                  | 2.52       |
|    value_loss           | 0.0554     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.99      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 90        |
|    time_elapsed         | 14445     |
|    total_timesteps      | 2949120   |
| train/                  |           |
|    approx_kl            | 0.4277166 |
|    clip_fraction        | 0.583     |
|    clip_range           | 0.2       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.921     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.3      |
|    n_updates            | 890       |
|    policy_gradient_loss | -0.0105   |
|    std                  | 2.55      |
|    value_loss           | 0.0556    |
---------------------------------------
Eval num_timesteps=2960000, episode_reward=2.60 +/- 0.93
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.6        |
| time/                   |            |
|    total_timesteps      | 2960000    |
| train/                  |            |
|    approx_kl            | 0.39459768 |
|    clip_fraction        | 0.586      |
|    clip_range           | 0.2        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0005     |
|    loss                 | -1.34      |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.00928   |
|    std                  | 2.58       |
|    value_loss           | 0.0583     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.51     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 91       |
|    time_elapsed    | 14609    |
|    total_timesteps | 2981888  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 288      |
|    ep_rew_mean          | 5.82     |
| time/                   |          |
|    fps                  | 204      |
|    iterations           | 92       |
|    time_elapsed         | 14769    |
|    total_timesteps      | 3014656  |
| train/                  |          |
|    approx_kl            | 0.381912 |
|    clip_fraction        | 0.571    |
|    clip_range           | 0.2      |
|    entropy_loss         | -128     |
|    explained_variance   | 0.922    |
|    learning_rate        | 0.0005   |
|    loss                 | -1.35    |
|    n_updates            | 910      |
|    policy_gradient_loss | -0.0107  |
|    std                  | 2.61     |
|    value_loss           | 0.0589   |
--------------------------------------
Eval num_timesteps=3040000, episode_reward=3.04 +/- 1.48
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 3.04       |
| time/                   |            |
|    total_timesteps      | 3040000    |
| train/                  |            |
|    approx_kl            | 0.39148206 |
|    clip_fraction        | 0.584      |
|    clip_range           | 0.2        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.32      |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.00929   |
|    std                  | 2.64       |
|    value_loss           | 0.0547     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.61     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 93       |
|    time_elapsed    | 14927    |
|    total_timesteps | 3047424  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.97       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 94         |
|    time_elapsed         | 15080      |
|    total_timesteps      | 3080192    |
| train/                  |            |
|    approx_kl            | 0.40002176 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.2        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.32      |
|    n_updates            | 930        |
|    policy_gradient_loss | -0.0152    |
|    std                  | 2.66       |
|    value_loss           | 0.0579     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.65       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 95         |
|    time_elapsed         | 15234      |
|    total_timesteps      | 3112960    |
| train/                  |            |
|    approx_kl            | 0.41075152 |
|    clip_fraction        | 0.588      |
|    clip_range           | 0.2        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.31      |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.0098    |
|    std                  | 2.69       |
|    value_loss           | 0.0612     |
----------------------------------------
Eval num_timesteps=3120000, episode_reward=2.39 +/- 0.88
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.39       |
| time/                   |            |
|    total_timesteps      | 3120000    |
| train/                  |            |
|    approx_kl            | 0.39421695 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.2        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.33      |
|    n_updates            | 950        |
|    policy_gradient_loss | -0.00703   |
|    std                  | 2.73       |
|    value_loss           | 0.0655     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.76     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 96       |
|    time_elapsed    | 15396    |
|    total_timesteps | 3145728  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.01       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 97         |
|    time_elapsed         | 15558      |
|    total_timesteps      | 3178496    |
| train/                  |            |
|    approx_kl            | 0.39689782 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.2        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.33      |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.00952   |
|    std                  | 2.76       |
|    value_loss           | 0.0634     |
----------------------------------------
Eval num_timesteps=3200000, episode_reward=2.52 +/- 1.63
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.52       |
| time/                   |            |
|    total_timesteps      | 3200000    |
| train/                  |            |
|    approx_kl            | 0.40079242 |
|    clip_fraction        | 0.591      |
|    clip_range           | 0.2        |
|    entropy_loss         | -132       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.37      |
|    n_updates            | 970        |
|    policy_gradient_loss | -0.0103    |
|    std                  | 2.79       |
|    value_loss           | 0.0625     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.39     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 98       |
|    time_elapsed    | 15723    |
|    total_timesteps | 3211264  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.42      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 99        |
|    time_elapsed         | 15886     |
|    total_timesteps      | 3244032   |
| train/                  |           |
|    approx_kl            | 0.3932702 |
|    clip_fraction        | 0.591     |
|    clip_range           | 0.2       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.918     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.34     |
|    n_updates            | 980       |
|    policy_gradient_loss | -0.0123   |
|    std                  | 2.83      |
|    value_loss           | 0.0673    |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.85      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 100       |
|    time_elapsed         | 16047     |
|    total_timesteps      | 3276800   |
| train/                  |           |
|    approx_kl            | 0.3839417 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.2       |
|    entropy_loss         | -133      |
|    explained_variance   | 0.924     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.4      |
|    n_updates            | 990       |
|    policy_gradient_loss | -0.00981  |
|    std                  | 2.86      |
|    value_loss           | 0.0655    |
---------------------------------------
Eval num_timesteps=3280000, episode_reward=2.11 +/- 1.09
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.11       |
| time/                   |            |
|    total_timesteps      | 3280000    |
| train/                  |            |
|    approx_kl            | 0.39794448 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.2        |
|    entropy_loss         | -134       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.27      |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.0203    |
|    std                  | 2.89       |
|    value_loss           | 0.0635     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.97     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 101      |
|    time_elapsed    | 16211    |
|    total_timesteps | 3309568  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.54       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 102        |
|    time_elapsed         | 16373      |
|    total_timesteps      | 3342336    |
| train/                  |            |
|    approx_kl            | 0.39986554 |
|    clip_fraction        | 0.583      |
|    clip_range           | 0.2        |
|    entropy_loss         | -134       |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.39      |
|    n_updates            | 1010       |
|    policy_gradient_loss | -0.0124    |
|    std                  | 2.92       |
|    value_loss           | 0.0634     |
----------------------------------------
Eval num_timesteps=3360000, episode_reward=2.18 +/- 0.89
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.18       |
| time/                   |            |
|    total_timesteps      | 3360000    |
| train/                  |            |
|    approx_kl            | 0.40688348 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.2        |
|    entropy_loss         | -135       |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.33      |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.0139    |
|    std                  | 2.95       |
|    value_loss           | 0.0599     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.35     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 103      |
|    time_elapsed    | 16540    |
|    total_timesteps | 3375104  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.93      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 104       |
|    time_elapsed         | 16699     |
|    total_timesteps      | 3407872   |
| train/                  |           |
|    approx_kl            | 0.4219799 |
|    clip_fraction        | 0.597     |
|    clip_range           | 0.2       |
|    entropy_loss         | -136      |
|    explained_variance   | 0.926     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.44     |
|    n_updates            | 1030      |
|    policy_gradient_loss | -0.0151   |
|    std                  | 2.99      |
|    value_loss           | 0.0663    |
---------------------------------------
Eval num_timesteps=3440000, episode_reward=1.94 +/- 0.29
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.94       |
| time/                   |            |
|    total_timesteps      | 3440000    |
| train/                  |            |
|    approx_kl            | 0.38254473 |
|    clip_fraction        | 0.588      |
|    clip_range           | 0.2        |
|    entropy_loss         | -136       |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.41      |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.00941   |
|    std                  | 3.02       |
|    value_loss           | 0.0656     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.68     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 105      |
|    time_elapsed    | 16866    |
|    total_timesteps | 3440640  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.39       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 106        |
|    time_elapsed         | 17028      |
|    total_timesteps      | 3473408    |
| train/                  |            |
|    approx_kl            | 0.39586276 |
|    clip_fraction        | 0.593      |
|    clip_range           | 0.2        |
|    entropy_loss         | -137       |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.33      |
|    n_updates            | 1050       |
|    policy_gradient_loss | -0.0124    |
|    std                  | 3.05       |
|    value_loss           | 0.0588     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.98       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 107        |
|    time_elapsed         | 17191      |
|    total_timesteps      | 3506176    |
| train/                  |            |
|    approx_kl            | 0.38810682 |
|    clip_fraction        | 0.578      |
|    clip_range           | 0.2        |
|    entropy_loss         | -137       |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.27      |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0163    |
|    std                  | 3.08       |
|    value_loss           | 0.0519     |
----------------------------------------
Eval num_timesteps=3520000, episode_reward=1.99 +/- 0.61
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.99       |
| time/                   |            |
|    total_timesteps      | 3520000    |
| train/                  |            |
|    approx_kl            | 0.38870454 |
|    clip_fraction        | 0.582      |
|    clip_range           | 0.2        |
|    entropy_loss         | -138       |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.46      |
|    n_updates            | 1070       |
|    policy_gradient_loss | -0.015     |
|    std                  | 3.12       |
|    value_loss           | 0.0611     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.82     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 108      |
|    time_elapsed    | 17358    |
|    total_timesteps | 3538944  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.83       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 109        |
|    time_elapsed         | 17519      |
|    total_timesteps      | 3571712    |
| train/                  |            |
|    approx_kl            | 0.36448509 |
|    clip_fraction        | 0.571      |
|    clip_range           | 0.2        |
|    entropy_loss         | -138       |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.36      |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.0101    |
|    std                  | 3.15       |
|    value_loss           | 0.0546     |
----------------------------------------
Eval num_timesteps=3600000, episode_reward=2.43 +/- 0.54
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 2.43      |
| time/                   |           |
|    total_timesteps      | 3600000   |
| train/                  |           |
|    approx_kl            | 0.3802723 |
|    clip_fraction        | 0.571     |
|    clip_range           | 0.2       |
|    entropy_loss         | -139      |
|    explained_variance   | 0.924     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.31     |
|    n_updates            | 1090      |
|    policy_gradient_loss | -0.0184   |
|    std                  | 3.18      |
|    value_loss           | 0.0585    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.74     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 110      |
|    time_elapsed    | 17681    |
|    total_timesteps | 3604480  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.14       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 111        |
|    time_elapsed         | 17841      |
|    total_timesteps      | 3637248    |
| train/                  |            |
|    approx_kl            | 0.37638605 |
|    clip_fraction        | 0.575      |
|    clip_range           | 0.2        |
|    entropy_loss         | -139       |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.43      |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.00675   |
|    std                  | 3.21       |
|    value_loss           | 0.0569     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.26       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 112        |
|    time_elapsed         | 18001      |
|    total_timesteps      | 3670016    |
| train/                  |            |
|    approx_kl            | 0.40133166 |
|    clip_fraction        | 0.585      |
|    clip_range           | 0.2        |
|    entropy_loss         | -140       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.4       |
|    n_updates            | 1110       |
|    policy_gradient_loss | -0.0139    |
|    std                  | 3.25       |
|    value_loss           | 0.0666     |
----------------------------------------
Eval num_timesteps=3680000, episode_reward=3.14 +/- 0.85
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 3.14      |
| time/                   |           |
|    total_timesteps      | 3680000   |
| train/                  |           |
|    approx_kl            | 0.3688197 |
|    clip_fraction        | 0.593     |
|    clip_range           | 0.2       |
|    entropy_loss         | -141      |
|    explained_variance   | 0.924     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.43     |
|    n_updates            | 1120      |
|    policy_gradient_loss | -0.0104   |
|    std                  | 3.29      |
|    value_loss           | 0.0629    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.05     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 113      |
|    time_elapsed    | 18167    |
|    total_timesteps | 3702784  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.38      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 114       |
|    time_elapsed         | 18327     |
|    total_timesteps      | 3735552   |
| train/                  |           |
|    approx_kl            | 0.3675205 |
|    clip_fraction        | 0.578     |
|    clip_range           | 0.2       |
|    entropy_loss         | -141      |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.47     |
|    n_updates            | 1130      |
|    policy_gradient_loss | -0.0114   |
|    std                  | 3.33      |
|    value_loss           | 0.066     |
---------------------------------------
Eval num_timesteps=3760000, episode_reward=3.72 +/- 1.13
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 3.72       |
| time/                   |            |
|    total_timesteps      | 3760000    |
| train/                  |            |
|    approx_kl            | 0.38270345 |
|    clip_fraction        | 0.589      |
|    clip_range           | 0.2        |
|    entropy_loss         | -142       |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.46      |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.016     |
|    std                  | 3.37       |
|    value_loss           | 0.057      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6        |
| time/              |          |
|    fps             | 203      |
|    iterations      | 115      |
|    time_elapsed    | 18492    |
|    total_timesteps | 3768320  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.89      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 116       |
|    time_elapsed         | 18648     |
|    total_timesteps      | 3801088   |
| train/                  |           |
|    approx_kl            | 0.3962373 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.2       |
|    entropy_loss         | -143      |
|    explained_variance   | 0.926     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.44     |
|    n_updates            | 1150      |
|    policy_gradient_loss | -0.0112   |
|    std                  | 3.4       |
|    value_loss           | 0.0651    |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.09      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 117       |
|    time_elapsed         | 18803     |
|    total_timesteps      | 3833856   |
| train/                  |           |
|    approx_kl            | 0.3691197 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.2       |
|    entropy_loss         | -143      |
|    explained_variance   | 0.92      |
|    learning_rate        | 0.0005    |
|    loss                 | -1.44     |
|    n_updates            | 1160      |
|    policy_gradient_loss | -0.0166   |
|    std                  | 3.44      |
|    value_loss           | 0.0719    |
---------------------------------------
Eval num_timesteps=3840000, episode_reward=2.14 +/- 0.82
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.14       |
| time/                   |            |
|    total_timesteps      | 3840000    |
| train/                  |            |
|    approx_kl            | 0.36189687 |
|    clip_fraction        | 0.582      |
|    clip_range           | 0.2        |
|    entropy_loss         | -144       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.53      |
|    n_updates            | 1170       |
|    policy_gradient_loss | -0.0281    |
|    std                  | 3.48       |
|    value_loss           | 0.0696     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.4      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 118      |
|    time_elapsed    | 18961    |
|    total_timesteps | 3866624  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.24       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 119        |
|    time_elapsed         | 19124      |
|    total_timesteps      | 3899392    |
| train/                  |            |
|    approx_kl            | 0.37197173 |
|    clip_fraction        | 0.571      |
|    clip_range           | 0.2        |
|    entropy_loss         | -144       |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.42      |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.0153    |
|    std                  | 3.52       |
|    value_loss           | 0.0693     |
----------------------------------------
Eval num_timesteps=3920000, episode_reward=2.67 +/- 0.69
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.67       |
| time/                   |            |
|    total_timesteps      | 3920000    |
| train/                  |            |
|    approx_kl            | 0.37925893 |
|    clip_fraction        | 0.573      |
|    clip_range           | 0.2        |
|    entropy_loss         | -145       |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0005     |
|    loss                 | -1.53      |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.0114    |
|    std                  | 3.55       |
|    value_loss           | 0.0723     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.3      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 120      |
|    time_elapsed    | 19289    |
|    total_timesteps | 3932160  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.97       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 121        |
|    time_elapsed         | 19451      |
|    total_timesteps      | 3964928    |
| train/                  |            |
|    approx_kl            | 0.36399886 |
|    clip_fraction        | 0.589      |
|    clip_range           | 0.2        |
|    entropy_loss         | -145       |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.48      |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.0169    |
|    std                  | 3.6        |
|    value_loss           | 0.0641     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.51      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 122       |
|    time_elapsed         | 19611     |
|    total_timesteps      | 3997696   |
| train/                  |           |
|    approx_kl            | 0.3454105 |
|    clip_fraction        | 0.576     |
|    clip_range           | 0.2       |
|    entropy_loss         | -146      |
|    explained_variance   | 0.923     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.51     |
|    n_updates            | 1210      |
|    policy_gradient_loss | -0.0167   |
|    std                  | 3.63      |
|    value_loss           | 0.0693    |
---------------------------------------
Eval num_timesteps=4000000, episode_reward=2.92 +/- 0.50
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.92       |
| time/                   |            |
|    total_timesteps      | 4000000    |
| train/                  |            |
|    approx_kl            | 0.37304822 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.2        |
|    entropy_loss         | -147       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.57      |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.0206    |
|    std                  | 3.67       |
|    value_loss           | 0.0643     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.81     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 123      |
|    time_elapsed    | 19777    |
|    total_timesteps | 4030464  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.88      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 124       |
|    time_elapsed         | 19939     |
|    total_timesteps      | 4063232   |
| train/                  |           |
|    approx_kl            | 0.3506826 |
|    clip_fraction        | 0.577     |
|    clip_range           | 0.2       |
|    entropy_loss         | -147      |
|    explained_variance   | 0.92      |
|    learning_rate        | 0.0005    |
|    loss                 | -1.48     |
|    n_updates            | 1230      |
|    policy_gradient_loss | -0.0116   |
|    std                  | 3.72      |
|    value_loss           | 0.0662    |
---------------------------------------
Eval num_timesteps=4080000, episode_reward=3.68 +/- 1.06
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 3.68      |
| time/                   |           |
|    total_timesteps      | 4080000   |
| train/                  |           |
|    approx_kl            | 0.3593956 |
|    clip_fraction        | 0.573     |
|    clip_range           | 0.2       |
|    entropy_loss         | -148      |
|    explained_variance   | 0.924     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.5      |
|    n_updates            | 1240      |
|    policy_gradient_loss | -0.0164   |
|    std                  | 3.75      |
|    value_loss           | 0.0672    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.76     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 125      |
|    time_elapsed    | 20106    |
|    total_timesteps | 4096000  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.81       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 126        |
|    time_elapsed         | 20269      |
|    total_timesteps      | 4128768    |
| train/                  |            |
|    approx_kl            | 0.34649616 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -148       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.55      |
|    n_updates            | 1250       |
|    policy_gradient_loss | -0.0122    |
|    std                  | 3.8        |
|    value_loss           | 0.0753     |
----------------------------------------
Eval num_timesteps=4160000, episode_reward=3.43 +/- 1.13
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 3.43      |
| time/                   |           |
|    total_timesteps      | 4160000   |
| train/                  |           |
|    approx_kl            | 0.3380592 |
|    clip_fraction        | 0.566     |
|    clip_range           | 0.2       |
|    entropy_loss         | -149      |
|    explained_variance   | 0.918     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.52     |
|    n_updates            | 1260      |
|    policy_gradient_loss | -0.0146   |
|    std                  | 3.83      |
|    value_loss           | 0.0663    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.69     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 127      |
|    time_elapsed    | 20431    |
|    total_timesteps | 4161536  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.93       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 128        |
|    time_elapsed         | 20589      |
|    total_timesteps      | 4194304    |
| train/                  |            |
|    approx_kl            | 0.33561495 |
|    clip_fraction        | 0.565      |
|    clip_range           | 0.2        |
|    entropy_loss         | -150       |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.55      |
|    n_updates            | 1270       |
|    policy_gradient_loss | -0.0151    |
|    std                  | 3.88       |
|    value_loss           | 0.0673     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.79      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 129       |
|    time_elapsed         | 20742     |
|    total_timesteps      | 4227072   |
| train/                  |           |
|    approx_kl            | 0.3314036 |
|    clip_fraction        | 0.569     |
|    clip_range           | 0.2       |
|    entropy_loss         | -150      |
|    explained_variance   | 0.91      |
|    learning_rate        | 0.0005    |
|    loss                 | -1.54     |
|    n_updates            | 1280      |
|    policy_gradient_loss | -0.0142   |
|    std                  | 3.92      |
|    value_loss           | 0.0703    |
---------------------------------------
Eval num_timesteps=4240000, episode_reward=2.08 +/- 1.31
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.08       |
| time/                   |            |
|    total_timesteps      | 4240000    |
| train/                  |            |
|    approx_kl            | 0.33614993 |
|    clip_fraction        | 0.563      |
|    clip_range           | 0.2        |
|    entropy_loss         | -151       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.49      |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.0159    |
|    std                  | 3.96       |
|    value_loss           | 0.069      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.01     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 130      |
|    time_elapsed    | 20901    |
|    total_timesteps | 4259840  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.57      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 131       |
|    time_elapsed         | 21068     |
|    total_timesteps      | 4292608   |
| train/                  |           |
|    approx_kl            | 0.3274275 |
|    clip_fraction        | 0.581     |
|    clip_range           | 0.2       |
|    entropy_loss         | -151      |
|    explained_variance   | 0.918     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.58     |
|    n_updates            | 1300      |
|    policy_gradient_loss | -0.0163   |
|    std                  | 4         |
|    value_loss           | 0.0647    |
---------------------------------------
Eval num_timesteps=4320000, episode_reward=2.75 +/- 0.85
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 2.75      |
| time/                   |           |
|    total_timesteps      | 4320000   |
| train/                  |           |
|    approx_kl            | 0.3268641 |
|    clip_fraction        | 0.575     |
|    clip_range           | 0.2       |
|    entropy_loss         | -152      |
|    explained_variance   | 0.917     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.51     |
|    n_updates            | 1310      |
|    policy_gradient_loss | -0.0165   |
|    std                  | 4.05      |
|    value_loss           | 0.0656    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.63     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 132      |
|    time_elapsed    | 21235    |
|    total_timesteps | 4325376  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.87       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 133        |
|    time_elapsed         | 21398      |
|    total_timesteps      | 4358144    |
| train/                  |            |
|    approx_kl            | 0.33240598 |
|    clip_fraction        | 0.572      |
|    clip_range           | 0.2        |
|    entropy_loss         | -152       |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.58      |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0157    |
|    std                  | 4.09       |
|    value_loss           | 0.0676     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.36       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 134        |
|    time_elapsed         | 21560      |
|    total_timesteps      | 4390912    |
| train/                  |            |
|    approx_kl            | 0.32983467 |
|    clip_fraction        | 0.567      |
|    clip_range           | 0.2        |
|    entropy_loss         | -153       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.63      |
|    n_updates            | 1330       |
|    policy_gradient_loss | -0.0163    |
|    std                  | 4.14       |
|    value_loss           | 0.0652     |
----------------------------------------
Eval num_timesteps=4400000, episode_reward=2.66 +/- 1.35
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.66       |
| time/                   |            |
|    total_timesteps      | 4400000    |
| train/                  |            |
|    approx_kl            | 0.33603978 |
|    clip_fraction        | 0.576      |
|    clip_range           | 0.2        |
|    entropy_loss         | -154       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.56      |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.0198    |
|    std                  | 4.18       |
|    value_loss           | 0.0697     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.75     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 135      |
|    time_elapsed    | 21727    |
|    total_timesteps | 4423680  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.74       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 136        |
|    time_elapsed         | 21888      |
|    total_timesteps      | 4456448    |
| train/                  |            |
|    approx_kl            | 0.32877123 |
|    clip_fraction        | 0.566      |
|    clip_range           | 0.2        |
|    entropy_loss         | -154       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.63      |
|    n_updates            | 1350       |
|    policy_gradient_loss | -0.0197    |
|    std                  | 4.23       |
|    value_loss           | 0.0714     |
----------------------------------------
Eval num_timesteps=4480000, episode_reward=2.71 +/- 0.23
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.71       |
| time/                   |            |
|    total_timesteps      | 4480000    |
| train/                  |            |
|    approx_kl            | 0.33465385 |
|    clip_fraction        | 0.557      |
|    clip_range           | 0.2        |
|    entropy_loss         | -155       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.57      |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.0178    |
|    std                  | 4.26       |
|    value_loss           | 0.0679     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.44     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 137      |
|    time_elapsed    | 22059    |
|    total_timesteps | 4489216  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 288      |
|    ep_rew_mean          | 6.28     |
| time/                   |          |
|    fps                  | 203      |
|    iterations           | 138      |
|    time_elapsed         | 22222    |
|    total_timesteps      | 4521984  |
| train/                  |          |
|    approx_kl            | 0.338763 |
|    clip_fraction        | 0.567    |
|    clip_range           | 0.2      |
|    entropy_loss         | -155     |
|    explained_variance   | 0.919    |
|    learning_rate        | 0.0005   |
|    loss                 | -1.57    |
|    n_updates            | 1370     |
|    policy_gradient_loss | -0.0139  |
|    std                  | 4.31     |
|    value_loss           | 0.07     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.6        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 139        |
|    time_elapsed         | 22386      |
|    total_timesteps      | 4554752    |
| train/                  |            |
|    approx_kl            | 0.32421547 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -156       |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.57      |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0182    |
|    std                  | 4.35       |
|    value_loss           | 0.0701     |
----------------------------------------
Eval num_timesteps=4560000, episode_reward=2.57 +/- 1.03
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.57       |
| time/                   |            |
|    total_timesteps      | 4560000    |
| train/                  |            |
|    approx_kl            | 0.32660317 |
|    clip_fraction        | 0.568      |
|    clip_range           | 0.2        |
|    entropy_loss         | -156       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.56      |
|    n_updates            | 1390       |
|    policy_gradient_loss | -0.017     |
|    std                  | 4.41       |
|    value_loss           | 0.0666     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.02     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 140      |
|    time_elapsed    | 22550    |
|    total_timesteps | 4587520  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.77      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 141       |
|    time_elapsed         | 22713     |
|    total_timesteps      | 4620288   |
| train/                  |           |
|    approx_kl            | 0.3252939 |
|    clip_fraction        | 0.564     |
|    clip_range           | 0.2       |
|    entropy_loss         | -157      |
|    explained_variance   | 0.924     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.62     |
|    n_updates            | 1400      |
|    policy_gradient_loss | -0.0157   |
|    std                  | 4.45      |
|    value_loss           | 0.0705    |
---------------------------------------
Eval num_timesteps=4640000, episode_reward=2.78 +/- 0.62
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 2.78      |
| time/                   |           |
|    total_timesteps      | 4640000   |
| train/                  |           |
|    approx_kl            | 0.3428525 |
|    clip_fraction        | 0.567     |
|    clip_range           | 0.2       |
|    entropy_loss         | -158      |
|    explained_variance   | 0.911     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.54     |
|    n_updates            | 1410      |
|    policy_gradient_loss | -0.0204   |
|    std                  | 4.5       |
|    value_loss           | 0.0773    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.64     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 142      |
|    time_elapsed    | 22878    |
|    total_timesteps | 4653056  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.3        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 143        |
|    time_elapsed         | 23039      |
|    total_timesteps      | 4685824    |
| train/                  |            |
|    approx_kl            | 0.32643408 |
|    clip_fraction        | 0.565      |
|    clip_range           | 0.2        |
|    entropy_loss         | -158       |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.63      |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.0168    |
|    std                  | 4.55       |
|    value_loss           | 0.0709     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.73      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 144       |
|    time_elapsed         | 23202     |
|    total_timesteps      | 4718592   |
| train/                  |           |
|    approx_kl            | 0.3291731 |
|    clip_fraction        | 0.573     |
|    clip_range           | 0.2       |
|    entropy_loss         | -159      |
|    explained_variance   | 0.917     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.65     |
|    n_updates            | 1430      |
|    policy_gradient_loss | -0.016    |
|    std                  | 4.59      |
|    value_loss           | 0.073     |
---------------------------------------
Eval num_timesteps=4720000, episode_reward=2.31 +/- 0.60
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 2.31      |
| time/                   |           |
|    total_timesteps      | 4720000   |
| train/                  |           |
|    approx_kl            | 0.3133365 |
|    clip_fraction        | 0.554     |
|    clip_range           | 0.2       |
|    entropy_loss         | -159      |
|    explained_variance   | 0.914     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.68     |
|    n_updates            | 1440      |
|    policy_gradient_loss | -0.0193   |
|    std                  | 4.64      |
|    value_loss           | 0.0648    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.09     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 145      |
|    time_elapsed    | 23368    |
|    total_timesteps | 4751360  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.85       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 146        |
|    time_elapsed         | 23534      |
|    total_timesteps      | 4784128    |
| train/                  |            |
|    approx_kl            | 0.32857335 |
|    clip_fraction        | 0.564      |
|    clip_range           | 0.2        |
|    entropy_loss         | -160       |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.62      |
|    n_updates            | 1450       |
|    policy_gradient_loss | -0.0194    |
|    std                  | 4.68       |
|    value_loss           | 0.0621     |
----------------------------------------
Eval num_timesteps=4800000, episode_reward=2.52 +/- 0.87
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.52       |
| time/                   |            |
|    total_timesteps      | 4800000    |
| train/                  |            |
|    approx_kl            | 0.33183604 |
|    clip_fraction        | 0.559      |
|    clip_range           | 0.2        |
|    entropy_loss         | -160       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.73      |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.0226    |
|    std                  | 4.73       |
|    value_loss           | 0.0694     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.5      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 147      |
|    time_elapsed    | 23698    |
|    total_timesteps | 4816896  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.48       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 148        |
|    time_elapsed         | 23861      |
|    total_timesteps      | 4849664    |
| train/                  |            |
|    approx_kl            | 0.31437057 |
|    clip_fraction        | 0.573      |
|    clip_range           | 0.2        |
|    entropy_loss         | -161       |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.58      |
|    n_updates            | 1470       |
|    policy_gradient_loss | -0.0203    |
|    std                  | 4.78       |
|    value_loss           | 0.0754     |
----------------------------------------
Eval num_timesteps=4880000, episode_reward=2.04 +/- 0.64
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.04       |
| time/                   |            |
|    total_timesteps      | 4880000    |
| train/                  |            |
|    approx_kl            | 0.31246775 |
|    clip_fraction        | 0.57       |
|    clip_range           | 0.2        |
|    entropy_loss         | -161       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.6       |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.0168    |
|    std                  | 4.83       |
|    value_loss           | 0.0751     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.07     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 149      |
|    time_elapsed    | 24027    |
|    total_timesteps | 4882432  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.6       |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 150       |
|    time_elapsed         | 24189     |
|    total_timesteps      | 4915200   |
| train/                  |           |
|    approx_kl            | 0.3317973 |
|    clip_fraction        | 0.568     |
|    clip_range           | 0.2       |
|    entropy_loss         | -162      |
|    explained_variance   | 0.925     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.63     |
|    n_updates            | 1490      |
|    policy_gradient_loss | -0.0187   |
|    std                  | 4.88      |
|    value_loss           | 0.0707    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.79       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 151        |
|    time_elapsed         | 24352      |
|    total_timesteps      | 4947968    |
| train/                  |            |
|    approx_kl            | 0.32731646 |
|    clip_fraction        | 0.575      |
|    clip_range           | 0.2        |
|    entropy_loss         | -163       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.64      |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.0217    |
|    std                  | 4.94       |
|    value_loss           | 0.07       |
----------------------------------------
Eval num_timesteps=4960000, episode_reward=1.81 +/- 0.19
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.81       |
| time/                   |            |
|    total_timesteps      | 4960000    |
| train/                  |            |
|    approx_kl            | 0.31720978 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -163       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.61      |
|    n_updates            | 1510       |
|    policy_gradient_loss | -0.0172    |
|    std                  | 4.99       |
|    value_loss           | 0.0808     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.81     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 152      |
|    time_elapsed    | 24514    |
|    total_timesteps | 4980736  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.35       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 153        |
|    time_elapsed         | 24675      |
|    total_timesteps      | 5013504    |
| train/                  |            |
|    approx_kl            | 0.31393555 |
|    clip_fraction        | 0.564      |
|    clip_range           | 0.2        |
|    entropy_loss         | -164       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.65      |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.0165    |
|    std                  | 5.05       |
|    value_loss           | 0.0674     |
----------------------------------------
Eval num_timesteps=5040000, episode_reward=1.64 +/- 0.77
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.64       |
| time/                   |            |
|    total_timesteps      | 5040000    |
| train/                  |            |
|    approx_kl            | 0.30505723 |
|    clip_fraction        | 0.563      |
|    clip_range           | 0.2        |
|    entropy_loss         | -164       |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.66      |
|    n_updates            | 1530       |
|    policy_gradient_loss | -0.0167    |
|    std                  | 5.1        |
|    value_loss           | 0.0641     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.45     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 154      |
|    time_elapsed    | 24836    |
|    total_timesteps | 5046272  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.56      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 155       |
|    time_elapsed         | 25000     |
|    total_timesteps      | 5079040   |
| train/                  |           |
|    approx_kl            | 0.3093129 |
|    clip_fraction        | 0.56      |
|    clip_range           | 0.2       |
|    entropy_loss         | -165      |
|    explained_variance   | 0.927     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.74     |
|    n_updates            | 1540      |
|    policy_gradient_loss | -0.0185   |
|    std                  | 5.15      |
|    value_loss           | 0.0697    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.75       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 156        |
|    time_elapsed         | 25164      |
|    total_timesteps      | 5111808    |
| train/                  |            |
|    approx_kl            | 0.31422055 |
|    clip_fraction        | 0.559      |
|    clip_range           | 0.2        |
|    entropy_loss         | -165       |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.66      |
|    n_updates            | 1550       |
|    policy_gradient_loss | -0.0204    |
|    std                  | 5.21       |
|    value_loss           | 0.0676     |
----------------------------------------
Eval num_timesteps=5120000, episode_reward=2.32 +/- 1.59
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 2.32      |
| time/                   |           |
|    total_timesteps      | 5120000   |
| train/                  |           |
|    approx_kl            | 0.3076529 |
|    clip_fraction        | 0.568     |
|    clip_range           | 0.2       |
|    entropy_loss         | -166      |
|    explained_variance   | 0.919     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.63     |
|    n_updates            | 1560      |
|    policy_gradient_loss | -0.0192   |
|    std                  | 5.27      |
|    value_loss           | 0.0707    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.4      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 157      |
|    time_elapsed    | 25330    |
|    total_timesteps | 5144576  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.43       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 158        |
|    time_elapsed         | 25492      |
|    total_timesteps      | 5177344    |
| train/                  |            |
|    approx_kl            | 0.30381095 |
|    clip_fraction        | 0.562      |
|    clip_range           | 0.2        |
|    entropy_loss         | -167       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.76      |
|    n_updates            | 1570       |
|    policy_gradient_loss | -0.0183    |
|    std                  | 5.32       |
|    value_loss           | 0.0687     |
----------------------------------------
Eval num_timesteps=5200000, episode_reward=2.19 +/- 1.01
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.19       |
| time/                   |            |
|    total_timesteps      | 5200000    |
| train/                  |            |
|    approx_kl            | 0.30944744 |
|    clip_fraction        | 0.567      |
|    clip_range           | 0.2        |
|    entropy_loss         | -167       |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.75      |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.0176    |
|    std                  | 5.38       |
|    value_loss           | 0.0688     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.96     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 159      |
|    time_elapsed    | 25658    |
|    total_timesteps | 5210112  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.74      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 160       |
|    time_elapsed         | 25819     |
|    total_timesteps      | 5242880   |
| train/                  |           |
|    approx_kl            | 0.3060625 |
|    clip_fraction        | 0.573     |
|    clip_range           | 0.2       |
|    entropy_loss         | -168      |
|    explained_variance   | 0.913     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.74     |
|    n_updates            | 1590      |
|    policy_gradient_loss | -0.0233   |
|    std                  | 5.45      |
|    value_loss           | 0.0687    |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.84      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 161       |
|    time_elapsed         | 25980     |
|    total_timesteps      | 5275648   |
| train/                  |           |
|    approx_kl            | 0.2908008 |
|    clip_fraction        | 0.561     |
|    clip_range           | 0.2       |
|    entropy_loss         | -168      |
|    explained_variance   | 0.913     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.72     |
|    n_updates            | 1600      |
|    policy_gradient_loss | -0.0205   |
|    std                  | 5.51      |
|    value_loss           | 0.0655    |
---------------------------------------
Eval num_timesteps=5280000, episode_reward=2.16 +/- 1.22
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 2.16      |
| time/                   |           |
|    total_timesteps      | 5280000   |
| train/                  |           |
|    approx_kl            | 0.2913302 |
|    clip_fraction        | 0.556     |
|    clip_range           | 0.2       |
|    entropy_loss         | -169      |
|    explained_variance   | 0.919     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.72     |
|    n_updates            | 1610      |
|    policy_gradient_loss | -0.0184   |
|    std                  | 5.56      |
|    value_loss           | 0.062     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.74     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 162      |
|    time_elapsed    | 26146    |
|    total_timesteps | 5308416  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.38       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 163        |
|    time_elapsed         | 26306      |
|    total_timesteps      | 5341184    |
| train/                  |            |
|    approx_kl            | 0.31589243 |
|    clip_fraction        | 0.557      |
|    clip_range           | 0.2        |
|    entropy_loss         | -170       |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.8       |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.0222    |
|    std                  | 5.62       |
|    value_loss           | 0.0674     |
----------------------------------------
Eval num_timesteps=5360000, episode_reward=2.86 +/- 1.07
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.86       |
| time/                   |            |
|    total_timesteps      | 5360000    |
| train/                  |            |
|    approx_kl            | 0.29606926 |
|    clip_fraction        | 0.557      |
|    clip_range           | 0.2        |
|    entropy_loss         | -170       |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.66      |
|    n_updates            | 1630       |
|    policy_gradient_loss | -0.0197    |
|    std                  | 5.68       |
|    value_loss           | 0.0704     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.2      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 164      |
|    time_elapsed    | 26464    |
|    total_timesteps | 5373952  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.37      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 165       |
|    time_elapsed         | 26618     |
|    total_timesteps      | 5406720   |
| train/                  |           |
|    approx_kl            | 0.3043903 |
|    clip_fraction        | 0.555     |
|    clip_range           | 0.2       |
|    entropy_loss         | -171      |
|    explained_variance   | 0.91      |
|    learning_rate        | 0.0005    |
|    loss                 | -1.77     |
|    n_updates            | 1640      |
|    policy_gradient_loss | -0.0168   |
|    std                  | 5.74      |
|    value_loss           | 0.0706    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.48       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 166        |
|    time_elapsed         | 26777      |
|    total_timesteps      | 5439488    |
| train/                  |            |
|    approx_kl            | 0.29307908 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.2        |
|    entropy_loss         | -171       |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.8       |
|    n_updates            | 1650       |
|    policy_gradient_loss | -0.0195    |
|    std                  | 5.8        |
|    value_loss           | 0.0654     |
----------------------------------------
Eval num_timesteps=5440000, episode_reward=2.75 +/- 1.17
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.75       |
| time/                   |            |
|    total_timesteps      | 5440000    |
| train/                  |            |
|    approx_kl            | 0.31001428 |
|    clip_fraction        | 0.568      |
|    clip_range           | 0.2        |
|    entropy_loss         | -172       |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.73      |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.0219    |
|    std                  | 5.86       |
|    value_loss           | 0.0675     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.32     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 167      |
|    time_elapsed    | 26941    |
|    total_timesteps | 5472256  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 288      |
|    ep_rew_mean          | 5.66     |
| time/                   |          |
|    fps                  | 203      |
|    iterations           | 168      |
|    time_elapsed         | 27101    |
|    total_timesteps      | 5505024  |
| train/                  |          |
|    approx_kl            | 0.299919 |
|    clip_fraction        | 0.57     |
|    clip_range           | 0.2      |
|    entropy_loss         | -172     |
|    explained_variance   | 0.909    |
|    learning_rate        | 0.0005   |
|    loss                 | -1.62    |
|    n_updates            | 1670     |
|    policy_gradient_loss | -0.0235  |
|    std                  | 5.93     |
|    value_loss           | 0.0701   |
--------------------------------------
Eval num_timesteps=5520000, episode_reward=3.00 +/- 0.93
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 3          |
| time/                   |            |
|    total_timesteps      | 5520000    |
| train/                  |            |
|    approx_kl            | 0.29348034 |
|    clip_fraction        | 0.556      |
|    clip_range           | 0.2        |
|    entropy_loss         | -173       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.71      |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.0183    |
|    std                  | 6          |
|    value_loss           | 0.0691     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.05     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 169      |
|    time_elapsed    | 27263    |
|    total_timesteps | 5537792  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.05       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 170        |
|    time_elapsed         | 27414      |
|    total_timesteps      | 5570560    |
| train/                  |            |
|    approx_kl            | 0.28962737 |
|    clip_fraction        | 0.552      |
|    clip_range           | 0.2        |
|    entropy_loss         | -174       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.83      |
|    n_updates            | 1690       |
|    policy_gradient_loss | -0.0199    |
|    std                  | 6.05       |
|    value_loss           | 0.0769     |
----------------------------------------
Eval num_timesteps=5600000, episode_reward=2.50 +/- 1.85
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.5        |
| time/                   |            |
|    total_timesteps      | 5600000    |
| train/                  |            |
|    approx_kl            | 0.29043803 |
|    clip_fraction        | 0.558      |
|    clip_range           | 0.2        |
|    entropy_loss         | -174       |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.73      |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.0208    |
|    std                  | 6.11       |
|    value_loss           | 0.0813     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.22     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 171      |
|    time_elapsed    | 27572    |
|    total_timesteps | 5603328  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.4        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 172        |
|    time_elapsed         | 27724      |
|    total_timesteps      | 5636096    |
| train/                  |            |
|    approx_kl            | 0.28258926 |
|    clip_fraction        | 0.547      |
|    clip_range           | 0.2        |
|    entropy_loss         | -175       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.79      |
|    n_updates            | 1710       |
|    policy_gradient_loss | -0.0199    |
|    std                  | 6.18       |
|    value_loss           | 0.0763     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.64       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 173        |
|    time_elapsed         | 27882      |
|    total_timesteps      | 5668864    |
| train/                  |            |
|    approx_kl            | 0.26826316 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.2        |
|    entropy_loss         | -175       |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.82      |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.0209    |
|    std                  | 6.24       |
|    value_loss           | 0.0781     |
----------------------------------------
Eval num_timesteps=5680000, episode_reward=1.95 +/- 0.82
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 1.95      |
| time/                   |           |
|    total_timesteps      | 5680000   |
| train/                  |           |
|    approx_kl            | 0.2720389 |
|    clip_fraction        | 0.537     |
|    clip_range           | 0.2       |
|    entropy_loss         | -176      |
|    explained_variance   | 0.917     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.77     |
|    n_updates            | 1730      |
|    policy_gradient_loss | -0.0218   |
|    std                  | 6.31      |
|    value_loss           | 0.0713    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.4      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 174      |
|    time_elapsed    | 28040    |
|    total_timesteps | 5701632  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 288      |
|    ep_rew_mean          | 5.88     |
| time/                   |          |
|    fps                  | 203      |
|    iterations           | 175      |
|    time_elapsed         | 28197    |
|    total_timesteps      | 5734400  |
| train/                  |          |
|    approx_kl            | 0.288561 |
|    clip_fraction        | 0.556    |
|    clip_range           | 0.2      |
|    entropy_loss         | -176     |
|    explained_variance   | 0.92     |
|    learning_rate        | 0.0005   |
|    loss                 | -1.74    |
|    n_updates            | 1740     |
|    policy_gradient_loss | -0.0216  |
|    std                  | 6.38     |
|    value_loss           | 0.0708   |
--------------------------------------
Eval num_timesteps=5760000, episode_reward=2.04 +/- 1.15
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.04       |
| time/                   |            |
|    total_timesteps      | 5760000    |
| train/                  |            |
|    approx_kl            | 0.29734755 |
|    clip_fraction        | 0.562      |
|    clip_range           | 0.2        |
|    entropy_loss         | -177       |
|    explained_variance   | 0.927      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.74      |
|    n_updates            | 1750       |
|    policy_gradient_loss | -0.0225    |
|    std                  | 6.44       |
|    value_loss           | 0.0678     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.77     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 176      |
|    time_elapsed    | 28354    |
|    total_timesteps | 5767168  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.26      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 177       |
|    time_elapsed         | 28508     |
|    total_timesteps      | 5799936   |
| train/                  |           |
|    approx_kl            | 0.2734366 |
|    clip_fraction        | 0.551     |
|    clip_range           | 0.2       |
|    entropy_loss         | -177      |
|    explained_variance   | 0.91      |
|    learning_rate        | 0.0005    |
|    loss                 | -1.84     |
|    n_updates            | 1760      |
|    policy_gradient_loss | -0.0215   |
|    std                  | 6.51      |
|    value_loss           | 0.071     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.23       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 178        |
|    time_elapsed         | 28666      |
|    total_timesteps      | 5832704    |
| train/                  |            |
|    approx_kl            | 0.27865496 |
|    clip_fraction        | 0.563      |
|    clip_range           | 0.2        |
|    entropy_loss         | -178       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.84      |
|    n_updates            | 1770       |
|    policy_gradient_loss | -0.0223    |
|    std                  | 6.58       |
|    value_loss           | 0.0663     |
----------------------------------------
Eval num_timesteps=5840000, episode_reward=2.86 +/- 1.31
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 2.86      |
| time/                   |           |
|    total_timesteps      | 5840000   |
| train/                  |           |
|    approx_kl            | 0.2721466 |
|    clip_fraction        | 0.558     |
|    clip_range           | 0.2       |
|    entropy_loss         | -179      |
|    explained_variance   | 0.926     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.86     |
|    n_updates            | 1780      |
|    policy_gradient_loss | -0.0226   |
|    std                  | 6.66      |
|    value_loss           | 0.0625    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.17     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 179      |
|    time_elapsed    | 28834    |
|    total_timesteps | 5865472  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.01       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 180        |
|    time_elapsed         | 28994      |
|    total_timesteps      | 5898240    |
| train/                  |            |
|    approx_kl            | 0.26802856 |
|    clip_fraction        | 0.547      |
|    clip_range           | 0.2        |
|    entropy_loss         | -179       |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.85      |
|    n_updates            | 1790       |
|    policy_gradient_loss | -0.0206    |
|    std                  | 6.73       |
|    value_loss           | 0.0648     |
----------------------------------------
Eval num_timesteps=5920000, episode_reward=2.80 +/- 1.53
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.8        |
| time/                   |            |
|    total_timesteps      | 5920000    |
| train/                  |            |
|    approx_kl            | 0.26019233 |
|    clip_fraction        | 0.545      |
|    clip_range           | 0.2        |
|    entropy_loss         | -180       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.87      |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.0191    |
|    std                  | 6.79       |
|    value_loss           | 0.0802     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.77     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 181      |
|    time_elapsed    | 29157    |
|    total_timesteps | 5931008  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.49      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 182       |
|    time_elapsed         | 29322     |
|    total_timesteps      | 5963776   |
| train/                  |           |
|    approx_kl            | 0.2575959 |
|    clip_fraction        | 0.54      |
|    clip_range           | 0.2       |
|    entropy_loss         | -180      |
|    explained_variance   | 0.903     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.89     |
|    n_updates            | 1810      |
|    policy_gradient_loss | -0.023    |
|    std                  | 6.86      |
|    value_loss           | 0.0802    |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.9       |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 183       |
|    time_elapsed         | 29483     |
|    total_timesteps      | 5996544   |
| train/                  |           |
|    approx_kl            | 0.2627731 |
|    clip_fraction        | 0.537     |
|    clip_range           | 0.2       |
|    entropy_loss         | -181      |
|    explained_variance   | 0.911     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.89     |
|    n_updates            | 1820      |
|    policy_gradient_loss | -0.0222   |
|    std                  | 6.93      |
|    value_loss           | 0.069     |
---------------------------------------
Eval num_timesteps=6000000, episode_reward=2.77 +/- 1.04
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.77       |
| time/                   |            |
|    total_timesteps      | 6000000    |
| train/                  |            |
|    approx_kl            | 0.26672298 |
|    clip_fraction        | 0.547      |
|    clip_range           | 0.2        |
|    entropy_loss         | -181       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.83      |
|    n_updates            | 1830       |
|    policy_gradient_loss | -0.0222    |
|    std                  | 7          |
|    value_loss           | 0.0763     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.8      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 184      |
|    time_elapsed    | 29647    |
|    total_timesteps | 6029312  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.95      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 185       |
|    time_elapsed         | 29808     |
|    total_timesteps      | 6062080   |
| train/                  |           |
|    approx_kl            | 0.2768296 |
|    clip_fraction        | 0.553     |
|    clip_range           | 0.2       |
|    entropy_loss         | -182      |
|    explained_variance   | 0.915     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.68     |
|    n_updates            | 1840      |
|    policy_gradient_loss | -0.0258   |
|    std                  | 7.08      |
|    value_loss           | 0.0755    |
---------------------------------------
Eval num_timesteps=6080000, episode_reward=1.37 +/- 0.62
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.37       |
| time/                   |            |
|    total_timesteps      | 6080000    |
| train/                  |            |
|    approx_kl            | 0.26178807 |
|    clip_fraction        | 0.549      |
|    clip_range           | 0.2        |
|    entropy_loss         | -182       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.85      |
|    n_updates            | 1850       |
|    policy_gradient_loss | -0.0194    |
|    std                  | 7.16       |
|    value_loss           | 0.0781     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.53     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 186      |
|    time_elapsed    | 29974    |
|    total_timesteps | 6094848  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.04       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 187        |
|    time_elapsed         | 30133      |
|    total_timesteps      | 6127616    |
| train/                  |            |
|    approx_kl            | 0.26815668 |
|    clip_fraction        | 0.559      |
|    clip_range           | 0.2        |
|    entropy_loss         | -183       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.92      |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.025     |
|    std                  | 7.23       |
|    value_loss           | 0.0676     |
----------------------------------------
Eval num_timesteps=6160000, episode_reward=1.65 +/- 0.34
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.65       |
| time/                   |            |
|    total_timesteps      | 6160000    |
| train/                  |            |
|    approx_kl            | 0.26511413 |
|    clip_fraction        | 0.547      |
|    clip_range           | 0.2        |
|    entropy_loss         | -184       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.87      |
|    n_updates            | 1870       |
|    policy_gradient_loss | -0.025     |
|    std                  | 7.31       |
|    value_loss           | 0.0707     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.83     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 188      |
|    time_elapsed    | 30294    |
|    total_timesteps | 6160384  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.47      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 189       |
|    time_elapsed         | 30454     |
|    total_timesteps      | 6193152   |
| train/                  |           |
|    approx_kl            | 0.2657687 |
|    clip_fraction        | 0.544     |
|    clip_range           | 0.2       |
|    entropy_loss         | -184      |
|    explained_variance   | 0.915     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.9      |
|    n_updates            | 1880      |
|    policy_gradient_loss | -0.0221   |
|    std                  | 7.39      |
|    value_loss           | 0.0745    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.87       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 190        |
|    time_elapsed         | 30614      |
|    total_timesteps      | 6225920    |
| train/                  |            |
|    approx_kl            | 0.25872016 |
|    clip_fraction        | 0.548      |
|    clip_range           | 0.2        |
|    entropy_loss         | -185       |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.83      |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.0224    |
|    std                  | 7.47       |
|    value_loss           | 0.0698     |
----------------------------------------
Eval num_timesteps=6240000, episode_reward=2.69 +/- 1.58
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 2.69      |
| time/                   |           |
|    total_timesteps      | 6240000   |
| train/                  |           |
|    approx_kl            | 0.2636699 |
|    clip_fraction        | 0.544     |
|    clip_range           | 0.2       |
|    entropy_loss         | -185      |
|    explained_variance   | 0.912     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.83     |
|    n_updates            | 1900      |
|    policy_gradient_loss | -0.0239   |
|    std                  | 7.53      |
|    value_loss           | 0.0726    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.7      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 191      |
|    time_elapsed    | 30780    |
|    total_timesteps | 6258688  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.44      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 192       |
|    time_elapsed         | 30938     |
|    total_timesteps      | 6291456   |
| train/                  |           |
|    approx_kl            | 0.2546767 |
|    clip_fraction        | 0.541     |
|    clip_range           | 0.2       |
|    entropy_loss         | -186      |
|    explained_variance   | 0.913     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.93     |
|    n_updates            | 1910      |
|    policy_gradient_loss | -0.0223   |
|    std                  | 7.61      |
|    value_loss           | 0.0743    |
---------------------------------------
Eval num_timesteps=6320000, episode_reward=2.54 +/- 0.38
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.54       |
| time/                   |            |
|    total_timesteps      | 6320000    |
| train/                  |            |
|    approx_kl            | 0.25484872 |
|    clip_fraction        | 0.548      |
|    clip_range           | 0.2        |
|    entropy_loss         | -186       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.91      |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.0267    |
|    std                  | 7.7        |
|    value_loss           | 0.0758     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.59     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 193      |
|    time_elapsed    | 31097    |
|    total_timesteps | 6324224  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.82      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 194       |
|    time_elapsed         | 31248     |
|    total_timesteps      | 6356992   |
| train/                  |           |
|    approx_kl            | 0.2471104 |
|    clip_fraction        | 0.546     |
|    clip_range           | 0.2       |
|    entropy_loss         | -187      |
|    explained_variance   | 0.909     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.89     |
|    n_updates            | 1930      |
|    policy_gradient_loss | -0.0248   |
|    std                  | 7.77      |
|    value_loss           | 0.0741    |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.35      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 195       |
|    time_elapsed         | 31401     |
|    total_timesteps      | 6389760   |
| train/                  |           |
|    approx_kl            | 0.2608674 |
|    clip_fraction        | 0.544     |
|    clip_range           | 0.2       |
|    entropy_loss         | -187      |
|    explained_variance   | 0.916     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.84     |
|    n_updates            | 1940      |
|    policy_gradient_loss | -0.0231   |
|    std                  | 7.85      |
|    value_loss           | 0.076     |
---------------------------------------
Eval num_timesteps=6400000, episode_reward=2.98 +/- 1.79
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.98       |
| time/                   |            |
|    total_timesteps      | 6400000    |
| train/                  |            |
|    approx_kl            | 0.26354146 |
|    clip_fraction        | 0.559      |
|    clip_range           | 0.2        |
|    entropy_loss         | -188       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.97      |
|    n_updates            | 1950       |
|    policy_gradient_loss | -0.0238    |
|    std                  | 7.93       |
|    value_loss           | 0.0822     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.6      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 196      |
|    time_elapsed    | 31561    |
|    total_timesteps | 6422528  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.32       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 197        |
|    time_elapsed         | 31715      |
|    total_timesteps      | 6455296    |
| train/                  |            |
|    approx_kl            | 0.25967836 |
|    clip_fraction        | 0.552      |
|    clip_range           | 0.2        |
|    entropy_loss         | -189       |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.93      |
|    n_updates            | 1960       |
|    policy_gradient_loss | -0.0214    |
|    std                  | 8.01       |
|    value_loss           | 0.0787     |
----------------------------------------
Eval num_timesteps=6480000, episode_reward=2.73 +/- 0.76
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.73       |
| time/                   |            |
|    total_timesteps      | 6480000    |
| train/                  |            |
|    approx_kl            | 0.24843983 |
|    clip_fraction        | 0.533      |
|    clip_range           | 0.2        |
|    entropy_loss         | -189       |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0005     |
|    loss                 | -1.86      |
|    n_updates            | 1970       |
|    policy_gradient_loss | -0.0208    |
|    std                  | 8.09       |
|    value_loss           | 0.0805     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.14     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 198      |
|    time_elapsed    | 31877    |
|    total_timesteps | 6488064  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.71       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 199        |
|    time_elapsed         | 32035      |
|    total_timesteps      | 6520832    |
| train/                  |            |
|    approx_kl            | 0.25101036 |
|    clip_fraction        | 0.541      |
|    clip_range           | 0.2        |
|    entropy_loss         | -190       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.95      |
|    n_updates            | 1980       |
|    policy_gradient_loss | -0.026     |
|    std                  | 8.18       |
|    value_loss           | 0.0752     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.19       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 200        |
|    time_elapsed         | 32196      |
|    total_timesteps      | 6553600    |
| train/                  |            |
|    approx_kl            | 0.26169932 |
|    clip_fraction        | 0.533      |
|    clip_range           | 0.2        |
|    entropy_loss         | -190       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.93      |
|    n_updates            | 1990       |
|    policy_gradient_loss | -0.027     |
|    std                  | 8.26       |
|    value_loss           | 0.0711     |
----------------------------------------
Eval num_timesteps=6560000, episode_reward=1.75 +/- 0.78
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.75       |
| time/                   |            |
|    total_timesteps      | 6560000    |
| train/                  |            |
|    approx_kl            | 0.24287236 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.2        |
|    entropy_loss         | -191       |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.93      |
|    n_updates            | 2000       |
|    policy_gradient_loss | -0.0226    |
|    std                  | 8.34       |
|    value_loss           | 0.0744     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.09     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 201      |
|    time_elapsed    | 32359    |
|    total_timesteps | 6586368  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.96       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 202        |
|    time_elapsed         | 32519      |
|    total_timesteps      | 6619136    |
| train/                  |            |
|    approx_kl            | 0.25759336 |
|    clip_fraction        | 0.541      |
|    clip_range           | 0.2        |
|    entropy_loss         | -191       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.91      |
|    n_updates            | 2010       |
|    policy_gradient_loss | -0.0281    |
|    std                  | 8.43       |
|    value_loss           | 0.0681     |
----------------------------------------
Eval num_timesteps=6640000, episode_reward=2.83 +/- 1.24
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 2.83      |
| time/                   |           |
|    total_timesteps      | 6640000   |
| train/                  |           |
|    approx_kl            | 0.2443428 |
|    clip_fraction        | 0.54      |
|    clip_range           | 0.2       |
|    entropy_loss         | -192      |
|    explained_variance   | 0.912     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.99     |
|    n_updates            | 2020      |
|    policy_gradient_loss | -0.0204   |
|    std                  | 8.51      |
|    value_loss           | 0.0735    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.34     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 203      |
|    time_elapsed    | 32683    |
|    total_timesteps | 6651904  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.15       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 204        |
|    time_elapsed         | 32840      |
|    total_timesteps      | 6684672    |
| train/                  |            |
|    approx_kl            | 0.24828255 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.2        |
|    entropy_loss         | -192       |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.99      |
|    n_updates            | 2030       |
|    policy_gradient_loss | -0.0211    |
|    std                  | 8.58       |
|    value_loss           | 0.0745     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.23       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 205        |
|    time_elapsed         | 32995      |
|    total_timesteps      | 6717440    |
| train/                  |            |
|    approx_kl            | 0.25168517 |
|    clip_fraction        | 0.551      |
|    clip_range           | 0.2        |
|    entropy_loss         | -193       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.97      |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.0257    |
|    std                  | 8.67       |
|    value_loss           | 0.0781     |
----------------------------------------
Eval num_timesteps=6720000, episode_reward=2.66 +/- 1.40
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.66       |
| time/                   |            |
|    total_timesteps      | 6720000    |
| train/                  |            |
|    approx_kl            | 0.26494563 |
|    clip_fraction        | 0.554      |
|    clip_range           | 0.2        |
|    entropy_loss         | -193       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.9       |
|    n_updates            | 2050       |
|    policy_gradient_loss | -0.0285    |
|    std                  | 8.77       |
|    value_loss           | 0.0741     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.93     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 206      |
|    time_elapsed    | 33152    |
|    total_timesteps | 6750208  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.83       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 207        |
|    time_elapsed         | 33306      |
|    total_timesteps      | 6782976    |
| train/                  |            |
|    approx_kl            | 0.24497946 |
|    clip_fraction        | 0.546      |
|    clip_range           | 0.2        |
|    entropy_loss         | -194       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.02      |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.0224    |
|    std                  | 8.86       |
|    value_loss           | 0.0735     |
----------------------------------------
Eval num_timesteps=6800000, episode_reward=2.29 +/- 0.97
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.29       |
| time/                   |            |
|    total_timesteps      | 6800000    |
| train/                  |            |
|    approx_kl            | 0.24524319 |
|    clip_fraction        | 0.535      |
|    clip_range           | 0.2        |
|    entropy_loss         | -194       |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0005     |
|    loss                 | -1.96      |
|    n_updates            | 2070       |
|    policy_gradient_loss | -0.0227    |
|    std                  | 8.94       |
|    value_loss           | 0.0725     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.25     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 208      |
|    time_elapsed    | 33465    |
|    total_timesteps | 6815744  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.27       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 209        |
|    time_elapsed         | 33627      |
|    total_timesteps      | 6848512    |
| train/                  |            |
|    approx_kl            | 0.24049324 |
|    clip_fraction        | 0.537      |
|    clip_range           | 0.2        |
|    entropy_loss         | -195       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.92      |
|    n_updates            | 2080       |
|    policy_gradient_loss | -0.0261    |
|    std                  | 9.03       |
|    value_loss           | 0.0727     |
----------------------------------------
Eval num_timesteps=6880000, episode_reward=2.29 +/- 1.21
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.29       |
| time/                   |            |
|    total_timesteps      | 6880000    |
| train/                  |            |
|    approx_kl            | 0.24429995 |
|    clip_fraction        | 0.53       |
|    clip_range           | 0.2        |
|    entropy_loss         | -196       |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0005     |
|    loss                 | -2.02      |
|    n_updates            | 2090       |
|    policy_gradient_loss | -0.0246    |
|    std                  | 9.12       |
|    value_loss           | 0.0763     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.76     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 210      |
|    time_elapsed    | 33793    |
|    total_timesteps | 6881280  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.83       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 211        |
|    time_elapsed         | 33953      |
|    total_timesteps      | 6914048    |
| train/                  |            |
|    approx_kl            | 0.24064937 |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.2        |
|    entropy_loss         | -196       |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0005     |
|    loss                 | -1.98      |
|    n_updates            | 2100       |
|    policy_gradient_loss | -0.0217    |
|    std                  | 9.21       |
|    value_loss           | 0.0831     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.04       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 212        |
|    time_elapsed         | 34111      |
|    total_timesteps      | 6946816    |
| train/                  |            |
|    approx_kl            | 0.25661045 |
|    clip_fraction        | 0.551      |
|    clip_range           | 0.2        |
|    entropy_loss         | -197       |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0005     |
|    loss                 | -1.99      |
|    n_updates            | 2110       |
|    policy_gradient_loss | -0.0233    |
|    std                  | 9.31       |
|    value_loss           | 0.0913     |
----------------------------------------
Eval num_timesteps=6960000, episode_reward=2.88 +/- 1.60
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.88       |
| time/                   |            |
|    total_timesteps      | 6960000    |
| train/                  |            |
|    approx_kl            | 0.23529121 |
|    clip_fraction        | 0.546      |
|    clip_range           | 0.2        |
|    entropy_loss         | -197       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.11      |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.0259    |
|    std                  | 9.41       |
|    value_loss           | 0.0811     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.42     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 213      |
|    time_elapsed    | 34273    |
|    total_timesteps | 6979584  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.48      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 214       |
|    time_elapsed         | 34434     |
|    total_timesteps      | 7012352   |
| train/                  |           |
|    approx_kl            | 0.2353209 |
|    clip_fraction        | 0.538     |
|    clip_range           | 0.2       |
|    entropy_loss         | -198      |
|    explained_variance   | 0.909     |
|    learning_rate        | 0.0005    |
|    loss                 | -2        |
|    n_updates            | 2130      |
|    policy_gradient_loss | -0.0238   |
|    std                  | 9.5       |
|    value_loss           | 0.0766    |
---------------------------------------
Eval num_timesteps=7040000, episode_reward=2.62 +/- 0.85
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.62       |
| time/                   |            |
|    total_timesteps      | 7040000    |
| train/                  |            |
|    approx_kl            | 0.24615303 |
|    clip_fraction        | 0.548      |
|    clip_range           | 0.2        |
|    entropy_loss         | -198       |
|    explained_variance   | 0.925      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.02      |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.0261    |
|    std                  | 9.6        |
|    value_loss           | 0.0737     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.6      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 215      |
|    time_elapsed    | 34598    |
|    total_timesteps | 7045120  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.77       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 216        |
|    time_elapsed         | 34760      |
|    total_timesteps      | 7077888    |
| train/                  |            |
|    approx_kl            | 0.23687473 |
|    clip_fraction        | 0.535      |
|    clip_range           | 0.2        |
|    entropy_loss         | -199       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.04      |
|    n_updates            | 2150       |
|    policy_gradient_loss | -0.0243    |
|    std                  | 9.7        |
|    value_loss           | 0.0713     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.7        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 217        |
|    time_elapsed         | 34920      |
|    total_timesteps      | 7110656    |
| train/                  |            |
|    approx_kl            | 0.22814876 |
|    clip_fraction        | 0.531      |
|    clip_range           | 0.2        |
|    entropy_loss         | -199       |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.09      |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.0233    |
|    std                  | 9.79       |
|    value_loss           | 0.0791     |
----------------------------------------
Eval num_timesteps=7120000, episode_reward=2.64 +/- 0.56
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.64       |
| time/                   |            |
|    total_timesteps      | 7120000    |
| train/                  |            |
|    approx_kl            | 0.23348606 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.2        |
|    entropy_loss         | -200       |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.09      |
|    n_updates            | 2170       |
|    policy_gradient_loss | -0.021     |
|    std                  | 9.87       |
|    value_loss           | 0.0704     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.67     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 218      |
|    time_elapsed    | 35087    |
|    total_timesteps | 7143424  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.92      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 219       |
|    time_elapsed         | 35247     |
|    total_timesteps      | 7176192   |
| train/                  |           |
|    approx_kl            | 0.2301999 |
|    clip_fraction        | 0.53      |
|    clip_range           | 0.2       |
|    entropy_loss         | -200      |
|    explained_variance   | 0.916     |
|    learning_rate        | 0.0005    |
|    loss                 | -2.04     |
|    n_updates            | 2180      |
|    policy_gradient_loss | -0.0231   |
|    std                  | 9.97      |
|    value_loss           | 0.0714    |
---------------------------------------
Eval num_timesteps=7200000, episode_reward=1.82 +/- 0.92
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.82       |
| time/                   |            |
|    total_timesteps      | 7200000    |
| train/                  |            |
|    approx_kl            | 0.23319459 |
|    clip_fraction        | 0.535      |
|    clip_range           | 0.2        |
|    entropy_loss         | -201       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.12      |
|    n_updates            | 2190       |
|    policy_gradient_loss | -0.0223    |
|    std                  | 10.1       |
|    value_loss           | 0.0719     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.47     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 220      |
|    time_elapsed    | 35411    |
|    total_timesteps | 7208960  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.75       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 221        |
|    time_elapsed         | 35572      |
|    total_timesteps      | 7241728    |
| train/                  |            |
|    approx_kl            | 0.22891158 |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.2        |
|    entropy_loss         | -201       |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.99      |
|    n_updates            | 2200       |
|    policy_gradient_loss | -0.021     |
|    std                  | 10.2       |
|    value_loss           | 0.0728     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.35       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 222        |
|    time_elapsed         | 35732      |
|    total_timesteps      | 7274496    |
| train/                  |            |
|    approx_kl            | 0.22179928 |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.2        |
|    entropy_loss         | -202       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.98      |
|    n_updates            | 2210       |
|    policy_gradient_loss | -0.0258    |
|    std                  | 10.3       |
|    value_loss           | 0.0721     |
----------------------------------------
Eval num_timesteps=7280000, episode_reward=2.61 +/- 0.90
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.61       |
| time/                   |            |
|    total_timesteps      | 7280000    |
| train/                  |            |
|    approx_kl            | 0.22661951 |
|    clip_fraction        | 0.543      |
|    clip_range           | 0.2        |
|    entropy_loss         | -203       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.06      |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0257    |
|    std                  | 10.4       |
|    value_loss           | 0.0727     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.12     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 223      |
|    time_elapsed    | 35897    |
|    total_timesteps | 7307264  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.55       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 224        |
|    time_elapsed         | 36055      |
|    total_timesteps      | 7340032    |
| train/                  |            |
|    approx_kl            | 0.22919472 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.2        |
|    entropy_loss         | -203       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0005     |
|    loss                 | -1.99      |
|    n_updates            | 2230       |
|    policy_gradient_loss | -0.0236    |
|    std                  | 10.5       |
|    value_loss           | 0.0707     |
----------------------------------------
Eval num_timesteps=7360000, episode_reward=1.88 +/- 0.49
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.88       |
| time/                   |            |
|    total_timesteps      | 7360000    |
| train/                  |            |
|    approx_kl            | 0.22886325 |
|    clip_fraction        | 0.535      |
|    clip_range           | 0.2        |
|    entropy_loss         | -204       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.14      |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.0263    |
|    std                  | 10.6       |
|    value_loss           | 0.0766     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.47     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 225      |
|    time_elapsed    | 36219    |
|    total_timesteps | 7372800  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6          |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 226        |
|    time_elapsed         | 36379      |
|    total_timesteps      | 7405568    |
| train/                  |            |
|    approx_kl            | 0.21308504 |
|    clip_fraction        | 0.531      |
|    clip_range           | 0.2        |
|    entropy_loss         | -204       |
|    explained_variance   | 0.895      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.08      |
|    n_updates            | 2250       |
|    policy_gradient_loss | -0.0198    |
|    std                  | 10.7       |
|    value_loss           | 0.0815     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.92      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 227       |
|    time_elapsed         | 36542     |
|    total_timesteps      | 7438336   |
| train/                  |           |
|    approx_kl            | 0.2070406 |
|    clip_fraction        | 0.515     |
|    clip_range           | 0.2       |
|    entropy_loss         | -205      |
|    explained_variance   | 0.911     |
|    learning_rate        | 0.0005    |
|    loss                 | -1.76     |
|    n_updates            | 2260      |
|    policy_gradient_loss | -0.023    |
|    std                  | 10.8      |
|    value_loss           | 0.0721    |
---------------------------------------
Eval num_timesteps=7440000, episode_reward=2.92 +/- 1.00
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.92       |
| time/                   |            |
|    total_timesteps      | 7440000    |
| train/                  |            |
|    approx_kl            | 0.20949289 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.2        |
|    entropy_loss         | -205       |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.08      |
|    n_updates            | 2270       |
|    policy_gradient_loss | -0.0253    |
|    std                  | 10.9       |
|    value_loss           | 0.075      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.89     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 228      |
|    time_elapsed    | 36706    |
|    total_timesteps | 7471104  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.03       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 229        |
|    time_elapsed         | 36866      |
|    total_timesteps      | 7503872    |
| train/                  |            |
|    approx_kl            | 0.21287699 |
|    clip_fraction        | 0.528      |
|    clip_range           | 0.2        |
|    entropy_loss         | -205       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.03      |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.0246    |
|    std                  | 11         |
|    value_loss           | 0.0801     |
----------------------------------------
Eval num_timesteps=7520000, episode_reward=1.81 +/- 1.19
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.81       |
| time/                   |            |
|    total_timesteps      | 7520000    |
| train/                  |            |
|    approx_kl            | 0.20267561 |
|    clip_fraction        | 0.527      |
|    clip_range           | 0.2        |
|    entropy_loss         | -206       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.09      |
|    n_updates            | 2290       |
|    policy_gradient_loss | -0.0241    |
|    std                  | 11.1       |
|    value_loss           | 0.0781     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.85     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 230      |
|    time_elapsed    | 37028    |
|    total_timesteps | 7536640  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.73       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 231        |
|    time_elapsed         | 37188      |
|    total_timesteps      | 7569408    |
| train/                  |            |
|    approx_kl            | 0.21196696 |
|    clip_fraction        | 0.528      |
|    clip_range           | 0.2        |
|    entropy_loss         | -207       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.13      |
|    n_updates            | 2300       |
|    policy_gradient_loss | -0.0263    |
|    std                  | 11.2       |
|    value_loss           | 0.0816     |
----------------------------------------
Eval num_timesteps=7600000, episode_reward=1.91 +/- 0.44
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.91       |
| time/                   |            |
|    total_timesteps      | 7600000    |
| train/                  |            |
|    approx_kl            | 0.21256453 |
|    clip_fraction        | 0.53       |
|    clip_range           | 0.2        |
|    entropy_loss         | -207       |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.03      |
|    n_updates            | 2310       |
|    policy_gradient_loss | -0.0282    |
|    std                  | 11.3       |
|    value_loss           | 0.0818     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.85     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 232      |
|    time_elapsed    | 37348    |
|    total_timesteps | 7602176  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.15       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 233        |
|    time_elapsed         | 37507      |
|    total_timesteps      | 7634944    |
| train/                  |            |
|    approx_kl            | 0.22010988 |
|    clip_fraction        | 0.539      |
|    clip_range           | 0.2        |
|    entropy_loss         | -208       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.15      |
|    n_updates            | 2320       |
|    policy_gradient_loss | -0.0255    |
|    std                  | 11.4       |
|    value_loss           | 0.0741     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.05       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 234        |
|    time_elapsed         | 37667      |
|    total_timesteps      | 7667712    |
| train/                  |            |
|    approx_kl            | 0.21127616 |
|    clip_fraction        | 0.533      |
|    clip_range           | 0.2        |
|    entropy_loss         | -208       |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.06      |
|    n_updates            | 2330       |
|    policy_gradient_loss | -0.0335    |
|    std                  | 11.5       |
|    value_loss           | 0.0807     |
----------------------------------------
Eval num_timesteps=7680000, episode_reward=2.47 +/- 0.69
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.47       |
| time/                   |            |
|    total_timesteps      | 7680000    |
| train/                  |            |
|    approx_kl            | 0.21669483 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.2        |
|    entropy_loss         | -209       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.17      |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.0269    |
|    std                  | 11.7       |
|    value_loss           | 0.0838     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.84     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 235      |
|    time_elapsed    | 37829    |
|    total_timesteps | 7700480  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.2        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 236        |
|    time_elapsed         | 37988      |
|    total_timesteps      | 7733248    |
| train/                  |            |
|    approx_kl            | 0.20822659 |
|    clip_fraction        | 0.522      |
|    clip_range           | 0.2        |
|    entropy_loss         | -209       |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.08      |
|    n_updates            | 2350       |
|    policy_gradient_loss | -0.0253    |
|    std                  | 11.8       |
|    value_loss           | 0.0817     |
----------------------------------------
Eval num_timesteps=7760000, episode_reward=2.63 +/- 2.01
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.63       |
| time/                   |            |
|    total_timesteps      | 7760000    |
| train/                  |            |
|    approx_kl            | 0.20580873 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.2        |
|    entropy_loss         | -210       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.16      |
|    n_updates            | 2360       |
|    policy_gradient_loss | -0.0232    |
|    std                  | 11.9       |
|    value_loss           | 0.0848     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.89     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 237      |
|    time_elapsed    | 38153    |
|    total_timesteps | 7766016  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.67       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 238        |
|    time_elapsed         | 38313      |
|    total_timesteps      | 7798784    |
| train/                  |            |
|    approx_kl            | 0.20964478 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.2        |
|    entropy_loss         | -210       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.15      |
|    n_updates            | 2370       |
|    policy_gradient_loss | -0.0289    |
|    std                  | 12         |
|    value_loss           | 0.0812     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.04       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 239        |
|    time_elapsed         | 38473      |
|    total_timesteps      | 7831552    |
| train/                  |            |
|    approx_kl            | 0.19537604 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.2        |
|    entropy_loss         | -211       |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.14      |
|    n_updates            | 2380       |
|    policy_gradient_loss | -0.0252    |
|    std                  | 12.2       |
|    value_loss           | 0.0889     |
----------------------------------------
Eval num_timesteps=7840000, episode_reward=2.42 +/- 1.18
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.42       |
| time/                   |            |
|    total_timesteps      | 7840000    |
| train/                  |            |
|    approx_kl            | 0.20025572 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.2        |
|    entropy_loss         | -212       |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.12      |
|    n_updates            | 2390       |
|    policy_gradient_loss | -0.024     |
|    std                  | 12.3       |
|    value_loss           | 0.0793     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.99     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 240      |
|    time_elapsed    | 38639    |
|    total_timesteps | 7864320  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.92       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 241        |
|    time_elapsed         | 38799      |
|    total_timesteps      | 7897088    |
| train/                  |            |
|    approx_kl            | 0.20108637 |
|    clip_fraction        | 0.526      |
|    clip_range           | 0.2        |
|    entropy_loss         | -212       |
|    explained_variance   | 0.898      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.16      |
|    n_updates            | 2400       |
|    policy_gradient_loss | -0.0256    |
|    std                  | 12.4       |
|    value_loss           | 0.082      |
----------------------------------------
Eval num_timesteps=7920000, episode_reward=2.51 +/- 1.80
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.51       |
| time/                   |            |
|    total_timesteps      | 7920000    |
| train/                  |            |
|    approx_kl            | 0.20670332 |
|    clip_fraction        | 0.531      |
|    clip_range           | 0.2        |
|    entropy_loss         | -212       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.2       |
|    n_updates            | 2410       |
|    policy_gradient_loss | -0.0304    |
|    std                  | 12.5       |
|    value_loss           | 0.0795     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.11     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 242      |
|    time_elapsed    | 38966    |
|    total_timesteps | 7929856  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.87       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 243        |
|    time_elapsed         | 39128      |
|    total_timesteps      | 7962624    |
| train/                  |            |
|    approx_kl            | 0.19081363 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.2        |
|    entropy_loss         | -213       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.22      |
|    n_updates            | 2420       |
|    policy_gradient_loss | -0.0261    |
|    std                  | 12.6       |
|    value_loss           | 0.0753     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.24       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 244        |
|    time_elapsed         | 39288      |
|    total_timesteps      | 7995392    |
| train/                  |            |
|    approx_kl            | 0.19638434 |
|    clip_fraction        | 0.514      |
|    clip_range           | 0.2        |
|    entropy_loss         | -214       |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.2       |
|    n_updates            | 2430       |
|    policy_gradient_loss | -0.0234    |
|    std                  | 12.7       |
|    value_loss           | 0.0755     |
----------------------------------------
Eval num_timesteps=8000000, episode_reward=2.71 +/- 1.13
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.71       |
| time/                   |            |
|    total_timesteps      | 8000000    |
| train/                  |            |
|    approx_kl            | 0.19451633 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.2        |
|    entropy_loss         | -214       |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.18      |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.0275    |
|    std                  | 12.9       |
|    value_loss           | 0.0733     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.85     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 245      |
|    time_elapsed    | 39455    |
|    total_timesteps | 8028160  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.11       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 246        |
|    time_elapsed         | 39617      |
|    total_timesteps      | 8060928    |
| train/                  |            |
|    approx_kl            | 0.19372775 |
|    clip_fraction        | 0.516      |
|    clip_range           | 0.2        |
|    entropy_loss         | -215       |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.16      |
|    n_updates            | 2450       |
|    policy_gradient_loss | -0.0244    |
|    std                  | 13         |
|    value_loss           | 0.0747     |
----------------------------------------
Eval num_timesteps=8080000, episode_reward=2.44 +/- 0.94
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.44       |
| time/                   |            |
|    total_timesteps      | 8080000    |
| train/                  |            |
|    approx_kl            | 0.19361629 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.2        |
|    entropy_loss         | -215       |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0005     |
|    loss                 | -2.13      |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.0247    |
|    std                  | 13.1       |
|    value_loss           | 0.0839     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.29     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 247      |
|    time_elapsed    | 39781    |
|    total_timesteps | 8093696  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.14       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 248        |
|    time_elapsed         | 39942      |
|    total_timesteps      | 8126464    |
| train/                  |            |
|    approx_kl            | 0.19878048 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.2        |
|    entropy_loss         | -216       |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.2       |
|    n_updates            | 2470       |
|    policy_gradient_loss | -0.025     |
|    std                  | 13.3       |
|    value_loss           | 0.0741     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6          |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 249        |
|    time_elapsed         | 40103      |
|    total_timesteps      | 8159232    |
| train/                  |            |
|    approx_kl            | 0.18702355 |
|    clip_fraction        | 0.514      |
|    clip_range           | 0.2        |
|    entropy_loss         | -216       |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.2       |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.0239    |
|    std                  | 13.4       |
|    value_loss           | 0.0776     |
----------------------------------------
Eval num_timesteps=8160000, episode_reward=2.31 +/- 1.46
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.31       |
| time/                   |            |
|    total_timesteps      | 8160000    |
| train/                  |            |
|    approx_kl            | 0.20028642 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.2        |
|    entropy_loss         | -217       |
|    explained_variance   | 0.898      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.18      |
|    n_updates            | 2490       |
|    policy_gradient_loss | -0.0239    |
|    std                  | 13.5       |
|    value_loss           | 0.081      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.25     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 250      |
|    time_elapsed    | 40267    |
|    total_timesteps | 8192000  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.58       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 251        |
|    time_elapsed         | 40425      |
|    total_timesteps      | 8224768    |
| train/                  |            |
|    approx_kl            | 0.18629268 |
|    clip_fraction        | 0.518      |
|    clip_range           | 0.2        |
|    entropy_loss         | -217       |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.25      |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.0279    |
|    std                  | 13.6       |
|    value_loss           | 0.0678     |
----------------------------------------
Eval num_timesteps=8240000, episode_reward=2.37 +/- 0.99
Episode length: 288.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 288      |
|    mean_reward          | 2.37     |
| time/                   |          |
|    total_timesteps      | 8240000  |
| train/                  |          |
|    approx_kl            | 0.193959 |
|    clip_fraction        | 0.525    |
|    clip_range           | 0.2      |
|    entropy_loss         | -218     |
|    explained_variance   | 0.9      |
|    learning_rate        | 0.0005   |
|    loss                 | -2.23    |
|    n_updates            | 2510     |
|    policy_gradient_loss | -0.0285  |
|    std                  | 13.8     |
|    value_loss           | 0.0752   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.34     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 252      |
|    time_elapsed    | 40589    |
|    total_timesteps | 8257536  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.07      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 253       |
|    time_elapsed         | 40748     |
|    total_timesteps      | 8290304   |
| train/                  |           |
|    approx_kl            | 0.1897411 |
|    clip_fraction        | 0.52      |
|    clip_range           | 0.2       |
|    entropy_loss         | -218      |
|    explained_variance   | 0.907     |
|    learning_rate        | 0.0005    |
|    loss                 | -2.24     |
|    n_updates            | 2520      |
|    policy_gradient_loss | -0.0274   |
|    std                  | 13.9      |
|    value_loss           | 0.0848    |
---------------------------------------
Eval num_timesteps=8320000, episode_reward=2.66 +/- 0.61
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.66       |
| time/                   |            |
|    total_timesteps      | 8320000    |
| train/                  |            |
|    approx_kl            | 0.19453758 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.2        |
|    entropy_loss         | -219       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.18      |
|    n_updates            | 2530       |
|    policy_gradient_loss | -0.0316    |
|    std                  | 14.1       |
|    value_loss           | 0.0695     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.09     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 254      |
|    time_elapsed    | 40920    |
|    total_timesteps | 8323072  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.92       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 255        |
|    time_elapsed         | 41079      |
|    total_timesteps      | 8355840    |
| train/                  |            |
|    approx_kl            | 0.18976524 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.2        |
|    entropy_loss         | -219       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.31      |
|    n_updates            | 2540       |
|    policy_gradient_loss | -0.0267    |
|    std                  | 14.2       |
|    value_loss           | 0.0751     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.06       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 256        |
|    time_elapsed         | 41239      |
|    total_timesteps      | 8388608    |
| train/                  |            |
|    approx_kl            | 0.19467935 |
|    clip_fraction        | 0.519      |
|    clip_range           | 0.2        |
|    entropy_loss         | -220       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.23      |
|    n_updates            | 2550       |
|    policy_gradient_loss | -0.0259    |
|    std                  | 14.3       |
|    value_loss           | 0.0844     |
----------------------------------------
Eval num_timesteps=8400000, episode_reward=2.92 +/- 1.31
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.92       |
| time/                   |            |
|    total_timesteps      | 8400000    |
| train/                  |            |
|    approx_kl            | 0.19542879 |
|    clip_fraction        | 0.517      |
|    clip_range           | 0.2        |
|    entropy_loss         | -220       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.19      |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.024     |
|    std                  | 14.5       |
|    value_loss           | 0.0832     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.56     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 257      |
|    time_elapsed    | 41404    |
|    total_timesteps | 8421376  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.54       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 258        |
|    time_elapsed         | 41565      |
|    total_timesteps      | 8454144    |
| train/                  |            |
|    approx_kl            | 0.19336727 |
|    clip_fraction        | 0.526      |
|    clip_range           | 0.2        |
|    entropy_loss         | -221       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.24      |
|    n_updates            | 2570       |
|    policy_gradient_loss | -0.0259    |
|    std                  | 14.6       |
|    value_loss           | 0.0824     |
----------------------------------------
Eval num_timesteps=8480000, episode_reward=2.81 +/- 0.70
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.81       |
| time/                   |            |
|    total_timesteps      | 8480000    |
| train/                  |            |
|    approx_kl            | 0.18502893 |
|    clip_fraction        | 0.502      |
|    clip_range           | 0.2        |
|    entropy_loss         | -221       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.33      |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.0258    |
|    std                  | 14.7       |
|    value_loss           | 0.0766     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.08     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 259      |
|    time_elapsed    | 41729    |
|    total_timesteps | 8486912  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.94      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 260       |
|    time_elapsed         | 41890     |
|    total_timesteps      | 8519680   |
| train/                  |           |
|    approx_kl            | 0.1867601 |
|    clip_fraction        | 0.512     |
|    clip_range           | 0.2       |
|    entropy_loss         | -222      |
|    explained_variance   | 0.913     |
|    learning_rate        | 0.0005    |
|    loss                 | -2.26     |
|    n_updates            | 2590      |
|    policy_gradient_loss | -0.026    |
|    std                  | 14.9      |
|    value_loss           | 0.0785    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.41       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 261        |
|    time_elapsed         | 42051      |
|    total_timesteps      | 8552448    |
| train/                  |            |
|    approx_kl            | 0.17412953 |
|    clip_fraction        | 0.508      |
|    clip_range           | 0.2        |
|    entropy_loss         | -222       |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.23      |
|    n_updates            | 2600       |
|    policy_gradient_loss | -0.0247    |
|    std                  | 15         |
|    value_loss           | 0.0745     |
----------------------------------------
Eval num_timesteps=8560000, episode_reward=2.66 +/- 1.09
Episode length: 288.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 288      |
|    mean_reward          | 2.66     |
| time/                   |          |
|    total_timesteps      | 8560000  |
| train/                  |          |
|    approx_kl            | 0.193703 |
|    clip_fraction        | 0.516    |
|    clip_range           | 0.2      |
|    entropy_loss         | -223     |
|    explained_variance   | 0.904    |
|    learning_rate        | 0.0005   |
|    loss                 | -2.25    |
|    n_updates            | 2610     |
|    policy_gradient_loss | -0.0292  |
|    std                  | 15.2     |
|    value_loss           | 0.0833   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.79     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 262      |
|    time_elapsed    | 42216    |
|    total_timesteps | 8585216  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.26      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 263       |
|    time_elapsed         | 42379     |
|    total_timesteps      | 8617984   |
| train/                  |           |
|    approx_kl            | 0.1767393 |
|    clip_fraction        | 0.497     |
|    clip_range           | 0.2       |
|    entropy_loss         | -223      |
|    explained_variance   | 0.908     |
|    learning_rate        | 0.0005    |
|    loss                 | -2.17     |
|    n_updates            | 2620      |
|    policy_gradient_loss | -0.0284   |
|    std                  | 15.3      |
|    value_loss           | 0.0772    |
---------------------------------------
Eval num_timesteps=8640000, episode_reward=2.78 +/- 0.47
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.78       |
| time/                   |            |
|    total_timesteps      | 8640000    |
| train/                  |            |
|    approx_kl            | 0.18585469 |
|    clip_fraction        | 0.507      |
|    clip_range           | 0.2        |
|    entropy_loss         | -224       |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.23      |
|    n_updates            | 2630       |
|    policy_gradient_loss | -0.024     |
|    std                  | 15.4       |
|    value_loss           | 0.0792     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.37     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 264      |
|    time_elapsed    | 42544    |
|    total_timesteps | 8650752  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.67       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 265        |
|    time_elapsed         | 42703      |
|    total_timesteps      | 8683520    |
| train/                  |            |
|    approx_kl            | 0.18794629 |
|    clip_fraction        | 0.522      |
|    clip_range           | 0.2        |
|    entropy_loss         | -224       |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.35      |
|    n_updates            | 2640       |
|    policy_gradient_loss | -0.0321    |
|    std                  | 15.6       |
|    value_loss           | 0.077      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.45       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 266        |
|    time_elapsed         | 42863      |
|    total_timesteps      | 8716288    |
| train/                  |            |
|    approx_kl            | 0.18824022 |
|    clip_fraction        | 0.506      |
|    clip_range           | 0.2        |
|    entropy_loss         | -225       |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.31      |
|    n_updates            | 2650       |
|    policy_gradient_loss | -0.0252    |
|    std                  | 15.7       |
|    value_loss           | 0.0895     |
----------------------------------------
Eval num_timesteps=8720000, episode_reward=2.10 +/- 1.28
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.1        |
| time/                   |            |
|    total_timesteps      | 8720000    |
| train/                  |            |
|    approx_kl            | 0.19096066 |
|    clip_fraction        | 0.523      |
|    clip_range           | 0.2        |
|    entropy_loss         | -225       |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.27      |
|    n_updates            | 2660       |
|    policy_gradient_loss | -0.0249    |
|    std                  | 15.8       |
|    value_loss           | 0.0807     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.42     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 267      |
|    time_elapsed    | 43028    |
|    total_timesteps | 8749056  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 6.25      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 268       |
|    time_elapsed         | 43189     |
|    total_timesteps      | 8781824   |
| train/                  |           |
|    approx_kl            | 0.1881814 |
|    clip_fraction        | 0.519     |
|    clip_range           | 0.2       |
|    entropy_loss         | -226      |
|    explained_variance   | 0.91      |
|    learning_rate        | 0.0005    |
|    loss                 | -2.29     |
|    n_updates            | 2670      |
|    policy_gradient_loss | -0.0273   |
|    std                  | 16        |
|    value_loss           | 0.0854    |
---------------------------------------
Eval num_timesteps=8800000, episode_reward=2.34 +/- 0.42
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 2.34      |
| time/                   |           |
|    total_timesteps      | 8800000   |
| train/                  |           |
|    approx_kl            | 0.1920097 |
|    clip_fraction        | 0.519     |
|    clip_range           | 0.2       |
|    entropy_loss         | -226      |
|    explained_variance   | 0.906     |
|    learning_rate        | 0.0005    |
|    loss                 | -2.38     |
|    n_updates            | 2680      |
|    policy_gradient_loss | -0.0288   |
|    std                  | 16.2      |
|    value_loss           | 0.0858    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.57     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 269      |
|    time_elapsed    | 43355    |
|    total_timesteps | 8814592  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.93       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 270        |
|    time_elapsed         | 43516      |
|    total_timesteps      | 8847360    |
| train/                  |            |
|    approx_kl            | 0.18376172 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.2        |
|    entropy_loss         | -227       |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.28      |
|    n_updates            | 2690       |
|    policy_gradient_loss | -0.0266    |
|    std                  | 16.3       |
|    value_loss           | 0.0798     |
----------------------------------------
Eval num_timesteps=8880000, episode_reward=3.06 +/- 1.14
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 3.06       |
| time/                   |            |
|    total_timesteps      | 8880000    |
| train/                  |            |
|    approx_kl            | 0.18831241 |
|    clip_fraction        | 0.512      |
|    clip_range           | 0.2        |
|    entropy_loss         | -227       |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.35      |
|    n_updates            | 2700       |
|    policy_gradient_loss | -0.0231    |
|    std                  | 16.5       |
|    value_loss           | 0.0819     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.32     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 271      |
|    time_elapsed    | 43678    |
|    total_timesteps | 8880128  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6          |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 272        |
|    time_elapsed         | 43843      |
|    total_timesteps      | 8912896    |
| train/                  |            |
|    approx_kl            | 0.17312413 |
|    clip_fraction        | 0.499      |
|    clip_range           | 0.2        |
|    entropy_loss         | -228       |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.36      |
|    n_updates            | 2710       |
|    policy_gradient_loss | -0.0294    |
|    std                  | 16.6       |
|    value_loss           | 0.0747     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.19       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 273        |
|    time_elapsed         | 44002      |
|    total_timesteps      | 8945664    |
| train/                  |            |
|    approx_kl            | 0.17902216 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.2        |
|    entropy_loss         | -228       |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.29      |
|    n_updates            | 2720       |
|    policy_gradient_loss | -0.0262    |
|    std                  | 16.8       |
|    value_loss           | 0.0906     |
----------------------------------------
Eval num_timesteps=8960000, episode_reward=3.20 +/- 1.68
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 3.2        |
| time/                   |            |
|    total_timesteps      | 8960000    |
| train/                  |            |
|    approx_kl            | 0.18001258 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.2        |
|    entropy_loss         | -229       |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.4       |
|    n_updates            | 2730       |
|    policy_gradient_loss | -0.0263    |
|    std                  | 16.9       |
|    value_loss           | 0.0833     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.53     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 274      |
|    time_elapsed    | 44163    |
|    total_timesteps | 8978432  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.99       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 275        |
|    time_elapsed         | 44325      |
|    total_timesteps      | 9011200    |
| train/                  |            |
|    approx_kl            | 0.17931852 |
|    clip_fraction        | 0.507      |
|    clip_range           | 0.2        |
|    entropy_loss         | -229       |
|    explained_variance   | 0.903      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.24      |
|    n_updates            | 2740       |
|    policy_gradient_loss | -0.0274    |
|    std                  | 17.1       |
|    value_loss           | 0.0851     |
----------------------------------------
Eval num_timesteps=9040000, episode_reward=3.13 +/- 1.38
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 3.13       |
| time/                   |            |
|    total_timesteps      | 9040000    |
| train/                  |            |
|    approx_kl            | 0.18202642 |
|    clip_fraction        | 0.512      |
|    clip_range           | 0.2        |
|    entropy_loss         | -230       |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0005     |
|    loss                 | -2.29      |
|    n_updates            | 2750       |
|    policy_gradient_loss | -0.0262    |
|    std                  | 17.2       |
|    value_loss           | 0.0854     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.82     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 276      |
|    time_elapsed    | 44491    |
|    total_timesteps | 9043968  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.31       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 277        |
|    time_elapsed         | 44653      |
|    total_timesteps      | 9076736    |
| train/                  |            |
|    approx_kl            | 0.19002444 |
|    clip_fraction        | 0.522      |
|    clip_range           | 0.2        |
|    entropy_loss         | -230       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.36      |
|    n_updates            | 2760       |
|    policy_gradient_loss | -0.0273    |
|    std                  | 17.4       |
|    value_loss           | 0.075      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.8        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 278        |
|    time_elapsed         | 44814      |
|    total_timesteps      | 9109504    |
| train/                  |            |
|    approx_kl            | 0.17659868 |
|    clip_fraction        | 0.509      |
|    clip_range           | 0.2        |
|    entropy_loss         | -231       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.27      |
|    n_updates            | 2770       |
|    policy_gradient_loss | -0.0307    |
|    std                  | 17.5       |
|    value_loss           | 0.0783     |
----------------------------------------
Eval num_timesteps=9120000, episode_reward=2.62 +/- 1.37
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.62       |
| time/                   |            |
|    total_timesteps      | 9120000    |
| train/                  |            |
|    approx_kl            | 0.18482667 |
|    clip_fraction        | 0.523      |
|    clip_range           | 0.2        |
|    entropy_loss         | -231       |
|    explained_variance   | 0.922      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.31      |
|    n_updates            | 2780       |
|    policy_gradient_loss | -0.0322    |
|    std                  | 17.7       |
|    value_loss           | 0.0764     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.04     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 279      |
|    time_elapsed    | 44977    |
|    total_timesteps | 9142272  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.09       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 280        |
|    time_elapsed         | 45136      |
|    total_timesteps      | 9175040    |
| train/                  |            |
|    approx_kl            | 0.18264414 |
|    clip_fraction        | 0.507      |
|    clip_range           | 0.2        |
|    entropy_loss         | -232       |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.33      |
|    n_updates            | 2790       |
|    policy_gradient_loss | -0.0306    |
|    std                  | 17.9       |
|    value_loss           | 0.0817     |
----------------------------------------
Eval num_timesteps=9200000, episode_reward=2.08 +/- 1.45
Episode length: 288.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 2.08      |
| time/                   |           |
|    total_timesteps      | 9200000   |
| train/                  |           |
|    approx_kl            | 0.1790774 |
|    clip_fraction        | 0.505     |
|    clip_range           | 0.2       |
|    entropy_loss         | -232      |
|    explained_variance   | 0.917     |
|    learning_rate        | 0.0005    |
|    loss                 | -2.42     |
|    n_updates            | 2800      |
|    policy_gradient_loss | -0.0298   |
|    std                  | 18.1      |
|    value_loss           | 0.0762    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.59     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 281      |
|    time_elapsed    | 45300    |
|    total_timesteps | 9207808  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.48       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 282        |
|    time_elapsed         | 45461      |
|    total_timesteps      | 9240576    |
| train/                  |            |
|    approx_kl            | 0.17444095 |
|    clip_fraction        | 0.497      |
|    clip_range           | 0.2        |
|    entropy_loss         | -233       |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.22      |
|    n_updates            | 2810       |
|    policy_gradient_loss | -0.0269    |
|    std                  | 18.2       |
|    value_loss           | 0.0824     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.4        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 283        |
|    time_elapsed         | 45622      |
|    total_timesteps      | 9273344    |
| train/                  |            |
|    approx_kl            | 0.17479777 |
|    clip_fraction        | 0.499      |
|    clip_range           | 0.2        |
|    entropy_loss         | -233       |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.26      |
|    n_updates            | 2820       |
|    policy_gradient_loss | -0.0266    |
|    std                  | 18.4       |
|    value_loss           | 0.083      |
----------------------------------------
Eval num_timesteps=9280000, episode_reward=2.00 +/- 0.66
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2          |
| time/                   |            |
|    total_timesteps      | 9280000    |
| train/                  |            |
|    approx_kl            | 0.18214688 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.2        |
|    entropy_loss         | -234       |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.44      |
|    n_updates            | 2830       |
|    policy_gradient_loss | -0.0304    |
|    std                  | 18.6       |
|    value_loss           | 0.0749     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.92     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 284      |
|    time_elapsed    | 45785    |
|    total_timesteps | 9306112  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.95       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 285        |
|    time_elapsed         | 45944      |
|    total_timesteps      | 9338880    |
| train/                  |            |
|    approx_kl            | 0.17619166 |
|    clip_fraction        | 0.501      |
|    clip_range           | 0.2        |
|    entropy_loss         | -234       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.36      |
|    n_updates            | 2840       |
|    policy_gradient_loss | -0.0257    |
|    std                  | 18.8       |
|    value_loss           | 0.0799     |
----------------------------------------
Eval num_timesteps=9360000, episode_reward=3.44 +/- 0.64
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 3.44       |
| time/                   |            |
|    total_timesteps      | 9360000    |
| train/                  |            |
|    approx_kl            | 0.17601098 |
|    clip_fraction        | 0.509      |
|    clip_range           | 0.2        |
|    entropy_loss         | -235       |
|    explained_variance   | 0.894      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.35      |
|    n_updates            | 2850       |
|    policy_gradient_loss | -0.0278    |
|    std                  | 18.9       |
|    value_loss           | 0.0822     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.33     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 286      |
|    time_elapsed    | 46110    |
|    total_timesteps | 9371648  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.55       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 287        |
|    time_elapsed         | 46273      |
|    total_timesteps      | 9404416    |
| train/                  |            |
|    approx_kl            | 0.18397692 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.2        |
|    entropy_loss         | -235       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.46      |
|    n_updates            | 2860       |
|    policy_gradient_loss | -0.0299    |
|    std                  | 19.1       |
|    value_loss           | 0.0765     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.63       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 288        |
|    time_elapsed         | 46433      |
|    total_timesteps      | 9437184    |
| train/                  |            |
|    approx_kl            | 0.17646022 |
|    clip_fraction        | 0.506      |
|    clip_range           | 0.2        |
|    entropy_loss         | -236       |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.23      |
|    n_updates            | 2870       |
|    policy_gradient_loss | -0.0295    |
|    std                  | 19.3       |
|    value_loss           | 0.0755     |
----------------------------------------
Eval num_timesteps=9440000, episode_reward=2.95 +/- 0.35
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.95       |
| time/                   |            |
|    total_timesteps      | 9440000    |
| train/                  |            |
|    approx_kl            | 0.17214482 |
|    clip_fraction        | 0.501      |
|    clip_range           | 0.2        |
|    entropy_loss         | -236       |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0005     |
|    loss                 | -2.37      |
|    n_updates            | 2880       |
|    policy_gradient_loss | -0.026     |
|    std                  | 19.5       |
|    value_loss           | 0.076      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.67     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 289      |
|    time_elapsed    | 46595    |
|    total_timesteps | 9469952  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.42       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 290        |
|    time_elapsed         | 46757      |
|    total_timesteps      | 9502720    |
| train/                  |            |
|    approx_kl            | 0.16682497 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.2        |
|    entropy_loss         | -237       |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.42      |
|    n_updates            | 2890       |
|    policy_gradient_loss | -0.0315    |
|    std                  | 19.7       |
|    value_loss           | 0.0745     |
----------------------------------------
Eval num_timesteps=9520000, episode_reward=2.62 +/- 1.35
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.62       |
| time/                   |            |
|    total_timesteps      | 9520000    |
| train/                  |            |
|    approx_kl            | 0.17155689 |
|    clip_fraction        | 0.503      |
|    clip_range           | 0.2        |
|    entropy_loss         | -237       |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.43      |
|    n_updates            | 2900       |
|    policy_gradient_loss | -0.0255    |
|    std                  | 19.8       |
|    value_loss           | 0.0797     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.96     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 291      |
|    time_elapsed    | 46923    |
|    total_timesteps | 9535488  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.41       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 292        |
|    time_elapsed         | 47084      |
|    total_timesteps      | 9568256    |
| train/                  |            |
|    approx_kl            | 0.16568904 |
|    clip_fraction        | 0.494      |
|    clip_range           | 0.2        |
|    entropy_loss         | -238       |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.42      |
|    n_updates            | 2910       |
|    policy_gradient_loss | -0.0266    |
|    std                  | 20         |
|    value_loss           | 0.0854     |
----------------------------------------
Eval num_timesteps=9600000, episode_reward=2.32 +/- 0.80
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.32       |
| time/                   |            |
|    total_timesteps      | 9600000    |
| train/                  |            |
|    approx_kl            | 0.17689544 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.2        |
|    entropy_loss         | -238       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.44      |
|    n_updates            | 2920       |
|    policy_gradient_loss | -0.0275    |
|    std                  | 20.2       |
|    value_loss           | 0.0802     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.24     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 293      |
|    time_elapsed    | 47248    |
|    total_timesteps | 9601024  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 4.99       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 294        |
|    time_elapsed         | 47405      |
|    total_timesteps      | 9633792    |
| train/                  |            |
|    approx_kl            | 0.17472374 |
|    clip_fraction        | 0.509      |
|    clip_range           | 0.2        |
|    entropy_loss         | -239       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.41      |
|    n_updates            | 2930       |
|    policy_gradient_loss | -0.0282    |
|    std                  | 20.4       |
|    value_loss           | 0.0793     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.1        |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 295        |
|    time_elapsed         | 47562      |
|    total_timesteps      | 9666560    |
| train/                  |            |
|    approx_kl            | 0.17670166 |
|    clip_fraction        | 0.502      |
|    clip_range           | 0.2        |
|    entropy_loss         | -239       |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.41      |
|    n_updates            | 2940       |
|    policy_gradient_loss | -0.0261    |
|    std                  | 20.6       |
|    value_loss           | 0.0807     |
----------------------------------------
Eval num_timesteps=9680000, episode_reward=1.61 +/- 0.93
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 1.61       |
| time/                   |            |
|    total_timesteps      | 9680000    |
| train/                  |            |
|    approx_kl            | 0.16221699 |
|    clip_fraction        | 0.497      |
|    clip_range           | 0.2        |
|    entropy_loss         | -240       |
|    explained_variance   | 0.898      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.43      |
|    n_updates            | 2950       |
|    policy_gradient_loss | -0.0278    |
|    std                  | 20.8       |
|    value_loss           | 0.0848     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.5      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 296      |
|    time_elapsed    | 47727    |
|    total_timesteps | 9699328  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 5.71      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 297       |
|    time_elapsed         | 47884     |
|    total_timesteps      | 9732096   |
| train/                  |           |
|    approx_kl            | 0.1675561 |
|    clip_fraction        | 0.502     |
|    clip_range           | 0.2       |
|    entropy_loss         | -240      |
|    explained_variance   | 0.897     |
|    learning_rate        | 0.0005    |
|    loss                 | -2.39     |
|    n_updates            | 2960      |
|    policy_gradient_loss | -0.0308   |
|    std                  | 21        |
|    value_loss           | 0.0814    |
---------------------------------------
Eval num_timesteps=9760000, episode_reward=2.74 +/- 1.42
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.74       |
| time/                   |            |
|    total_timesteps      | 9760000    |
| train/                  |            |
|    approx_kl            | 0.16451432 |
|    clip_fraction        | 0.492      |
|    clip_range           | 0.2        |
|    entropy_loss         | -241       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.4       |
|    n_updates            | 2970       |
|    policy_gradient_loss | -0.0257    |
|    std                  | 21.2       |
|    value_loss           | 0.0786     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 6.3      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 298      |
|    time_elapsed    | 48049    |
|    total_timesteps | 9764864  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.25       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 299        |
|    time_elapsed         | 48211      |
|    total_timesteps      | 9797632    |
| train/                  |            |
|    approx_kl            | 0.16287243 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.2        |
|    entropy_loss         | -241       |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.5       |
|    n_updates            | 2980       |
|    policy_gradient_loss | -0.0265    |
|    std                  | 21.4       |
|    value_loss           | 0.0894     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.93       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 300        |
|    time_elapsed         | 48371      |
|    total_timesteps      | 9830400    |
| train/                  |            |
|    approx_kl            | 0.16509262 |
|    clip_fraction        | 0.496      |
|    clip_range           | 0.2        |
|    entropy_loss         | -242       |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.29      |
|    n_updates            | 2990       |
|    policy_gradient_loss | -0.0275    |
|    std                  | 21.6       |
|    value_loss           | 0.0776     |
----------------------------------------
Eval num_timesteps=9840000, episode_reward=2.27 +/- 1.31
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.27       |
| time/                   |            |
|    total_timesteps      | 9840000    |
| train/                  |            |
|    approx_kl            | 0.16984084 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.2        |
|    entropy_loss         | -242       |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.49      |
|    n_updates            | 3000       |
|    policy_gradient_loss | -0.0281    |
|    std                  | 21.8       |
|    value_loss           | 0.0846     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.95     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 301      |
|    time_elapsed    | 48537    |
|    total_timesteps | 9863168  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.35       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 302        |
|    time_elapsed         | 48699      |
|    total_timesteps      | 9895936    |
| train/                  |            |
|    approx_kl            | 0.17208031 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.2        |
|    entropy_loss         | -243       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.46      |
|    n_updates            | 3010       |
|    policy_gradient_loss | -0.0268    |
|    std                  | 22         |
|    value_loss           | 0.0839     |
----------------------------------------
Eval num_timesteps=9920000, episode_reward=2.60 +/- 0.97
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 2.6        |
| time/                   |            |
|    total_timesteps      | 9920000    |
| train/                  |            |
|    approx_kl            | 0.15813278 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.2        |
|    entropy_loss         | -243       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.43      |
|    n_updates            | 3020       |
|    policy_gradient_loss | -0.029     |
|    std                  | 22.2       |
|    value_loss           | 0.0809     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.5      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 303      |
|    time_elapsed    | 48865    |
|    total_timesteps | 9928704  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 6.04       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 304        |
|    time_elapsed         | 49027      |
|    total_timesteps      | 9961472    |
| train/                  |            |
|    approx_kl            | 0.16732815 |
|    clip_fraction        | 0.498      |
|    clip_range           | 0.2        |
|    entropy_loss         | -244       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.46      |
|    n_updates            | 3030       |
|    policy_gradient_loss | -0.0281    |
|    std                  | 22.4       |
|    value_loss           | 0.0703     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 288        |
|    ep_rew_mean          | 5.79       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 305        |
|    time_elapsed         | 49189      |
|    total_timesteps      | 9994240    |
| train/                  |            |
|    approx_kl            | 0.16071591 |
|    clip_fraction        | 0.493      |
|    clip_range           | 0.2        |
|    entropy_loss         | -244       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.48      |
|    n_updates            | 3040       |
|    policy_gradient_loss | -0.0274    |
|    std                  | 22.6       |
|    value_loss           | 0.0835     |
----------------------------------------
Eval num_timesteps=10000000, episode_reward=3.05 +/- 1.13
Episode length: 288.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 3.05       |
| time/                   |            |
|    total_timesteps      | 10000000   |
| train/                  |            |
|    approx_kl            | 0.16188005 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.2        |
|    entropy_loss         | -245       |
|    explained_variance   | 0.903      |
|    learning_rate        | 0.0005     |
|    loss                 | -2.56      |
|    n_updates            | 3050       |
|    policy_gradient_loss | -0.0306    |
|    std                  | 22.9       |
|    value_loss           | 0.0761     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 5.46     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 306      |
|    time_elapsed    | 49354    |
|    total_timesteps | 10027008 |
---------------------------------
Training complete. Model saved to logs/final_model/rppo_evcharging_final
Evaluating model...
Mean reward: 2.34 +/- 1.19
